{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HMS-Commander\n",
    "Author: William (Bill) Katzenmeyer, P.E., C.F.M. (C.H. Fenstermaker and Associates, LLC)\n",
    "\n",
    "Source: https://github.com/billk-FM/HEC-Commander-Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User Defined Inputs\n",
    "\n",
    "# Provide the HMS Project Directory and Project Name\n",
    "hms_project_directory = r\"C:\\Your_HMS_Project_Directory\"\n",
    "hms_basin_file = \"Your_HMS_Basin.basin\"\n",
    "\n",
    "# Define Calibration Runs CSV Filename (place in HMS Project Directory)\n",
    "user_calibration_runs_csv_filename = \"Example_Test.csv\"\n",
    "\n",
    "hms_run_names = [\n",
    "    \"Your_Run_Name_1\",\n",
    "]\n",
    "\n",
    "# If multiple run names are provided, each will generate a full set of user-defined runs\n",
    "\n",
    "#Set a DSS output file suffix to differentiate run sets\n",
    "hms_dss_suffix = \"_Batch_Name\"  # define your suffix here\n",
    "print(\"DSS Suffix: \" + hms_dss_suffix)\n",
    "\n",
    "# Override Baseflow:None to Baseflow: Recession\n",
    "hms_recession_baseflow = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Additional Settings ----\n",
    "\n",
    "# Override Baseflow None to Baseflow Recession\n",
    "Baseflow_Method_Set_to_Recession = \"Yes\"\n",
    "\n",
    "# Define the path to the HEC-HMS executable\n",
    "hms_executable_path = r\"C:\\Program Files\\HEC\\HEC-HMS\\4.9\"\n",
    "print(\"HEC-HMS Executable Path: \" + hms_executable_path)\n",
    "\n",
    "# Define paths for HMScompute.py and HMScompute.bat which are used to run HMS through Jython\n",
    "hms_compute_py_path = r\"C:\\jython2.7.3\\HMScompute.py\"\n",
    "hms_compute_bat_path = r\"C:\\jython2.7.3\\HMScompute.bat\"\n",
    "\n",
    "''' If jython heap size is too small, the HMS will freeze at 100% CPU and never finish'''\n",
    "jython_initial_heap_size = \"256m\"       #initial heap size for java virtual machine\n",
    "print(\"Jython Initial Heap Size: \" + jython_initial_heap_size)\n",
    "jython_maximum_heap_size = \"4096m\"      #maximum heap size for java virtual machine\n",
    "print(\"Jython Maximum Heap Size: \" + jython_maximum_heap_size)\n",
    "\n",
    "# Set Canopy Method to Simple Zero to avoid warnings (Deprecated, not needed after HMS 4.9)\n",
    "Canopy_Method_Set_To_Simple_Zero = \"No\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install/Import Required Python Packages\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# List of packages you want to ensure are installed\n",
    "packages = [\"os\", \"shutil\", \"pandas\", \"geopandas\", \"subprocess\", \"re\", \"csv\", \"itertools\"]\n",
    "\n",
    "# Logic to Install Packages\n",
    "def install(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "    print(\"Installed \" + package + \" successfully\")\n",
    "\n",
    "for package in packages:\n",
    "    try:\n",
    "        # If the import succeeds, the package is installed, so we can move on\n",
    "        __import__(package)\n",
    "    except ImportError:\n",
    "        # If the import fails, the package is not installed and we try to install it\n",
    "        install(package)\n",
    "\n",
    "# Import All Required Libraries for Script\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import itertools\n",
    "import re\n",
    "import csv\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for Jython 2.7.3 at C:\\jython2.7.3\n",
    "\n",
    "jython_path = r\"C:\\jython2.7.3\"\n",
    "jython_jar_path = os.path.join(jython_path, \"jython.jar\")\n",
    "\n",
    "def jython_exists(path):\n",
    "    exists = os.path.exists(path)\n",
    "    return exists\n",
    "\n",
    "def print_installation_instructions():\n",
    "    print(\"Jython is not found at the provided path.\")\n",
    "    print(\"Please install the necessary software:\")\n",
    "    print(\"1. Java: https://www.java.com/en/download/\")\n",
    "    print(\"2. Java SE Development Kit (check for the latest version): https://www.oracle.com/java/technologies/javase-jdk-downloads.html\")\n",
    "    print(\"3. Jython: https://www.jython.org/download.html\")\n",
    "\n",
    "if jython_exists(jython_jar_path):\n",
    "    print(f\"Jython Exists at {jython_jar_path}\")\n",
    "else:\n",
    "    print_installation_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build HMScompute.bat and HMScompute.py for Jython 2.7.3\n",
    "\n",
    "\n",
    "# Define the path and content for HMScompute.py\n",
    "hms_compute_py_content = '''from hms.model import Project\n",
    "from hms import Hms\n",
    "\n",
    "import glob\n",
    "import sys\n",
    "\n",
    "hmspth = sys.argv[1]\n",
    "runName = sys.argv[2]\n",
    "# print 'running' + str(hmspth)\n",
    "\n",
    "myProject = Project.open(hmspth)\n",
    "myProject.computeRun(runName)\n",
    "myProject.close()\n",
    "\n",
    "Hms.shutdownEngine()\n",
    "'''\n",
    "\n",
    "# Define the path and content for HMScompute.bat\n",
    "hms_compute_bat_content = f'''set \"HMS={hms_executable_path}\"\n",
    "set \"PATH=%HMS%\\\\bin\\\\gdal;%PATH%\"\n",
    "set \"GDAL_DRIVER_PATH=%HMS%\\\\bin\\\\gdal\\\\gdalplugins\"\n",
    "set \"GDAL_DATA=%HMS%\\\\bin\\\\gdal\\\\gdal-data\"\n",
    "set \"PROJ_LIB=%HMS%\\\\bin\\\\gdal\\\\projlib\"\n",
    "\n",
    "set \"CLASSPATH=%HMS%\\\\hms.jar;%HMS%\\\\lib\\\\*\"\n",
    "C:\\\\jython2.7.3\\\\bin\\\\jython.exe -J-Xms{jython_initial_heap_size} -J-Xmx{jython_maximum_heap_size} -Djava.library.path=\"%HMS%\\\\bin;%HMS%\\\\bin\\\\gdal\" {hms_compute_py_path} %1 %2\n",
    "'''\n",
    "\n",
    "# Write the content for HMScompute.py\n",
    "with open(hms_compute_py_path, 'w') as py_file:\n",
    "    py_file.write(hms_compute_py_content)\n",
    "print(f\"Wrote content to {hms_compute_py_path}\")\n",
    "\n",
    "\n",
    "# Write the content for HMScompute.bat\n",
    "with open(hms_compute_bat_path, 'w') as bat_file:\n",
    "    bat_file.write(hms_compute_bat_content)\n",
    "print(f\"Wrote content to {hms_compute_bat_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logic to determine HMS Project Name\n",
    "\n",
    "# Get HMS Project Name from hms_project_directory\n",
    "def get_hms_project_name(hms_project_directory):\n",
    "    \"\"\"\n",
    "    Given an HMS project directory, return the HMS project name.\n",
    "    The project name is determined based on the .hms file in the directory.\n",
    "    If more than one .hms file exists or none exist, an appropriate error message is returned.\n",
    "\n",
    "    Args:\n",
    "    - hms_project_directory (str): Path to the HMS project directory\n",
    "\n",
    "    Returns:\n",
    "    - str: HMS project name or error message\n",
    "    \"\"\"\n",
    "\n",
    "    # List all files in the directory\n",
    "    all_files = os.listdir(hms_project_directory)\n",
    "\n",
    "    # Filter for files with .hms extension\n",
    "    hms_files = [f for f in all_files if f.endswith('.hms')]\n",
    "\n",
    "    # Check if there's exactly one .hms file\n",
    "    if len(hms_files) == 1:\n",
    "        # Extract the project name by stripping the .hms extension\n",
    "        return hms_files[0].replace('.hms', '')\n",
    "    elif len(hms_files) > 1:\n",
    "        return \"Error: More than one .hms file exists in the directory!\"\n",
    "    else:\n",
    "        return \"Error: No .hms file found in the directory!\"\n",
    "\n",
    "# Print Project Name or Error\n",
    "hms_project_name = get_hms_project_name(hms_project_directory)\n",
    "print(\"HMS Project Name: \", hms_project_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and Print necessary paths and file names\n",
    "\n",
    "# Calculated Paths and File Names (.run, .basin, backup paths)\n",
    "user_calibration_runs_csv_fullpath = os.path.join(hms_project_directory, user_calibration_runs_csv_filename)\n",
    "hms_project_run_file = f\"{hms_project_name}.run\"\n",
    "hms_project_run_file_path = os.path.join(hms_project_directory, hms_project_run_file)\n",
    "hms_grid_file = f\"{hms_project_name}.grid\"\n",
    "hms_basin_file_path = os.path.join(hms_project_directory, hms_basin_file)\n",
    "hms_project_run_file_backup_path = hms_project_run_file_path + \".bak\"\n",
    "hms_basin_file_backup_path = hms_basin_file_path + \".bak\"\n",
    "\n",
    "hms_grid_file_path = os.path.join(hms_project_directory, hms_grid_file)\n",
    "hms_grid_file_backup_path = hms_grid_file_path + \".bak\"\n",
    "\n",
    "# Print statements\n",
    "print(\"HMS User Calibration Runs CSV: \", user_calibration_runs_csv_fullpath)\n",
    "print(\"HMS Project Run File:\", hms_project_run_file)\n",
    "print(\"HMS Project Run File Path:\", hms_project_run_file_path)\n",
    "print(\"HMS Basin File:\", hms_basin_file)\n",
    "print(\"HMS Basin File Path:\", hms_basin_file_path)\n",
    "print(\"HMS Project Run File Backup Path:\", hms_project_run_file_backup_path)\n",
    "print(\"HMS Basin File Backup Path:\", hms_basin_file_backup_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Lock File and Logic for Working File Backup and Restore\n",
    "# Define the path for the lock file based on one of the .bak file paths\n",
    "lock_file_directory = os.path.dirname(hms_project_run_file_backup_path)\n",
    "lock_file_path = os.path.join(lock_file_directory, \"HMSCommander.lock\")\n",
    "\n",
    "# Step 1: Check if HMSCommander.lock exists\n",
    "if os.path.exists(lock_file_path):\n",
    "    print(\"Lock file exists. Restoring files from .bak backup.\")\n",
    "\n",
    "    # Step 2: Restore files from backup\n",
    "    for backup, original in [(hms_project_run_file_backup_path, hms_project_run_file_path),\n",
    "                             (hms_basin_file_backup_path, hms_basin_file_path),\n",
    "                             (hms_grid_file_backup_path, hms_grid_file_path)]:\n",
    "        try:\n",
    "            if os.path.exists(backup):\n",
    "                shutil.copy(backup, original)\n",
    "                # Delete the .bak file\n",
    "                os.remove(backup)\n",
    "                print(f\"Restored and deleted file {backup}\")\n",
    "            else:\n",
    "                print(f\"Backup file {backup} does not exist. Skipping restore.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error restoring file from {backup} to {original}: {e}\")\n",
    "\n",
    "    # Step 3: Delete the lock file\n",
    "    os.remove(lock_file_path)\n",
    "else:\n",
    "    print(\"Lock file does not exist. Proceeding to backup files.\")\n",
    "\n",
    "# Step 4: Backup files\n",
    "for original, backup in [(hms_project_run_file_path, hms_project_run_file_backup_path),\n",
    "                         (hms_basin_file_path, hms_basin_file_backup_path),\n",
    "                         (hms_grid_file_path, hms_grid_file_backup_path)]:\n",
    "    try:\n",
    "        if os.path.exists(original):\n",
    "            shutil.copy(original, backup)\n",
    "            print(f\"Backed up file from {original} to {backup}\")\n",
    "        else:\n",
    "            print(f\"Original file {original} does not exist. Skipping backup.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error backing up file from {original} to {backup}: {e}\")\n",
    "\n",
    "# Step 5: Create a new HMSCommander.lock file\n",
    "with open(lock_file_path, 'w') as f:\n",
    "    f.write(\"Lock file for HMS Commander operations.\")\n",
    "\n",
    "print(\"Lock file created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read run parameters from CSV file\n",
    "user_calibration_df = pd.read_csv(user_calibration_runs_csv_fullpath, dtype={'user_run_number_from_csv': int})\n",
    "\n",
    "def load_user_calibration_csv_data(file_path):\n",
    "    user_calibration_df = pd.read_csv(file_path, dtype={'user_run_number_from_csv': int})  # Directly reading csv using pandas to return DataFrame\n",
    "    return user_calibration_df # Show DataFrame\n",
    "\n",
    "load_user_calibration_csv_data(user_calibration_runs_csv_fullpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all necessary functions\n",
    "\n",
    "# Reads the content of the HMS basin file\n",
    "def read_basin_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "    return content\n",
    "\n",
    "\n",
    "#Updates the grid file with the new DSS file path based on the impervious area scale.\n",
    "def update_impervious_grid_definitions(grid_file_path, grid_def, dss_file_path, impervious_area_scale):\n",
    "    \n",
    "    with open(grid_file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    grid_def_found = False\n",
    "    for i, line in enumerate(lines):\n",
    "        if f\"Grid: {grid_def}\" in line:\n",
    "            grid_def_found = True\n",
    "            print(\"Grid Definition Found!\")\n",
    "        if grid_def_found and \"DSS File Name:\" in line:\n",
    "            old_dss_file_name = line.split(\": \")[1].strip()\n",
    "            new_dss_file_name = \"data\\\\\" + dss_file_path.split(\"\\\\\")[-1]\n",
    "            lines[i] = line.replace(old_dss_file_name, new_dss_file_name)\n",
    "            print(\"DSS FileName Replaced!\")\n",
    "            break\n",
    "\n",
    "    with open(grid_file_path, 'w') as file:\n",
    "        file.writelines(lines)\n",
    "\n",
    "\n",
    "# Optional: pauses the script execution until the user provides input.\n",
    "def pause_script():\n",
    "    \n",
    "    user_input = input(\"Press Enter to continue or 'q' to quit: \")\n",
    "    if user_input.lower() == 'q':\n",
    "        exit()\n",
    "\n",
    "\n",
    "# Optional: print the first 60 lines of a file\n",
    "def print_first_60_lines(file_path):\n",
    "    \"\"\"Prints the first 60 lines of a specified file.\"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        for i, line in enumerate(lines):\n",
    "            if i < 60:\n",
    "                print(line.strip())\n",
    "            else:\n",
    "                break\n",
    "\n",
    "\n",
    "# Loads the user calibration data from the specified file path into a pandas dataframe.\n",
    "def load_subbasin_tc_data_to_pandas_dataframe(filepath):\n",
    "    \"\"\"Loads subbasin time of concentration and storage coefficient data into a pandas dataframe.\"\"\"\n",
    "    with open(filepath, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    subbasin_data = []\n",
    "    current_subbasin = None\n",
    "    toc = None\n",
    "    sc = None\n",
    "\n",
    "    for line in lines:\n",
    "        if 'Subbasin:' in line:\n",
    "            current_subbasin = line.split(': ')[1].strip()\n",
    "        elif 'Time of Concentration:' in line:\n",
    "            toc = float(line.split(': ')[1].strip())\n",
    "        elif 'Storage Coefficient:' in line:\n",
    "            sc = float(line.split(': ')[1].strip())\n",
    "        elif 'End:' in line:\n",
    "            if current_subbasin and toc and sc:\n",
    "                subbasin_data.append([current_subbasin, toc, sc])\n",
    "                current_subbasin, toc, sc = None, None, None\n",
    "\n",
    "    subbasin_tc_r = pd.DataFrame(subbasin_data, columns=['subbasin_name', 'time_of_concentration', 'storage_coefficient'])\n",
    "    return subbasin_tc_r\n",
    "\n",
    "# Loads the user calibration data for the current run number into a pandas dataframe.\n",
    "def fetch_calibration_parameters_for_current_run(user_calibration_df, current_run_number):\n",
    "    \"\"\"Fetches the calibration parameters for a specified run number.\"\"\"\n",
    "    current_calibration = user_calibration_df[user_calibration_df['user_run_number_from_csv'] == current_run_number]\n",
    "    if not current_calibration.empty:\n",
    "        return current_calibration.iloc[0]  \n",
    "    else:\n",
    "        print(\"No match found for current run number in user calibration data.\")\n",
    "        return None  # Return None if no matching calibration is found\n",
    "\n",
    "\n",
    "# Scales the time of concentration and storage coefficient data using the provided calibration parameters.\n",
    "def scale_tcr_by_run_scale_factor(subbasin_tc_df, calibration_parameters):\n",
    "    \"\"\"Scales the time of concentration and storage coefficient data using the provided calibration parameters.\"\"\"\n",
    "    if calibration_parameters is not None:\n",
    "        subbasin_tc_df['time_of_concentration_scaled'] = (subbasin_tc_df['time_of_concentration'] * calibration_parameters['time_of_concentration_scale']).round(2)\n",
    "        subbasin_tc_df['storage_coefficient_scaled'] = (subbasin_tc_df['storage_coefficient'] * calibration_parameters['storage_coefficient_scale']).round(2)\n",
    "        return subbasin_tc_df\n",
    "    else:\n",
    "        print(\"No calibration parameters provided. Unable to scale data.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "# Writes the scaled time of concentration and storage coefficient data back to the basin file.\n",
    "def write_subbasin_tc_data_to_basin_file(filepath, subbasin_tc_r):\n",
    "    \"\"\"Writes the scaled time of concentration and storage coefficient data back to the basin file.\"\"\"\n",
    "    with open(filepath, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    new_lines = []\n",
    "    subbasin_counter = 0\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line.startswith('Subbasin: '):\n",
    "            current_subbasin = line.split(': ')[1]\n",
    "            \n",
    "            # Increase the subbasin counter\n",
    "            subbasin_counter += 1\n",
    "            \n",
    "            # Print output only for the first 2 subbasins\n",
    "            if subbasin_counter <= 2:\n",
    "                print(f'Processing subbasin: {current_subbasin}')\n",
    "        elif line.startswith('Time of Concentration: '):\n",
    "            old_toc = line.split(': ')[1]\n",
    "            toc_scale = subbasin_tc_r.loc[subbasin_tc_r['subbasin_name'] == current_subbasin, 'time_of_concentration_scaled'].item()\n",
    "            line = f\"Time of Concentration: {toc_scale}\"\n",
    "            \n",
    "            # Print output only for the first 5 subbasins\n",
    "            if subbasin_counter <= 5:\n",
    "                print(f'For subbasin {current_subbasin}, Time of Concentration was changed from {old_toc} to {toc_scale}')\n",
    "        elif line.startswith('Storage Coefficient: '):\n",
    "            old_sc = line.split(': ')[1]\n",
    "            sc_scale = subbasin_tc_r.loc[subbasin_tc_r['subbasin_name'] == current_subbasin, 'storage_coefficient_scaled'].item()\n",
    "            line = f\"Storage Coefficient: {sc_scale}\"\n",
    "            \n",
    "            # Print output only for the first 5 subbasins\n",
    "            if subbasin_counter <= 5:\n",
    "                print(f'For subbasin {current_subbasin}, Storage Coefficient was changed from {old_sc} to {sc_scale}')\n",
    "        new_lines.append(line + '\\n')\n",
    "    \n",
    "    with open(filepath, 'w') as file:\n",
    "        print(\"Writing TC and R to .basin file\")\n",
    "        file.writelines(new_lines)\n",
    "\n",
    "\n",
    "#Finds the path of the Impervious DSS file based on the impervious area scale.\n",
    "def find_impervious_dss_grids(directory, impervious_area_scale):\n",
    "    \n",
    "    scale_factor_to_filename = {\n",
    "        1.2: \"Impervious_1.2_SF.dss\",\n",
    "        1.4: \"Impervious_1.4_SF.dss\",\n",
    "        1.6: \"Impervious_1.6_SF.dss\",\n",
    "        1.8: \"Impervious_1.8_SF.dss\",\n",
    "        2.0: \"Impervious_2.0_SF.dss\",\n",
    "    }\n",
    "    filename = scale_factor_to_filename.get(impervious_area_scale)\n",
    "    if filename:\n",
    "        full_path = os.path.join(directory, \"data\", filename)\n",
    "        if os.path.isfile(full_path):\n",
    "            return full_path\n",
    "        else:\n",
    "            print(f\"Error: File '{filename}' cannot be found in the directory '{directory}/data'.\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"Error: No corresponding dss file found for the impervious area scale '{impervious_area_scale}'.\")\n",
    "        return None\n",
    "\n",
    "# Updates the basin file with new soil and baseflow parameters\n",
    "def process_soil_and_baseflow_to_basin_file(file_path, values):\n",
    "    # Your original search_replace dictionary\n",
    "    search_replace = {\n",
    "        r\"Initial Deficit Scale:\\s*([\\d.]+)\": f\"Initial Deficit Scale: {values['initial_deficit_scale']}\",\n",
    "        r\"Maximum Deficit Scale:\\s*([\\d.]+)\": f\"Maximum Deficit Scale: {values['maximum_deficit_scale']}\",\n",
    "        r\"Percolation Rate Scale:\\s*([\\d.]+)\": f\"Percolation Rate Scale: {values['percolation_rate_scale']}\",\n",
    "        r\"Impervious Area Scale:\\s*([\\d.]+)\": f\"Impervious Area Scale: {values['impervious_area_scale']}\" if values['impervious_area_scale'] <= 1.0 else f\"Impervious Area Scale: 1.0\",\n",
    "        r\"Recession Factor:\\s*([\\d.]+)\": f\"Recession Factor: {values['recession_factor']}\",\n",
    "        r\"Initial Flow/Area Ratio:\\s*([\\d.]+)\": f\"Initial Flow/Area Ratio: {values['initial_flow_area_ratio']}\",\n",
    "        r\"Threshold Flow to Peak Ratio:\\s*([\\d.]+)\": f\"Threshold Flow to Peak Ratio: {values['threshold_flow_to_peak_ratio']}\"\n",
    "    }\n",
    "\n",
    "    # Read and modify the file line-by-line\n",
    "    with open(file_path, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    modified_lines = []\n",
    "    for line in lines:\n",
    "        modified_line = line\n",
    "        for pattern, replacement in search_replace.items():\n",
    "            match = re.search(pattern, line, flags=re.IGNORECASE)\n",
    "            if match:\n",
    "                old_value = match.group(1)\n",
    "                modified_line = re.sub(pattern, replacement, line, flags=re.IGNORECASE)\n",
    "                print(f'Replacing {pattern} in {file_path} from {old_value} to {replacement.split(\": \")[1]}')\n",
    "                break\n",
    "        modified_lines.append(modified_line)\n",
    "\n",
    "    # Write the modified content back to the file\n",
    "    with open(file_path, \"w\") as file:\n",
    "        file.writelines(modified_lines)\n",
    "\n",
    "def update_baseflow_recession():\n",
    "    \"\"\"\n",
    "    Updates the baseflow in hms_basin_file_data to Recession  Baseflow Method if set to \"None\".\n",
    "\n",
    "    Parameters:\n",
    "    - hms_basin_file_data: The initial basin file data\n",
    "    - run: Dictionary containing the parameters 'recession_factor', 'initial_flow_area_ratio', and 'threshold_flow_to_peak_ratio'\n",
    "    - hms_basin_file_path: Path to the basin file\n",
    "    \n",
    "    Returns:\n",
    "    - Modified hms_basin_file_data\n",
    "    \"\"\"\n",
    "    with open(hms_basin_file_path, 'r') as file:\n",
    "        hms_basin_file_data = file.read()\n",
    "\n",
    "    # Fill in Recession Baseflow if set as \"None\" and add required variables\n",
    "    baseflow_none_text = \"Baseflow: None\"\n",
    "    if baseflow_none_text in hms_basin_file_data:\n",
    "        baseflow_text = \"Baseflow: Recession\\n Recession Factor: {}\\n Initial Flow/Area Ratio: {}\\n Threshold Flow to Peak Ratio: {}\"\n",
    "        baseflow_text = baseflow_text.format(run['recession_factor'], run['initial_flow_area_ratio'], run['threshold_flow_to_peak_ratio'])\n",
    "        hms_basin_file_data = hms_basin_file_data.replace(baseflow_none_text, baseflow_text)\n",
    "        print(\"Baseflow Set to None.  Inserting Recession Baseflow Parameters\")  # Indicate that Baseflow data was filled\n",
    "\n",
    "    with open(hms_basin_file_path, 'w') as file:\n",
    "        file.write(hms_basin_file_data)\n",
    "    return hms_basin_file_data\n",
    "\n",
    "\n",
    "\n",
    "def modify_canopy_method(file_path):\n",
    "    # Initialize an empty list to hold the lines of the file\n",
    "    modified_lines = []\n",
    "\n",
    "    # Read the file\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "\n",
    "    # If the flag is True, perform the replacement\n",
    "\n",
    "    lines_to_replace = '''\\\n",
    "    Canopy: None\n",
    "    Allow Simultaneous Precip Et: No\n",
    "    Plant Uptake Method: None\n",
    "'''\n",
    "    lines_to_insert = '''\\\n",
    "    Canopy: Simple\n",
    "    Allow Simultaneous Precip Et: No\n",
    "    Plant Uptake Method: None\n",
    "    Initial Canopy Storage Percent: 1\n",
    "    Canopy Storage Capacity: 0\n",
    "    Crop Coefficient: 1.0\n",
    "    End Canopy:\n",
    "'''\n",
    "\n",
    "    # Replace occurrences\n",
    "    joined_lines = ''.join(lines)\n",
    "    modified_content = joined_lines.replace(lines_to_replace, lines_to_insert)\n",
    "\n",
    "    # Write back the modified content to the file\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(modified_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Logic of Script to Run HMS for each Event and Calibration Run\n",
    "\n",
    "\n",
    "# For each run_name in hms_run_names:\n",
    "for run_name in hms_run_names:\n",
    "    hms_run_name = run_name  # Set the current run name\n",
    "    user_calibration_df['user_run_number_from_csv'] = user_calibration_df['user_run_number_from_csv'].astype(int)\n",
    "\n",
    "    # For each calibration run, the script will:\n",
    "    for _, run in user_calibration_df.iterrows():\n",
    "        \n",
    "        run = run.rename({k: k.lower() for k in run.index})\n",
    "        current_run_number = int(run['user_run_number_from_csv'])\n",
    "\n",
    "        print(f\"Processing run number: {current_run_number}\")  # Indicate which run is being processed\n",
    "\n",
    "        hms_run_output_dss = f\"{hms_run_name}_HMS_Run_{current_run_number}{hms_dss_suffix}.dss\"\n",
    "        hms_run_output_dss_path = os.path.join(hms_project_directory, f\"{hms_run_output_dss}\")\n",
    "\n",
    "        # Backup the .grid file\n",
    "        shutil.copy(hms_grid_file_path, hms_grid_file_backup_path)\n",
    "        print(\"Backup of .grid file created.\")  # Indicate that backup of .grid file has been made\n",
    "\n",
    "        if not os.path.isfile(hms_run_output_dss_path):\n",
    "            print(\"Output DSS file does not exist. Preparing HMS Run.\")  \n",
    "\n",
    "            # Update the .run file with the new DSS file name\n",
    "            with open(hms_project_run_file_path, 'r') as file:\n",
    "                run_data = file.readlines()\n",
    "\n",
    "            run_found = False\n",
    "            for i, line in enumerate(run_data):\n",
    "                if f\"Run: {hms_run_name}\" in line:\n",
    "                    run_found = True\n",
    "                if \"End:\" in line:\n",
    "                    run_found = False\n",
    "                if run_found and line.strip().startswith(\"DSS File:\"):\n",
    "                    old_dss_file_name = line.split(\":\")[1].strip()\n",
    "                    run_data[i] = line.replace(old_dss_file_name, hms_run_output_dss)\n",
    "            print(f\"Output DSS file name changed from {old_dss_file_name} to {hms_run_output_dss} in .run file.\")\n",
    "\n",
    "            # Write the updated .run data\n",
    "            with open(hms_project_run_file_path, 'w') as file:\n",
    "                file.writelines(run_data)\n",
    "\n",
    "            # Load subbasin TC data and scale it\n",
    "            subbasin_tc_r = load_subbasin_tc_data_to_pandas_dataframe(hms_basin_file_path)\n",
    "            calibration_parameters = fetch_calibration_parameters_for_current_run(user_calibration_df, current_run_number)\n",
    "            scaled_subbasin_tc_data = scale_tcr_by_run_scale_factor(subbasin_tc_r, calibration_parameters)\n",
    "\n",
    "            if not scaled_subbasin_tc_data.empty:\n",
    "                # Write back scaled 'time_of_concentration' and 'storage_coefficient' to the .basin file\n",
    "                write_subbasin_tc_data_to_basin_file(hms_basin_file_path, scaled_subbasin_tc_data)\n",
    "\n",
    "            # The main logic to check the condition before updating baseflow recession parameters\n",
    "            if hms_recession_baseflow and Baseflow_Method_Set_to_Recession == \"Yes\":\n",
    "                # Assuming you have already defined and set the variables hms_basin_file_data, run, and hms_basin_file_path\n",
    "                update_baseflow_recession()\n",
    "            else:\n",
    "                print(\"Baseflow already set, or Baseflow_Method_Set_to_Recession is not set to 'Yes'. No changes made.\")\n",
    "\n",
    "            # Update the .basin file with the new soil and recession baseflow parameters \n",
    "            process_soil_and_baseflow_to_basin_file(hms_basin_file_path, run)\n",
    "\n",
    "            # Update Canopy Method condition is met, then modify the file\n",
    "            if Canopy_Method_Set_To_Simple_Zero == \"Yes\":\n",
    "                modify_canopy_method(hms_basin_file_path)\n",
    "\n",
    "            # Check if Impervious Area Scale is greater than 1 and update the .grid file if necessary\n",
    "            if run['impervious_area_scale'] > 1:\n",
    "                dss_file_path = find_impervious_dss_grids(hms_project_directory, run['impervious_area_scale'])\n",
    "                if dss_file_path:\n",
    "                    update_impervious_grid_definitions(hms_grid_file_path, \"2019_Percent_Impervious\", dss_file_path, run['impervious_area_scale'])\n",
    "                else:\n",
    "                    # Restore files from backup and exit the script if the appropriate DSS file is not found\n",
    "                    shutil.copy(hms_project_run_file_backup_path, hms_project_run_file_path)\n",
    "                    shutil.copy(hms_basin_file_backup_path, hms_basin_file_path)\n",
    "                    shutil.copy(hms_grid_file_backup_path, hms_grid_file_path)\n",
    "                    print(\"Impervious DSS File Not Found. Restoring files and Exiting.\")\n",
    "                    exit()\n",
    "\n",
    "            # Pause then execute HMS\n",
    "            time.sleep(5)\n",
    "\n",
    "            cmd = ['cmd', '/c', hms_compute_bat_path, os.path.join(hms_project_directory, hms_project_name + '.hms'), hms_run_name]\n",
    "            print(f\"Executing HMS with command: {' '.join(cmd)}\")\n",
    "\n",
    "            # Execute HMS\n",
    "            subprocess.run(cmd)\n",
    "            \n",
    "            # HEC-HMS creates a log file with the hms_run_name with a .log extension\n",
    "            # To avoid overwriting the log file, we rename it to include the run number and the suffix (matching the DSS file name with a .log extension)\n",
    "            logfile_path = os.path.join(hms_project_directory, hms_run_output_dss.replace('.dss', '.log'))\n",
    "\n",
    "            # Remove existing log file, if it exists\n",
    "            if os.path.exists(logfile_path):\n",
    "                os.remove(logfile_path)\n",
    "\n",
    "            # Now rename the default HMS log file to match DSS file naming convention\n",
    "            os.rename(os.path.join(hms_project_directory, hms_run_name + '.log'), logfile_path) if os.path.exists(os.path.join(hms_project_directory, hms_run_name + '.log')) else print(f\"The file {os.path.join(hms_project_directory, hms_run_name + '.log')} does not exist.\")\n",
    "            print(f\"Renamed log file to {logfile_path}\")\n",
    "            \n",
    "                \n",
    "            # Restore original HMS files\n",
    "            shutil.copy(hms_basin_file_backup_path, hms_basin_file_path)\n",
    "            print(f\"Restored {hms_basin_file_backup_path} to {hms_basin_file_path}\")\n",
    "            shutil.copy(hms_grid_file_backup_path, hms_grid_file_path)\n",
    "            print(f\"Restored {hms_grid_file_backup_path} to {hms_grid_file_path}\")\n",
    "            print(\"\")\n",
    "        else:\n",
    "            print(f\"Run Output DSS file for Run {current_run_number} already exists.\")\n",
    "            print(\"\")\n",
    "\n",
    "    print (\"All Runs Completed\")\n",
    "\n",
    "    # Delete the lock file for file backup and restoration\n",
    "    if os.path.exists(lock_file_path):\n",
    "        os.remove(lock_file_path)\n",
    "        print(\"Lock file deleted.\")\n",
    "    else:\n",
    "        print(\"Lock file does not exist.\")\n",
    "\n",
    "        \n",
    "    print(f\"All Runs Completed for {run_name}\")\n",
    "    \n",
    "print (\"All Events Processed\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AutoHMS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
