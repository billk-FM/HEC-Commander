{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HMS-Commander (with Calibration Regions)\n",
    "Author: William (Bill) Katzenmeyer, P.E., C.F.M. (C.H. Fenstermaker and Associates, LLC)\n",
    "\n",
    "Source: https://github.com/billk-FM/HEC-Commander-\n",
    "\n",
    "Changelog: \n",
    "2023-11-11 Raises error if hms_run_name not present in .run file\n",
    "2024-01-30 User notification when restoring from backup file (to prevent overwrite loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User Defined Inputs\n",
    "\n",
    "# Provide the HMS Project Directory and Project Name\n",
    "hms_project_directory = r\"C:\\Your_HMS_Project_Directory\"\n",
    "hms_basin_file = \"Your_HMS_Basin.basin\"\n",
    "\n",
    "# Define Calibration Runs CSV Filename (place in HMS Project Directory)\n",
    "user_calibration_runs_csv_filename = \"regions.csv\"\n",
    "\n",
    "hms_run_names = [\n",
    "    \"Your_Run_Name_1\",\n",
    "]\n",
    "# If multiple run names are provided, each will generate a full set of user-defined runs\n",
    "\n",
    "#Set a DSS output file suffix to differentiate run sets\n",
    "hms_dss_suffix = \"_Batch_Name\"  # define your suffix here\n",
    "print(\"DSS Suffix: \" + hms_dss_suffix)\n",
    "\n",
    "# Load the calibration shapefile\n",
    "# To Create, export subbasins file from HEC-HMS, load in RASMapper, and merge subbasins.  \n",
    "# Then, add \"calregion\" column and number calibration regions (1,2,3,etc.)\n",
    "calibration_shapefile_path = 'C:\\Calibration_Regions.shp' \n",
    "\n",
    "# Override Baseflow:None to Baseflow: Recession\n",
    "hms_recession_baseflow = True\n",
    "\n",
    "# Override Baseflow None to Baseflow Recession\n",
    "Baseflow_Method_Set_to_Recession = \"Yes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Additional Settings ----\n",
    "\n",
    "# Define the path to the HEC-HMS executable\n",
    "hms_executable_path = r\"C:\\Program Files\\HEC\\HEC-HMS\\4.9\"\n",
    "print(\"HEC-HMS Executable Path: \" + hms_executable_path)\n",
    "\n",
    "# Jython Installation Path\n",
    "jython_path = r\"C:\\jython2.7.3\"\n",
    "# This MUST match your Jython installation path or HMS will not run\n",
    "\n",
    "\n",
    "# Define paths for HMScompute.py and HMScompute.bat which are used to run HMS through Jython\n",
    "hms_compute_py_path = r\"C:\\jython2.7.3\\HMScompute.py\"\n",
    "hms_compute_bat_path = r\"C:\\jython2.7.3\\HMScompute.bat\"\n",
    "\n",
    "''' If jython heap size is too small, the HMS will freeze at 100% CPU and never finish'''\n",
    "jython_initial_heap_size = \"256m\"       #initial heap size for java virtual machine\n",
    "print(\"Jython Initial Heap Size: \" + jython_initial_heap_size)\n",
    "jython_maximum_heap_size = \"4096m\"      #maximum heap size for java virtual machine\n",
    "print(\"Jython Maximum Heap Size: \" + jython_maximum_heap_size)\n",
    "\n",
    "# Set Canopy Method to Simple Zero to avoid warnings (Deprecated, not needed after HMS 4.9)\n",
    "Canopy_Method_Set_To_Simple_Zero = \"No\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install/Import Required Python Packages\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "\n",
    "# List of packages you want to ensure are installed\n",
    "packages = [\"os\", \"shutil\", \"pandas\", \"geopandas\", \"subprocess\", \"re\", \"csv\", \"itertools\"]\n",
    "\n",
    "\n",
    "# Logic to Install Packages\n",
    "def install(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "    print(\"Installed \" + package + \" successfully\")\n",
    "\n",
    "for package in packages:\n",
    "    try:\n",
    "        # If the import succeeds, the package is installed, so we can move on\n",
    "        __import__(package)\n",
    "    except ImportError:\n",
    "        # If the import fails, the package is not installed and we try to install it\n",
    "        install(package)\n",
    "\n",
    "\n",
    "# Import All Required Libraries for Script\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import itertools\n",
    "import re\n",
    "import csv\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from regex import D\n",
    "\n",
    "\n",
    "# Check the Python version\n",
    "python_version = sys.version_info\n",
    "\n",
    "# Provide feedback if earlier Python version than 3.11 is used.\n",
    "if python_version < (3, 11):\n",
    "    raise SystemError(\"HMS-Commander's Calibration Regions relies on Geopandas and \"\n",
    "                      \"may not work with python environments lower than 3.11\")\n",
    "else:\n",
    "    print(f\"Python version {python_version.major}.{python_version.minor}.{python_version.micro} is supported.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for Jython 2.7.3 at C:\\jython2.7.3\n",
    "\n",
    "jython_jar_path = os.path.join(jython_path, \"jython.jar\")\n",
    "\n",
    "def jython_exists(path):\n",
    "    exists = os.path.exists(path)\n",
    "    return exists\n",
    "\n",
    "def print_installation_instructions():\n",
    "    print(\"Jython is not found at the provided path.\")\n",
    "    print(\"Please install the necessary software:\")\n",
    "    print(\"1. Java SE Development Kit (check for the latest version): https://www.oracle.com/java/technologies/javase-jdk-downloads.html\")\n",
    "    print(\"   For HEC-HMS 4.9, use jdk-20.0.1  https://download.oracle.com/java/20/archive/jdk-20.0.1_windows-x64_bin.msi\")\n",
    "    print(\"2. Jython2.7.3: https://www.jython.org/download.html\")\n",
    "    print(\"   Direct Link: https://repo1.maven.org/maven2/org/python/jython-installer/2.7.3/jython-installer-2.7.3.jar\")\n",
    "    print(\"Be sure to install jython to the default location: C:\\jython2.7.3 or change the jython_path variable in this script.\")\n",
    "\n",
    "if jython_exists(jython_jar_path):\n",
    "    print(f\"Jython Exists at {jython_jar_path}\")\n",
    "else:\n",
    "    print_installation_instructions()\n",
    "    raise FileNotFoundError(f\"Jython not found at path: {jython_jar_path}\")\n",
    "\n",
    "# USER MUST INSTALL JYTHON 2.7.3 TO C:\\jython2.7.3 OR CHANGE PATH IN THIS SCRIPT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build HMScompute.bat and HMScompute.py for Jython 2.7.3\n",
    "\n",
    "# Define the path and content for HMScompute.py\n",
    "hms_compute_py_content = '''from hms.model import Project\n",
    "from hms import Hms\n",
    "\n",
    "import glob\n",
    "import sys\n",
    "\n",
    "hmspth = sys.argv[1]\n",
    "runName = sys.argv[2]\n",
    "# print 'running' + str(hmspth)\n",
    "\n",
    "myProject = Project.open(hmspth)\n",
    "myProject.computeRun(runName)\n",
    "myProject.close()\n",
    "\n",
    "Hms.shutdownEngine()\n",
    "'''\n",
    "\n",
    "# Define the path and content for HMScompute.bat\n",
    "hms_compute_bat_content = f'''set \"HMS={hms_executable_path}\"\n",
    "set \"PATH=%HMS%\\\\bin\\\\gdal;%PATH%\"\n",
    "set \"GDAL_DRIVER_PATH=%HMS%\\\\bin\\\\gdal\\\\gdalplugins\"\n",
    "set \"GDAL_DATA=%HMS%\\\\bin\\\\gdal\\\\gdal-data\"\n",
    "set \"PROJ_LIB=%HMS%\\\\bin\\\\gdal\\\\projlib\"\n",
    "\n",
    "set \"CLASSPATH=%HMS%\\\\hms.jar;%HMS%\\\\lib\\\\*\"\n",
    "C:\\\\jython2.7.3\\\\bin\\\\jython.exe -J-Xms{jython_initial_heap_size} -J-Xmx{jython_maximum_heap_size} -Djava.library.path=\"%HMS%\\\\bin;%HMS%\\\\bin\\\\gdal\" {hms_compute_py_path} %1 %2\n",
    "'''\n",
    "\n",
    "# Write the content for HMScompute.py\n",
    "with open(hms_compute_py_path, 'w') as py_file:\n",
    "    py_file.write(hms_compute_py_content)\n",
    "print(f\"Wrote content to {hms_compute_py_path}\")\n",
    "\n",
    "\n",
    "# Write the content for HMScompute.bat\n",
    "with open(hms_compute_bat_path, 'w') as bat_file:\n",
    "    bat_file.write(hms_compute_bat_content)\n",
    "print(f\"Wrote content to {hms_compute_bat_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logic to determine HMS Project Name and Required Paths\n",
    "\n",
    "# Get HMS Project Name from hms_project_directory\n",
    "def get_hms_project_name(hms_project_directory):\n",
    "    \"\"\"\n",
    "    Given an HMS project directory, return the HMS project name.\n",
    "    The project name is determined based on the .hms file in the directory.\n",
    "    If more than one .hms file exists or none exist, an appropriate error message is returned.\n",
    "\n",
    "    Args:\n",
    "    - hms_project_directory (str): Path to the HMS project directory\n",
    "\n",
    "    Returns:\n",
    "    - str: HMS project name or error message\n",
    "    \"\"\"\n",
    "\n",
    "    # List all files in the directory\n",
    "    all_files = os.listdir(hms_project_directory)\n",
    "\n",
    "    # Filter for files with .hms extension\n",
    "    hms_files = [f for f in all_files if f.endswith('.hms')]\n",
    "\n",
    "    # Check if there's exactly one .hms file\n",
    "    if len(hms_files) == 1:\n",
    "        # Extract the project name by stripping the .hms extension\n",
    "        return hms_files[0].replace('.hms', '')\n",
    "    elif len(hms_files) > 1:\n",
    "        return \"Error: More than one .hms file exists in the directory!\"\n",
    "    else:\n",
    "        return \"Error: No .hms file found in the directory!\"\n",
    "\n",
    "# Print Project Name or Error\n",
    "hms_project_name = get_hms_project_name(hms_project_directory)\n",
    "print(\"HMS Project Name: \", hms_project_name)\n",
    "\n",
    "\n",
    "# Calculate and Print necessary paths and file names\n",
    "\n",
    "# Calculated Paths and File Names (.run, .basin, backup paths)\n",
    "user_calibration_runs_csv_fullpath = os.path.join(hms_project_directory, user_calibration_runs_csv_filename)\n",
    "hms_project_run_file = f\"{hms_project_name}.run\"\n",
    "hms_project_run_file_path = os.path.join(hms_project_directory, hms_project_run_file)\n",
    "hms_grid_file = f\"{hms_project_name}.grid\"\n",
    "hms_basin_file_path = os.path.join(hms_project_directory, hms_basin_file)\n",
    "hms_project_run_file_backup_path = hms_project_run_file_path + \".bak\"\n",
    "hms_basin_file_backup_path = hms_basin_file_path + \".bak\"\n",
    "\n",
    "hms_grid_file_path = os.path.join(hms_project_directory, hms_grid_file)\n",
    "hms_grid_file_backup_path = hms_grid_file_path + \".bak\"\n",
    "\n",
    "# Print statements\n",
    "print(\"HMS User Calibration Runs CSV: \", user_calibration_runs_csv_fullpath)\n",
    "print(\"HMS Project Run File:\", hms_project_run_file)\n",
    "print(\"HMS Project Run File Path:\", hms_project_run_file_path)\n",
    "print(\"HMS Basin File:\", hms_basin_file)\n",
    "print(\"HMS Basin File Path:\", hms_basin_file_path)\n",
    "print(\"HMS Project Run File Backup Path:\", hms_project_run_file_backup_path)\n",
    "print(\"HMS Basin File Backup Path:\", hms_basin_file_backup_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Lock File and Logic for Working File Backup and Restore\n",
    "# Define the path for the lock file based on one of the .bak file paths\n",
    "lock_file_directory = os.path.dirname(hms_project_run_file_backup_path)\n",
    "lock_file_path = os.path.join(lock_file_directory, \"HMSCommander.lock\")\n",
    "\n",
    "# Step 1: Check if HMSCommander.lock exists\n",
    "if os.path.exists(lock_file_path):\n",
    "    user_confirmation = input(\"It appears the script did not run successfully and .bak files are still present \"\n",
    "                              \"in the HMS folder. Please confirm that you want to restore the original \"\n",
    "                              \".basin, .run, and .grid files from backup. Type 'yes' or 'y' to confirm, \"\n",
    "                              \"or any other key to cancel: \").lower()\n",
    "\n",
    "    if user_confirmation in ['yes', 'y']:\n",
    "        # Step 2: Restore files from backup\n",
    "        for backup, original in [(hms_project_run_file_backup_path, hms_project_run_file_path),\n",
    "                                 (hms_basin_file_backup_path, hms_basin_file_path),\n",
    "                                 (hms_grid_file_backup_path, hms_grid_file_path)]:\n",
    "            try:\n",
    "                if os.path.exists(backup):\n",
    "                    shutil.copy(backup, original)\n",
    "                    # Delete the .bak file\n",
    "                    os.remove(backup)\n",
    "                    print(f\"Restored and deleted file {backup}\")\n",
    "                else:\n",
    "                    print(f\"Backup file {backup} does not exist. Skipping restore.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error restoring file from {backup} to {original}: {e}\")\n",
    "\n",
    "        # Step 3: Delete the lock file\n",
    "        os.remove(lock_file_path)\n",
    "    else:\n",
    "        print(\"Restoration process cancelled by user. Please inspect the files manually.\")\n",
    "\n",
    "else:\n",
    "    print(\"Lock file does not exist. Proceeding to backup files.\")\n",
    "\n",
    "# Step 4: Backup files\n",
    "for original, backup in [(hms_project_run_file_path, hms_project_run_file_backup_path),\n",
    "                         (hms_basin_file_path, hms_basin_file_backup_path),\n",
    "                         (hms_grid_file_path, hms_grid_file_backup_path)]:\n",
    "    try:\n",
    "        if os.path.exists(original):\n",
    "            shutil.copy(original, backup)\n",
    "            print(f\"Backed up file from {original} to {backup}\")\n",
    "        else:\n",
    "            print(f\"Original file {original} does not exist. Skipping backup.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error backing up file from {original} to {backup}: {e}\")\n",
    "\n",
    "# Step 5: Create a new HMSCommander.lock file\n",
    "with open(lock_file_path, 'w') as f:\n",
    "    f.write(\"Lock file for HMS Commander operations.\")\n",
    "\n",
    "print(\"Lock file created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read run parameters from CSV file\n",
    "user_calibration_df = pd.read_csv(user_calibration_runs_csv_fullpath, dtype={'user_run_number_from_csv': int})\n",
    "\n",
    "def load_user_calibration_csv_data(file_path):\n",
    "    user_calibration_df = pd.read_csv(file_path, dtype={'user_run_number_from_csv': int})  # Directly reading csv using pandas to return DataFrame\n",
    "    return user_calibration_df # Show DataFrame\n",
    "\n",
    "load_user_calibration_csv_data(user_calibration_runs_csv_fullpath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Basin File Prepreocessing Functions (Scale Factors, Recession Baseflow, etc.)\n",
    "\n",
    "def update_baseflow_recession():\n",
    "    \"\"\"\n",
    "    Updates the baseflow in hms_basin_file_data to Recession  Baseflow Method if set to \"None\".\n",
    "\n",
    "    Parameters:\n",
    "    - hms_basin_file_data: The initial basin file data\n",
    "    - run: Dictionary containing the parameters 'recession_factor', 'initial_flow_area_ratio', and 'threshold_flow_to_peak_ratio'\n",
    "    - hms_basin_file_path: Path to the basin file\n",
    "    \n",
    "    Returns:\n",
    "    - Modified hms_basin_file_data\n",
    "    \"\"\"\n",
    "    with open(hms_basin_file_path, 'r') as file:\n",
    "        hms_basin_file_data = file.read()\n",
    "\n",
    "    # Fill in Recession Baseflow if set as \"None\" and add required variables\n",
    "    baseflow_none_text = \"Baseflow: None\"\n",
    "    if baseflow_none_text in hms_basin_file_data:\n",
    "        baseflow_text = \"Baseflow: Recession\\n Recession Factor: {}\\n Initial Flow/Area Ratio: {}\\n Threshold Flow to Peak Ratio: {}\"\n",
    "        baseflow_text = baseflow_text.format(run['recession_factor'], run['initial_flow_area_ratio'], run['threshold_flow_to_peak_ratio'])\n",
    "        hms_basin_file_data = hms_basin_file_data.replace(baseflow_none_text, baseflow_text)\n",
    "        print(\"Baseflow Set to None.  Inserting Recession Baseflow Parameters\")  # Indicate that Baseflow data was filled\n",
    "\n",
    "    with open(hms_basin_file_path, 'w') as file:\n",
    "        file.write(hms_basin_file_data)\n",
    "    return hms_basin_file_data\n",
    "\n",
    "def modify_canopy_method(file_path):\n",
    "    modified_lines = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        if 'Canopy: None' in line:\n",
    "            modified_lines.append('    Canopy: Simple\\n')\n",
    "        elif 'Allow Simultaneous Precip Et: No' in line:\n",
    "            modified_lines.append(line)\n",
    "            modified_lines.append('    Initial Canopy Storage Percent: 1\\n')\n",
    "            modified_lines.append('    Canopy Storage Capacity: 0\\n')\n",
    "            modified_lines.append('    Crop Coefficient: 1.0\\n')\n",
    "            modified_lines.append('    End Canopy:\\n')\n",
    "        else:\n",
    "            modified_lines.append(line)\n",
    "\n",
    "    # Write back the modified content to the file\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.writelines(modified_lines)\n",
    "\n",
    "        \n",
    "#Updates the grid file with the new DSS file path based on the impervious area scale.\n",
    "def update_impervious_grid_definitions(grid_file_path, grid_def, dss_file_path, impervious_area_scale):\n",
    "    \n",
    "    with open(grid_file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    grid_def_found = False\n",
    "    for i, line in enumerate(lines):\n",
    "        if f\"Grid: {grid_def}\" in line:\n",
    "            grid_def_found = True\n",
    "            print(\"Grid Definition Found!\")\n",
    "        if grid_def_found and \"DSS File Name:\" in line:\n",
    "            old_dss_file_name = line.split(\": \")[1].strip()\n",
    "            new_dss_file_name = \"data\\\\\" + dss_file_path.split(\"\\\\\")[-1]\n",
    "            lines[i] = line.replace(old_dss_file_name, new_dss_file_name)\n",
    "            print(\"DSS FileName Replaced!\")\n",
    "            break\n",
    "\n",
    "    with open(grid_file_path, 'w') as file:\n",
    "        file.writelines(lines)\n",
    "\n",
    "#Finds the path of the Impervious DSS file based on the impervious area scale.\n",
    "def find_impervious_dss_grids(directory, impervious_area_scale):\n",
    "    \n",
    "    scale_factor_to_filename = {\n",
    "        1.2: \"Impervious_1.2_SF.dss\",\n",
    "        1.4: \"Impervious_1.4_SF.dss\",\n",
    "        1.6: \"Impervious_1.6_SF.dss\",\n",
    "        1.8: \"Impervious_1.8_SF.dss\",\n",
    "        2.0: \"Impervious_2.0_SF.dss\",\n",
    "    }\n",
    "    filename = scale_factor_to_filename.get(impervious_area_scale)\n",
    "    if filename:\n",
    "        full_path = os.path.join(directory, \"data\", filename)\n",
    "        if os.path.isfile(full_path):\n",
    "            return full_path\n",
    "        else:\n",
    "            print(f\"Error: File '{filename}' cannot be found in the directory '{directory}/data'.\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"Error: No corresponding dss file found for the impervious area scale '{impervious_area_scale}'.\")\n",
    "        return None\n",
    "        \n",
    "# Reads the content of the HMS basin file\n",
    "def read_basin_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "    return content\n",
    "\n",
    "# Optional: pauses the script execution until the user provides input.\n",
    "def pause_script():\n",
    "    \n",
    "    user_input = input(\"Press Enter to continue or 'q' to quit: \")\n",
    "    if user_input.lower() == 'q':\n",
    "        exit()\n",
    "\n",
    "# Optional: print the first 60 lines of a file\n",
    "def print_first_60_lines(file_path):\n",
    "    \"\"\"Prints the first 60 lines of a specified file.\"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        for i, line in enumerate(lines):\n",
    "            if i < 60:\n",
    "                print(line.strip())\n",
    "            else:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all basin update functions\n",
    "\n",
    "# Loads the user calibration data from the specified file path into a pandas dataframe.\n",
    "def load_subbasin_tc_data_to_pandas_dataframe(filepath):\n",
    "    \"\"\"Loads subbasin time of concentration and storage coefficient data into a pandas dataframe.\"\"\"\n",
    "    with open(filepath, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    subbasin_data = []\n",
    "    current_subbasin = None\n",
    "    toc = None\n",
    "    sc = None\n",
    "\n",
    "    for line in lines:\n",
    "        if 'Subbasin:' in line:\n",
    "            current_subbasin = line.split(': ')[1].strip()\n",
    "        elif 'Time of Concentration:' in line:\n",
    "            toc = float(line.split(': ')[1].strip())\n",
    "        elif 'Storage Coefficient:' in line:\n",
    "            sc = float(line.split(': ')[1].strip())\n",
    "        elif 'End:' in line:\n",
    "            if current_subbasin and toc and sc:\n",
    "                subbasin_data.append([current_subbasin, toc, sc])\n",
    "                current_subbasin, toc, sc = None, None, None\n",
    "\n",
    "    subbasin_tc_r = pd.DataFrame(subbasin_data, columns=['subbasin_name', 'time_of_concentration', 'storage_coefficient'])\n",
    "    return subbasin_tc_r\n",
    "\n",
    "# Loads the user calibration data for the current run number into a pandas dataframe.\n",
    "def fetch_calibration_parameters_for_current_run(user_calibration_df, current_run_number):\n",
    "    \"\"\"Fetches the calibration parameters for a specified run number.\"\"\"\n",
    "    current_calibration = user_calibration_df[user_calibration_df['user_run_number_from_csv'] == current_run_number]\n",
    "    if not current_calibration.empty:\n",
    "        return current_calibration.iloc[0]  \n",
    "    else:\n",
    "        print(\"No match found for current run number in user calibration data.\")\n",
    "        return None  # Return None if no matching calibration is found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Calibration Region Data \n",
    "# Function to load subbasin data\n",
    "def load_all_subbasin_data(filepath):\n",
    "    try:\n",
    "        with open(filepath, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file {filepath} does not exist.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    subbasin_data = []\n",
    "    current_subbasin = {}\n",
    "    collect_data = False\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        elif 'Subbasin:' in line:\n",
    "            collect_data = True\n",
    "            current_subbasin['Subbasin'] = line.split(': ')[1]\n",
    "            #print(f\"Processing subbasin: {current_subbasin['Subbasin']}\")\n",
    "        elif line == 'End:' and collect_data:\n",
    "            collect_data = False\n",
    "            subbasin_data.append(current_subbasin)\n",
    "            current_subbasin = {}\n",
    "        elif collect_data:\n",
    "            parts = line.split(': ')\n",
    "            if len(parts) == 2:\n",
    "                key, value = parts\n",
    "                current_subbasin[key] = value\n",
    "\n",
    "    hms_subbasin_data_for_calibration_regions = pd.DataFrame(subbasin_data)\n",
    "    return hms_subbasin_data_for_calibration_regions\n",
    "\n",
    "# Load the subbasin data\n",
    "print(\"Loading subbasin data\")\n",
    "hms_subbasin_data_for_calibration_regions_df = load_all_subbasin_data(hms_basin_file_path)\n",
    "\n",
    "# Display the subbasin data\n",
    "#display(hms_subbasin_data_for_calibration_regions_df)\n",
    "\n",
    "# Write the subbasin data to a CSV file for reference\n",
    "hms_subbasin_data_for_calibration_regions_df.to_csv(\n",
    "    os.path.join(hms_project_directory, 'hms_subbasin_data_for_calibration_regions_df.csv'),\n",
    "    index=False  # Avoid writing row names (index)\n",
    ")\n",
    "\n",
    "# Load the calibration shapefile\n",
    "print(\"Loading calibration shapefile\")\n",
    "calibration_shapefile_gdf = gpd.read_file(calibration_shapefile_path)\n",
    "\n",
    "# Converting the CRS of the shapefile to EPSG:4326 to match the subbasin coordinates\n",
    "print(\"Converting CRS of calibration shapefile to EPSG:4326 (WSG84 to match lat/long coordinates)\")\n",
    "calibration_shapefile_gdf = calibration_shapefile_gdf.to_crs(\"EPSG:4326\")\n",
    "\n",
    "# Assigning the CRS of the subbasin GeoDataFrame to EPSG:4326\n",
    "hms_subbasin_data_calregion_mapping_df = hms_subbasin_data_for_calibration_regions_df\n",
    "hms_subbasin_data_calregion_mapping_df['geometry'] = hms_subbasin_data_calregion_mapping_df.apply(lambda row: Point(float(row['Longitude Degrees']), float(row['Latitude Degrees'])), axis=1)\n",
    "hms_subbasin_data_calregion_mapping_gdf = gpd.GeoDataFrame(hms_subbasin_data_calregion_mapping_df, geometry='geometry')\n",
    "hms_subbasin_data_calregion_mapping_gdf.crs = \"EPSG:4326\"\n",
    "\n",
    "# Manually iterating over each subbasin point to find which calibration region it falls within\n",
    "def find_cal_region(row):\n",
    "    point = Point(float(row['Longitude Degrees']), float(row['Latitude Degrees']))\n",
    "    for idx, region in calibration_shapefile_gdf.iterrows():\n",
    "        if region['geometry'].contains(point):\n",
    "            return region['CalRegion']\n",
    "    return None\n",
    "\n",
    "# Applying the function to each row in the DataFrame\n",
    "hms_subbasin_data_calregion_mapping_df['CalRegion'] = hms_subbasin_data_calregion_mapping_df.apply(find_cal_region, axis=1)\n",
    "\n",
    "# Reporting subbasins that do not have a calibration region\n",
    "no_cal_region = hms_subbasin_data_calregion_mapping_df[hms_subbasin_data_calregion_mapping_df['CalRegion'].isnull()]\n",
    "\n",
    "# Displaying the resulting DataFrame and subbasins with no calibration region\n",
    "display(hms_subbasin_data_calregion_mapping_df[['Subbasin', 'Latitude Degrees', 'Longitude Degrees', 'CalRegion']])\n",
    "\n",
    "# Assuming hms_subbasin_data_calregion_mapping_df is your DataFrame\n",
    "hms_subbasin_data_calregion_mapping_df[['Subbasin', 'Latitude Degrees', 'Longitude Degrees', 'CalRegion']].to_csv(\n",
    "    os.path.join(hms_project_directory, 'hms_subbasin_data_calregion_mapping_df.csv'),\n",
    "    index=False  # Avoid writing row names (index)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to Scale and Update Basin with Calibration Parameters by Calibration Region\n",
    "\n",
    "'''\" Example of In-Cell Context and Prompting \n",
    "\n",
    "Write a python function will take the existing hms_subbasin_data_for_calibration_regions dataframe and create a copy hms_subbasin_data_for_calibration_regions_scaled and edit it as follows:\n",
    "\n",
    "hms_subbasin_data_calregion_mapping_df is the dataframe that contains all the subbasin data for all the calibration regions, before scaling\n",
    "\n",
    "hms_subbasin_data_calregion_mapping_scaled_df is the dataframe that contains all the subbasin data for all the calibration regions, after scaling\n",
    "\n",
    "\n",
    "Example Input CSV:  \n",
    "user_run_number_from_csv,calregion,initial_deficit_scale,maximum_deficit_scale,percolation_rate_scale,impervious_area_scale,recession_factor,initial_flow_area_ratio,threshold_flow_to_peak_ratio,\n",
    "1,1,0.99,0.91,0.1,0.85,0.11,1.01,0.15,0.9,1.1\n",
    "1,2,0.98,0.92,0.05,0.84,0.12,1.02,0.14,0.8,1.2\n",
    "1,3,0.97,0.93,0.01,0.83,0.13,1.03,0.13,0.7,1.3\n",
    "2,1,0.96,0.94,0.02,0.82,0.14,1.04,0.12,0.6,1.4\n",
    "2,2,0.95,0.95,0.03,0.81,0.15,1.05,0.11,0.5,1.5\n",
    "2,3,0.94,0.96,0.04,0.8,0.26,1.06,0.1,0.4,1.6\n",
    "\n",
    "\n",
    "Example hms_subbasin_data_calregion_mapping_df in CSV format:\n",
    "Subbasin,Latitude Degrees,Longitude Degrees,CalRegion\n",
    "S_TriggerBr_01, 30.68085406918698,-93.02073832096788,2\n",
    "S_MuleyBr_01, 30.687537221284185,-93.00095193010225,2\n",
    "\n",
    "\n",
    "Example hms_subbasin_data_calregion_mapping_df in CSV format:\n",
    "Subbasin,Last Modified Date,Last Modified Time,Latitude Degrees,Longitude Degrees,Canvas X,Canvas Y,Area,Discretization,File,Projection,Cell Size,Canopy,Allow Simultaneous Precip Et,Plant Uptake Method,Initial Canopy Storage Percent,Canopy Storage Capacity,Crop Coefficient,Surface,LossRate,Initial Deficit Grid,Maximum Deficit Grid,Constant Rate Grid,Impervious Area Grid,Initial Deficit Scale,Maximum Deficit Scale,Percolation Rate Scale,Impervious Area Scale,Transform,Time of Concentration,Storage Coefficient,Baseflow,Recession Factor,Initial Flow/Area Ratio,Threshold Flow to Peak Ratio\n",
    "S_TriggerBr_01,23-Sep-23,16:53:17,30.68085407,-93.02073832,2750381.379,797017.1065,1.7748,Structured,2018_Existing_Conditions.sqlite,5070,500,Simple,No,None,1,0,1,None,Gridded Deficit Constant,Initial_Moisture_Deficit,Max_Moisture_Deficit,Percolation_Rate,2019_Percent_Impervious,1,1,0.01,1,Modified Clark,10.77,2.69,Recession,0.1,1,0.1\n",
    "S_MuleyBr_01,19-Jan-23,14:00:30,30.68753722,-93.00095193,2756636.354,799356.6226,2.8583,Structured,2018_Existing_Conditions.sqlite,5070,500,None,No,None,,,,None,Gridded Deficit Constant,Initial_Moisture_Deficit,Max_Moisture_Deficit,Percolation_Rate,2019_Percent_Impervious,1,1,0.01,1,Modified Clark,13.78,3.44,Recession,0.1,1,0.1\n",
    "\n",
    "\n",
    "For each subbasin, the calibration region from hms_subbasin_data_calregion_mapping_df is used to determine the calibration region.  The corresponding input csv column is \"calregion\".\n",
    "\n",
    "The columns that will be edited in hms_subbasin_data_calregion_mapping_df are:\n",
    "Initial Deficit Scale       (directly copied from input csv)\n",
    "Maximum Deficit Scale       (directly copied from input csv)\n",
    "Percolation Rate Scale       (directly copied from input csv)\n",
    "Impervious Area Scale (if <= 1.0)       (directly copied from input csv)\n",
    "Recession Factor       (directly copied from input csv)\n",
    "Initial Flow/Area Ratio       (directly copied from input csv)\n",
    "Threshold Flow to Peak Ratio       (directly copied from input csv)\n",
    "Time of Concentration       (use time_of_concentration_scale to scale the value from hms_subbasin_data_for_calibration_regions)\n",
    "Storage Coefficient         (use storage_coefficient_scale to scale the value from hms_subbasin_data_for_calibration_regions)\n",
    "\n",
    "\n",
    "Once the scaled dataframe has been edited, these values are written back to the .basin file.  Use this is a helpful function to emulate, its the one that populates hms_subbasin_data_for_calibration_regions.  The proposed function should run in reverse, and write only the needed lines from the dataframe back to the .basin file. \n",
    "\n",
    "def load_all_subbasin_data(filepath):\n",
    "    try:\n",
    "        with open(filepath, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file {filepath} does not exist.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    subbasin_data = []\n",
    "    current_subbasin = {}\n",
    "    collect_data = False\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        elif 'Subbasin:' in line:\n",
    "            collect_data = True\n",
    "            current_subbasin['Subbasin'] = line.split(': ')[1]\n",
    "            print(f\"Processing subbasin: {current_subbasin['Subbasin']}\")\n",
    "        elif line == 'End:' and collect_data:\n",
    "            collect_data = False\n",
    "            subbasin_data.append(current_subbasin)\n",
    "            current_subbasin = {}\n",
    "        elif collect_data:\n",
    "            parts = line.split(': ')\n",
    "            if len(parts) == 2:\n",
    "                key, value = parts\n",
    "                current_subbasin[key] = value\n",
    "\n",
    "    hms_subbasin_data_for_calibration_regions = pd.DataFrame(subbasin_data)\n",
    "    return hms_subbasin_data_for_calibration_regions\n",
    "\n",
    "Here is an example .basin file for example:\n",
    "\n",
    "Basin: 2018_Existing_Conditions\n",
    "Description: Existing Conditions based on USGS 2018 LiDAR\n",
    "Last Modified Date: 1 September 2022\n",
    "Last Modified Time: 19:53:15\n",
    "Version: 4.9\n",
    "Filepath Separator: \\\n",
    "Unit System: English\n",
    "Missing Flow To Zero: No\n",
    "Enable Flow Ratio: No\n",
    "Compute Local Flow At Junctions: No\n",
    "Unregulated Output Required: No\n",
    "\n",
    "Enable Sediment Routing: No\n",
    "\n",
    "Enable Quality Routing: No\n",
    "End:\n",
    "\n",
    "Subbasin: S_TriggerBr_01\n",
    "Last Modified Date: 23 September 2023\n",
    "Last Modified Time: 16:53:17\n",
    "Latitude Degrees:  30.68085406918698\n",
    "Longitude Degrees: -93.02073832096788\n",
    "Canvas X: 2750381.3786021993\n",
    "Canvas Y: 797017.1065448882\n",
    "Area: 1.7748\n",
    "\n",
    "Discretization: Structured\n",
    "File: 2018_Existing_Conditions.sqlite\n",
    "Projection: 5070\n",
    "Cell Size: 500.0\n",
    "\n",
    "Canopy: Simple\n",
    "Allow Simultaneous Precip Et: No\n",
    "Plant Uptake Method: None\n",
    "Initial Canopy Storage Percent: 1\n",
    "Canopy Storage Capacity: 0\n",
    "Crop Coefficient: 1.0\n",
    "End Canopy:\n",
    "\n",
    "Surface: None\n",
    "\n",
    "LossRate: Gridded Deficit Constant\n",
    "Initial Deficit Grid: Initial_Moisture_Deficit\n",
    "Maximum Deficit Grid: Max_Moisture_Deficit\n",
    "Constant Rate Grid: Percolation_Rate\n",
    "Impervious Area Grid: 2019_Percent_Impervious\n",
    "Initial Deficit Scale: 1.0\n",
    "Maximum Deficit Scale: 1.0\n",
    "Percolation Rate Scale: 0.01\n",
    "Impervious Area Scale: 1.0\n",
    "\n",
    "Transform: Modified Clark\n",
    "Time of Concentration: 10.77\n",
    "Storage Coefficient: 2.69\n",
    "\n",
    "Baseflow: Recession\n",
    "Recession Factor: 0.1\n",
    "Initial Flow/Area Ratio: 1.0\n",
    "Threshold Flow to Peak Ratio: 0.1\n",
    "End:\n",
    "\n",
    "Subbasin: S_MuleyBr_01\n",
    "Last Modified Date: 19 January 2023\n",
    "Last Modified Time: 14:00:30\n",
    "Latitude Degrees:  30.687537221284185\n",
    "Longitude Degrees: -93.00095193010225\n",
    "Canvas X: 2756636.3536137324\n",
    "Canvas Y: 799356.6225776641\n",
    "Area: 2.8583\n",
    "\n",
    "Discretization: Structured\n",
    "File: 2018_Existing_Conditions.sqlite\n",
    "Projection: 5070\n",
    "Cell Size: 500.0\n",
    "\n",
    "Canopy: None\n",
    "Allow Simultaneous Precip Et: No\n",
    "Plant Uptake Method: None\n",
    "\n",
    "Surface: None\n",
    "\n",
    "LossRate: Gridded Deficit Constant\n",
    "Initial Deficit Grid: Initial_Moisture_Deficit\n",
    "Maximum Deficit Grid: Max_Moisture_Deficit\n",
    "Constant Rate Grid: Percolation_Rate\n",
    "Impervious Area Grid: 2019_Percent_Impervious\n",
    "Initial Deficit Scale: 1.0\n",
    "Maximum Deficit Scale: 1.0\n",
    "Percolation Rate Scale: 0.01\n",
    "Impervious Area Scale: 1.0\n",
    "\n",
    "Transform: Modified Clark\n",
    "Time of Concentration: 13.78\n",
    "Storage Coefficient: 3.44\n",
    "\n",
    "Baseflow: Recession\n",
    "Recession Factor: 0.1\n",
    "Initial Flow/Area Ratio: 1.0\n",
    "Threshold Flow to Peak Ratio: 0.1\n",
    "End:\n",
    "'''\n",
    "\n",
    "def scale_subbasin_values_by_calregion_d(mapping_df, calib_data_df, run_number):\n",
    "    # Filter the calib_data_df by the run number\n",
    "    calib_data_df = calib_data_df[calib_data_df['user_run_number_from_csv'] == run_number]\n",
    "    \n",
    "    # Continue with the rest of the logic as before\n",
    "    mapping_df['CalRegion'] = mapping_df['CalRegion'].astype('int64')\n",
    "    merged_df = pd.merge(mapping_df, calib_data_df, left_on='CalRegion', right_on='calregion', how='left')\n",
    "    \n",
    "    # ... rest of the code remains unchanged\n",
    "\n",
    "    #display(merged_df)\n",
    "    # Create a new DataFrame to store the updated values\n",
    "    updated_values = {\n",
    "        'Subbasin': [],\n",
    "        'Initial Deficit Scale': [],\n",
    "        'Maximum Deficit Scale': [],\n",
    "        'Percolation Rate Scale': [],\n",
    "        'Impervious Area Scale': [],\n",
    "        'Recession Factor': [],\n",
    "        'Initial Flow/Area Ratio': [],\n",
    "        'Threshold Flow to Peak Ratio': [],\n",
    "        'Time of Concentration': [],\n",
    "        'Storage Coefficient': []\n",
    "    }\n",
    "\n",
    "    # Iterate over the merged DataFrame and update the values\n",
    "    for _, row in merged_df.iterrows():\n",
    "        subbasin = row['Subbasin']\n",
    "        updated_values['Subbasin'].append(subbasin)\n",
    "        updated_values['Initial Deficit Scale'].append(row['initial_deficit_scale'])\n",
    "        updated_values['Maximum Deficit Scale'].append(row['maximum_deficit_scale'])\n",
    "        updated_values['Percolation Rate Scale'].append(row['percolation_rate_scale'])\n",
    "        updated_values['Impervious Area Scale'].append(row['impervious_area_scale'])\n",
    "        updated_values['Recession Factor'].append(row['recession_factor'])\n",
    "        updated_values['Initial Flow/Area Ratio'].append(row['initial_flow_area_ratio'])\n",
    "        updated_values['Threshold Flow to Peak Ratio'].append(row['threshold_flow_to_peak_ratio'])\n",
    "        # ... These values are scaled per the scale factor, then rounded to 2 decimal places\n",
    "        updated_values['Time of Concentration'].append(round(float(row['Time of Concentration']) * row['time_of_concentration_scale'], 2))\n",
    "        updated_values['Storage Coefficient'].append(round(float(row['Storage Coefficient']) * row['storage_coefficient_scale'], 2))\n",
    "     \n",
    "    # Create the DataFrame\n",
    "    updated_df = pd.DataFrame(updated_values)\n",
    "\n",
    "    return updated_df\n",
    "\n",
    "\n",
    "# Function to write values back to .basin file\n",
    "\n",
    "def write_scaled_values_to_basin_file(scaled_df, basin_filepath):\n",
    "    try:\n",
    "        with open(basin_filepath, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file {basin_filepath} does not exist.\")\n",
    "        return\n",
    "\n",
    "    with open(basin_filepath, 'w') as outfile:\n",
    "        current_subbasin = None\n",
    "        for i, line in enumerate(lines):\n",
    "            if 'Subbasin:' in line:\n",
    "                current_subbasin = line.split(': ')[1].strip()\n",
    "                subbasin_row = scaled_df[scaled_df['Subbasin'] == current_subbasin]\n",
    "                if not subbasin_row.empty:\n",
    "                    subbasin_data = subbasin_row.iloc[0]\n",
    "            elif current_subbasin:\n",
    "                parts = line.split(': ')\n",
    "                if len(parts) == 2:\n",
    "                    key, _ = parts\n",
    "                    key = key.strip()\n",
    "                    if key in scaled_df.columns:\n",
    "                        new_value = subbasin_data.get(key)\n",
    "                        if new_value is not None:\n",
    "                            lines[i] = f\"{key}: {new_value}\\n\"\n",
    "        outfile.writelines(lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Logic of Script to Run HMS for each Event and Calibration Run\n",
    "\n",
    "# Convert hms_run_names to a set to remove duplicates and then convert it back to a list\n",
    "unique_hms_run_names = list(set(hms_run_names))\n",
    "print(f\"Unique HMS Run Names: {unique_hms_run_names}\")\n",
    "\n",
    "# Now iterate over the unique run names\n",
    "for run_name in unique_hms_run_names:\n",
    "    hms_run_name = run_name  # Set the current run name\n",
    "    user_calibration_df['user_run_number_from_csv'] = user_calibration_df['user_run_number_from_csv'].astype(int)\n",
    "\n",
    "    # For each calibration run, the script will:\n",
    "    for _, run in user_calibration_df.iterrows():\n",
    "        \n",
    "        run = run.rename({k: k.lower() for k in run.index})\n",
    "        current_run_number = int(run['user_run_number_from_csv'])\n",
    "\n",
    "        print(f\"Processing run number: {current_run_number}\")  # Indicate which run is being processed\n",
    "\n",
    "        hms_run_output_dss = f\"{hms_run_name}_HMS_Run_{current_run_number}{hms_dss_suffix}.dss\"\n",
    "        hms_run_output_dss_path = os.path.join(hms_project_directory, f\"{hms_run_output_dss}\")\n",
    "\n",
    "        # Backup the .grid file\n",
    "        shutil.copy(hms_grid_file_path, hms_grid_file_backup_path)\n",
    "        print(\"Backup of .grid file created.\")  # Indicate that backup of .grid file has been made\n",
    "\n",
    "        if not os.path.isfile(hms_run_output_dss_path):\n",
    "            print(\"Output DSS file does not exist. Preparing HMS Run.\")  \n",
    "\n",
    "            # Update the .run file with the new DSS file name\n",
    "            with open(hms_project_run_file_path, 'r') as file:\n",
    "                run_data = file.readlines()\n",
    "\n",
    "            run_found = False\n",
    "            old_dss_file_name = None\n",
    "            for i, line in enumerate(run_data):\n",
    "                if f\"Run: {hms_run_name}\" in line:\n",
    "                    run_found = True\n",
    "                if \"End:\" in line:\n",
    "                    run_found = False\n",
    "                if run_found and line.strip().startswith(\"DSS File:\"):\n",
    "                    old_dss_file_name = line.split(\":\")[1].strip()\n",
    "                    run_data[i] = line.replace(old_dss_file_name, hms_run_output_dss)\n",
    "            \n",
    "            if old_dss_file_name is None:\n",
    "                raise Exception(\"hms_run_name not found in the HMS .run file. Check the run name in the User Input Section\")\n",
    "            else:\n",
    "                print(f\"Output DSS file name changed from {old_dss_file_name} to {hms_run_output_dss} in .run file.\")\n",
    "\n",
    "            # Write the updated .run data\n",
    "            with open(hms_project_run_file_path, 'w') as file:\n",
    "                file.writelines(run_data)\n",
    "\n",
    "\n",
    "            # The condition is checked outside the function and the function is called if the condition is met\n",
    "            if Canopy_Method_Set_To_Simple_Zero == \"Yes\":\n",
    "                modify_canopy_method(hms_basin_file_path) # make sure hms_basin_file_path is defined\n",
    "\n",
    "\n",
    "            # The main logic to check the condition before updating baseflow recession parameters\n",
    "            if hms_recession_baseflow and Baseflow_Method_Set_to_Recession == \"Yes\":\n",
    "                # Assuming you have already defined and set the variables hms_basin_file_data, run, and hms_basin_file_path\n",
    "                update_baseflow_recession()\n",
    "            else:\n",
    "                print(\"Baseflow already set, or Baseflow_Method_Set_to_Recession is not set to 'Yes'. No changes made.\")\n",
    "\n",
    "            hms_subbasin_data_calregion_mapping_scaled_df = scale_subbasin_values_by_calregion_d(hms_subbasin_data_calregion_mapping_df, user_calibration_df, current_run_number)\n",
    "            print(\"Scaled subbasin values by calibration region\")\n",
    "            display(hms_subbasin_data_calregion_mapping_scaled_df)\n",
    "\n",
    "\n",
    "            write_scaled_values_to_basin_file(hms_subbasin_data_calregion_mapping_scaled_df, hms_basin_file_path)\n",
    "            print(\"Done writing scaled values to .basin file\")\n",
    "\n",
    "            # THIS LOGIC NEEDS TO BE UPDATED SINCE WE HAVE 3 CALIBRATION REGION ENTERED IN THE CSV FILE\n",
    "            # FOR NOW, DO NOT VARY IMPERVIOUS AREA SCALING BETWEEN >1.0 AND <1.0 - and if >1,0, use a single value for all calibration regions\n",
    "            # Check if ANY Impervious Area Scale is greater than 1 and update the .grid file if necessary\n",
    "            if run['impervious_area_scale'] > 1:\n",
    "                dss_file_path = find_impervious_dss_grids(hms_project_directory, run['impervious_area_scale'])\n",
    "                if dss_file_path:\n",
    "                    update_impervious_grid_definitions(hms_grid_file_path, \"2019_Percent_Impervious\", dss_file_path, run['impervious_area_scale'])\n",
    "                else:\n",
    "                    # Restore files from backup and exit the script if the appropriate DSS file is not found\n",
    "                    shutil.copy(hms_project_run_file_backup_path, hms_project_run_file_path)\n",
    "                    shutil.copy(hms_basin_file_backup_path, hms_basin_file_path)\n",
    "                    shutil.copy(hms_grid_file_backup_path, hms_grid_file_path)\n",
    "                    print(\"Impervious DSS File Not Found. Restoring files and Exiting.\")\n",
    "                    exit()\n",
    "\n",
    "            # Pause then execute HMS\n",
    "            time.sleep(5)\n",
    "\n",
    "            cmd = ['cmd', '/c', hms_compute_bat_path, os.path.join(hms_project_directory, hms_project_name + '.hms'), hms_run_name]\n",
    "            print(f\"Executing HMS with command: {' '.join(cmd)}\")\n",
    "\n",
    "            # Execute HMS\n",
    "            subprocess.run(cmd)\n",
    "            \n",
    "            # HEC-HMS creates a log file with the hms_run_name with a .log extension\n",
    "            # To avoid overwriting the log file, we rename it to include the run number and the suffix (matching the DSS file name with a .log extension)\n",
    "            logfile_path = os.path.join(hms_project_directory, hms_run_output_dss.replace('.dss', '.log'))\n",
    "\n",
    "            # Remove existing log file, if it exists\n",
    "            if os.path.exists(logfile_path):\n",
    "                os.remove(logfile_path)\n",
    "\n",
    "            # Now rename the default HMS log file to match DSS file naming convention\n",
    "            os.rename(os.path.join(hms_project_directory, hms_run_name + '.log'), logfile_path) if os.path.exists(os.path.join(hms_project_directory, hms_run_name + '.log')) else print(f\"The file {os.path.join(hms_project_directory, hms_run_name + '.log')} does not exist.\")\n",
    "            print(f\"Renamed log file to {logfile_path}\")\n",
    "            \n",
    "                \n",
    "            # Restore original HMS files\n",
    "            shutil.copy(hms_basin_file_backup_path, hms_basin_file_path)\n",
    "            print(f\"Restored {hms_basin_file_backup_path} to {hms_basin_file_path}\")\n",
    "            shutil.copy(hms_grid_file_backup_path, hms_grid_file_path)\n",
    "            print(f\"Restored {hms_grid_file_backup_path} to {hms_grid_file_path}\")\n",
    "            print(\"\")\n",
    "        else:\n",
    "            print(f\"Run Output DSS file for Run {current_run_number} already exists.\")\n",
    "            print(\"\")\n",
    "\n",
    "    print (\"All Runs Completed\")\n",
    "\n",
    "    # Delete the lock file for file backup and restoration\n",
    "    if os.path.exists(lock_file_path):\n",
    "        os.remove(lock_file_path)\n",
    "        print(\"Lock file deleted.\")\n",
    "    else:\n",
    "        print(\"Lock file does not exist.\")\n",
    "\n",
    "        \n",
    "    print(f\"All Runs Completed for {run_name}\")\n",
    "    \n",
    "print (\"All Events Processed\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AutoHMS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
