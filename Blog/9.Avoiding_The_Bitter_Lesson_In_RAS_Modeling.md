# Insights from "The Bitter Lesson" Applied to Hydraulic Modeling with HEC-RAS

[Rich Sutton's "The Bitter Lesson"](http://www.incompleteideas.net/IncIdeas/BitterLesson.html) presents insights that resonate deeply with my experience in the field of hydraulic modeling, particularly in using HEC-RAS in the post-6.0 2D modeling era. Having authored RAS-Commander before ever stumbling across this essay,the parallels between Sutton's writings and my own efforts were evident, and provided a helpful framing that I will attempt to convey further in this post.
</br>
<p align="center">
  <img src="img/bitterlesson.png" width="70%">
</p>
<p align="center">GPT-4o's Visual Take on The Bitter Lesson Applied to Hydraulic Modeling</p>

## Demands of 2D models and Moore's Law's Exhaustion

With the release of HEC-RAS 5.0.7 in 2015, the compute demands for hydraulic modeling exploded.  Since release, a trove of useful features, robustness, stability and steady improvements have pushed a significant amount of modeling effort into 2D.  The 5.0.7 model release which introduced 2D modeling also fatefully coincided with a trend in the semiconductor industry that saw a failure to keep up with Moore Law.  As transistor density gains became more marginal, more chip space became dedicated to additional cores, which provide little if any marginal benefit to HEC-RAS model runtimes beyond a single order of magnitude (and rapidly diminishing marginal returns beyond just 2 cores).
This reality has plagued modelers, as the expectation has always been that the newest machines will always support every-increasing model cell counts and runtime expectations.  Especially with the proliferation of relatively low-performing cloud architectures, modelers would be forgiven for feeling that the state of technology in their field of practice has largely stood still or has even worsened relative to clients' expectations that computers get significantly "better" over time.  In comparison to the advances in scaling highly parallelizable AI architectures, well, there is no comparison.  There are some reasons for that, which make the bitter lesson all the more pertinent.

## Parallelism isn't Built in to HEC-RAS

One fundamental limitation of the HEC-RAS system is its lack of parallelization, meaning models are typically run serially (one plan must complete before the next is executed). To run multiple models in parallel, multiple instances of the program must be manually opened. To avoid errors, especially in automated runs, the entire project folder often needs to be copied. This practice prevents file conflicts that could arise from overlapping preprocessing and model execution stages.  Previous generations of 1D models simply didn't have long enough runtimes to justify such features, as they could always be queued to be completed over a lunch break, or overnight, etc.  But with the aforementioned explosion in model mesh counts and associated nonlinear increase in compute demand, these workflow assumtions simply don't hold, and have been effectively obsoleted for most large scale 2D efforts.  It can often take overnight simply to generate one model run, in which case a parallelized effort proves to be very useful from a real productivity and utilization standpoint. 

Existing solutions using Linux leverage containerization do exist for massively parallel runs. However, these are not accessible to most practicing engineers and are generally operated by specialists.  There are also quite a few implicit limitations of such architectures that I have covered in previous blog posts. Not to mention compute costs! Before RAS-Commander there was a significant gap between solutions capable of running thousands of HEC-RAS runs and the default software, which can only handle a few before becoming unmanageable.  The approach isnt closed source or especially novel.  There has simply been a failure across the industry to recognize and adapt to larger technological trends that have changed the physical layout and "shape" of available CPU compute over the last decade.  While not currently high on the development roadmap, parallelism will undoubtedly be implemented eventually in every major modeling package as the shape of compute diverges from the development roadmap and the bitter lesson slowly becomes more apparent. 

## Applying Insights from "The Bitter Lesson"

Rich Sutton's "The Bitter Lesson" emphasizes the limitations of heuristic methods and highlights the power of leveraging parallelization to scale computational resources. Moving from one serial operation to ten simultaneous runs on a workstation, and then expanding further to 99 runs, unlocks significant new data-driven approaches that reduce uncertainty and improve accuracy. This approach mirrors the broader trend in technology, where innovations have been driven by parallel processing since the early 2000s, as physical limits were reached in CPU design. Many approaches have been implicitly limited by Moore’s Law, where parallelism was not efficiently achievable.

In a recent keynote, NVIDIA's CEO Jensen Huang highlighted the paradigm of diminishing returns experienced by CPU-bound processes. As long as we are running x86 processors, this won’t change. GPUs, designed as Reduced Instruction Set Chips (RISC) with fast interconnects designed for highly parallelizable tasks, are pivotal. AI as we know it is built on massively parallelized systems that are not as easily constrained by Moore’s Law.

Similarly, in hydraulic modeling, leveraging more data and compute power allows for more informed decision-making. While the fundamental physics equations will remain tightly coupled and difficult to scale across any particular model domain, parallelism can still be leveraged.  While RAS-Commander allows more effective model-level parallelism, it is limited to just 99 runs which does reasonably cover the most immediate frontier of modeling innovation which was previously only available with proprietary tools.  

## Optimizing Within Constraints of Available Technologies

The HEC-RAS system has long focused on improving resolution in data sets, land cover layers, and mesh cells. These improvements are all increasing the tightly coupled solution set, fundamentally constrained by the scaling limitations of CPUs with x86 instruction sets. Using more than two CPUs becomes inefficient, and even with the most advanced server architectures, benefits are marginal (and many times, less performant than other options at similar cost).

The future of hydraulic modeling in the near term lies in overcoming these constraints. By leveraging parallelization on a single workstation as well as across multiple workstations, significant computational power can be unlocked, moving beyond the limitations of serialized operations. This approach aligns with the principles of "The Bitter Lesson," advocating for more data and compute resources to achieve better outcomes. In practice, the RAS Commander tool exemplifies this approach. It bridges the gap between large-scale, specialist-driven solutions and accessible tools for practicing engineers. RAS Commander enables efficient use of local machines for parallel execution, vastly increasing computational capacity for HEC-RAS projects. 

Even when GPU-based solvers arrive, the calculations within any physically-linked model domain will remain tightly coupled and probably won’t scale like the matrix multiplication of an AI. Waiting for GPU solvers to solve the problem is a dead end. There will still be a need for parallelized approaches to reduce uncertainties at many crucial steps in our hydrologic and hydraulic modeling efforts. So far, this approach has been applied to calibration and validation workflows, but countless examples exist where having more than one result at a time would simplify approaches and reduce uncertainty.

## What's the Lesson?  

Restating the Bitter Lesson, waiting for someone to make a “smarter model” or to leapfrog Moore’s Law (which we haven’t kept up with since 2015) is not a reasonable expectation. Until someone figures out how to tokenize a watershed and feed it to a transformer, developing and leveraging parallelization tools should be a focus of hydrologic and hydraulic model software development. with more advanced methods, comes more data and more potential uncertainties that can be signficantly reduced with even the simplest bright force
methods.

The breakthrough that has happened in AI will eventually make its way to H&H modeling, but the paradigm shift is not totally unrelated to the fundamental constraints of our work.  Until that breakthrough happens, learning from the Bitter Lesson to learn from that paradigm shift and adjusting one's mental models and approaches is the most immediate challenge.

## Conclusion

By increasing the number of parallel runs from 1-2 up to 99 runs in parallel, RAS-Commander represents a natural progression of fitting the required methods with the available hardware topology. With more cores available but not effectively utilized on a single model, long runtimes, and increasingly detailed and complex datasets and models, the demands and expectations on the models have long exceeded their ability to keep up, as they are more constrained by compute than methods. By leveraging multiple orders of magnitude of additional effective compute and data to solve problems that were previously only solved with heuristic methods, RAS Commander is an exemplar of "The Bitter Lesson" in practice. Given the technological landscape, this approach may be the only way to exceed the incremental advances offered by solvers and transistor density or SSE extensions.

In conclusion, the Bitter Lesson should be heeded not just by those in AI who were disrupted, but also by others whose work is fundamentally compute constrained.
