# Insights from "The Bitter Lesson" Applied to Hydraulic Modeling with HEC-RAS

[Rich Sutton's "The Bitter Lesson"](http://www.incompleteideas.net/IncIdeas/BitterLesson.html) presents insights that resonate deeply with my experience in the field of hydraulic modeling, particularly in using HEC-RAS in the post-6.0 2D modeling era. Having authored RAS-Commander before ever stumbling across this essay,the parallels between Sutton's writings and my own efforts were evident, and provided a helpful framing that I will attempt to convey further in this post.

## Demands of 2D models 


## Paralellism isnt Built in to HEC-RAS

One fundamental limitation of the HEC-RAS system is its lack of parallelization, meaning models are typically run serially. To run multiple models, multiple instances of the program must be opened. To avoid errors, especially in automated runs, the entire project folder often needs to be copied. This practice prevents file conflicts that could arise from overlapping preprocessing and model execution stages.

For large-scale modeling, solutions using Linux leverage containerization for massively parallel runs. However, these are not accessible to most practicing engineers and are generally operated by specialists. This creates a significant gap between solutions capable of running thousands of HEC-RAS runs and the default software, which can only handle a few before becoming unmanageable.

## Applying Insights from "The Bitter Lesson"

Rich Sutton's "The Bitter Lesson" emphasizes the limitations of heuristic methods and highlights the power of leveraging parallelization to scale computational resources. Moving from one to ten simultaneous runs on a workstation, and then expanding further to 99 runs, unlocks significant new data-driven approaches that reduce uncertainty and improve accuracy. This approach mirrors the broader trend in technology, where innovations have been driven by parallel processing since the early 2000s, as physical limits were reached in CPU design. Many approaches have been implicitly limited by Moore’s Law, where parallelism was not efficiently achievable.

In a recent keynote, NVIDIA's CEO Jensen Huang highlighted the paradigm of diminishing returns experienced by CPU-bound processes. As long as we are running x86 processors, this won’t change. GPUs, designed as Reduced Instruction Set Chips (RISC) with fast interconnects designed for highly parallelizable tasks, are pivotal. AI as we know it is built on massively parallelized systems that are not as easily constrained by Moore’s Law.

Similarly, in hydraulic modeling, leveraging more data and compute power allows for more informed decision-making. While the fundamental physics equations will remain tightly coupled and difficult to scale across any particular model domain, parallelism can still be leveraged.

## Optimizing Within Constraints of Available Technologies

The HEC-RAS system has long focused on improving resolution in data sets, land cover layers, and mesh cells. These improvements are all increasing the tightly coupled solution set, fundamentally constrained by the scaling limitations of CPUs with x86 instruction sets. Using more than two CPUs becomes inefficient, and even with the most advanced server architectures, benefits are marginal (and many times, less performant than other options at similar cost).

The future of hydraulic modeling in the near term lies in overcoming these constraints. By leveraging parallelization on a single workstation as well as across multiple workstations, significant computational power can be unlocked, moving beyond the limitations of serialized operations. This approach aligns with the principles of "The Bitter Lesson," advocating for more data and compute resources to achieve better outcomes. In practice, the RAS Commander tool exemplifies this approach. It bridges the gap between large-scale, specialist-driven solutions and accessible tools for practicing engineers. RAS Commander enables efficient use of local machines for parallel execution, vastly increasing computational capacity for HEC-RAS projects. 

Even when GPU-based solvers arrive, the calculations within any physically-linked model domain will remain tightly coupled and probably won’t scale like the matrix multiplication of an AI. Waiting for GPU solvers to solve the problem is a dead end. There will still be a need for parallelized approaches to reduce uncertainties at many crucial steps in our hydrologic and hydraulic modeling efforts. So far, this approach has been applied to calibration and validation workflows, but countless examples exist where having more than one result at a time would simplify approaches and reduce uncertainty.

## What's the Lesson?  

Restating the Bitter Lesson, waiting for someone to make a “smarter model” or to leapfrog Moore’s Law (which we haven’t kept up with since 2015) is not a reasonable expectation. Until someone figures out how to tokenize a watershed and feed it to a transformer, developing and leveraging parallelization tools should be a focus of hydrologic and hydraulic model software development. with more advanced methods, comes more data and more potential uncertainties that can be signficabtly reduced with even the simplest brite force
methods.

The breakthrough that has happened in AI will eventually make its way to H&H modeling, but the paradigm shift is not totally unrelated to the fundamental constraints of our work.  Until that breakthrough happens, learning from the Bitter Lesson to learn from that paradigm shift and adjusting one's mental models and apporaches is the most immediate challenge.

## Conclusion

By increasing the number of parallel runs from 1-2 up to 99 runs in parallel, RAS-Commander represents a natural progression of fitting the required methods with the available hardware topology. With more cores available but not effectively utilized on a single model, long runtimes, and increasingly detailed and complex datasets and models, the demands and expectations on the models have long exceeded their ability to keep up, as they are more constrained by compute than methods. By leveraging multiple orders of magnitude of additional effective compute and data to solve problems that were previously only solved with heuristic methods, RAS Commander is an exemplar of "The Bitter Lesson" in practice. Given the technological landscape, this approach may be the only way to exceed the incremental advances offered by solvers and transistor density or SSE extensions.

In conclusion, the Bitter Lesson should be heeded not just by those in AI who were disrupted, but also by others whose work is fundamentally compute constrained.