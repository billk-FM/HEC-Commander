{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSS-Commander (DSS Results Plotting with Calibration Statistics)\n",
    "Authors: <br>William (Bill) Katzenmeyer, P.E., C.F.M. (C.H. Fenstermaker and Associates, LLC) \n",
    "<br>         Tyler Young, E.I. (Calibration Metrics Calculations, Debugging and Testing)\n",
    "\n",
    "Source: https://github.com/billk-FM/HEC-Commander-Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER INPUTS: \n",
    "\n",
    "# DSS and Plotting Parameters\n",
    "DSS_Source_Path = r\"C:\\Path\\To_Your\\DSS Results\"                                            # DSS Source Path with DSS files to plot   \n",
    "gauge_csv_file_name = \"Gauge_Data.csv\"                                                      # Gauge data file name (within root folder)     \n",
    "river_stations = [\"12345\", \"6789\", \"101112\"]                                                # Subbasin names can also be used to analyze HEC-HMS results\n",
    "search_word = \"March\"                                                                       # Enter a word from your HMS or RAS run name to filter results\n",
    "plot_window_start , plot_window_end = \"01APR2021 00:00:00\" , \"10APR2021 00:00:00\"           # Example: \"28MAR2018 00:00:00\" , \"10APR2018 00:00:00\"   \n",
    "base_condition_dss = \"EVENT_BASE\"                                                           # Time series matching this search phrase will be plotted in blue\n",
    "\n",
    "\n",
    "# Time Series Type: ex. \"STAGE\" or \"FLOW\"\n",
    "search_parameter1 = \"STAGE\"\n",
    "search_parameter2 = \"FLOW\"\n",
    "\n",
    "# PLOT CUSTOMIZATION\n",
    "# Customization Options:\n",
    "bokeh_plot_width = 1250\n",
    "bokeh_plot_height = 600\n",
    "bokeh_table_width = 1250\n",
    "bokeh_table_height = 200\n",
    "\n",
    "# File Names\n",
    "output_html_file_name = \"_Plots.html\"\n",
    "bokeh_plot_title = \"Time Series Plots by River Station and Series Type\"\n",
    "\n",
    "# Additional Information \n",
    "# This is the bottom elevation of the river_stations mentioned above, so depth can be calculated (in the same order as the river_stations)\n",
    "chan_bottom_elevation_dict = {\"89804\": 112.3, \"117188\": 79.25, \"110449\": 43.66}\n",
    "\n",
    "\n",
    "\n",
    "'''For best results, ensure the gauge data and dss files fully cover the plot window.  '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Example Gauge Data CSV File:\n",
    "River Station,89804,89804,117188,110449,110449\n",
    "STAGE or FLOW,STAGE,FLOW,STAGE,STAGE,FLOW\n",
    "Date,USGS_STA_NUM1,USGS_STA_NUM1,USGS_STA_NUM2,USGS_STA_NUM3,USGS_STA_NUM3\n",
    "3/28/2018 0:00,116.8,60.7,95.195,50.146,371\n",
    "3/28/2018 1:00,116.8,60.7,95.195,50.126,368\n",
    "3/28/2018 2:00,116.79,60.1,95.195,50.136,369\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required Libraries\n",
    "import subprocess\n",
    "import sys\n",
    "import platform\n",
    "\n",
    "def install_module(module_name):\n",
    "    try:\n",
    "        __import__(module_name)\n",
    "        #print(f\"{module_name} is already installed.\")\n",
    "    except ImportError:\n",
    "        print(f\"{module_name} not found. Installing...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-U\", module_name])\n",
    "\n",
    "import sys\n",
    "\n",
    "def python_version_check(version):\n",
    "    current_version = tuple(map(int, sys.version_info[:3]))\n",
    "    required_version = tuple(map(int, version.split(\".\")))\n",
    "    if current_version < required_version:\n",
    "        print(f\"Current Python version ({'.'.join(map(str, current_version))}) is lower than the required version ({version}).\")\n",
    "        print(\"Please create an environment with Python version\", version, \"or higher.\")\n",
    "    else:\n",
    "        print(f\"Current Python version ({'.'.join(map(str, current_version))}) meets the required version ({version}).\")\n",
    "\n",
    "# Check Python version\n",
    "python_version_check(\"3.9.1\")\n",
    "\n",
    "\n",
    "def download_and_install_wheel(wheel_url):\n",
    "    import requests \n",
    "    wheel_file = wheel_url.split(\"/\")[-1]\n",
    "    response = requests.get(wheel_url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        with open(wheel_file, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", wheel_file])\n",
    "        print(f\"Successfully installed GDAL from {wheel_file}.\")\n",
    "    else:\n",
    "        print(f\"Failed to download {wheel_file}. HTTP Status Code: {response.status_code}\")\n",
    "\n",
    "\n",
    "\n",
    "def install_osgeo_and_pydss():\n",
    "    # Check and install osgeo\n",
    "    try:\n",
    "        from osgeo import ogr\n",
    "        print(\"Successfully imported ogr from osgeo.\")\n",
    "    except ImportError:\n",
    "        print(\"Failed to import ogr from osgeo. Attempting to download and install GDAL wheel...\")\n",
    "\n",
    "        # Get Python version and system architecture\n",
    "        python_version = f\"cp{sys.version_info.major}{sys.version_info.minor}\"\n",
    "        arch = 'win_amd64' if platform.architecture()[0] == '64bit' else 'win32'\n",
    "\n",
    "        # Generate the wheel URL dynamically\n",
    "        wheel_url = f\"https://download.lfd.uci.edu/pythonlibs/archived/GDAL-3.4.3-{python_version}-{python_version}-{arch}.whl\"\n",
    "\n",
    "        download_and_install_wheel(wheel_url)\n",
    "\n",
    "        # Re-try importing the ogr module\n",
    "        try:\n",
    "            from osgeo import ogr\n",
    "            print(\"Successfully imported ogr from osgeo.\")\n",
    "        except ImportError:\n",
    "            print(\"Still unable to import ogr from osgeo after attempting to install. Please check your environment.\")\n",
    "\n",
    "    # Check and install PyDSS\n",
    "    try:\n",
    "        from pydsstools.heclib.dss import HecDss\n",
    "    except ImportError:\n",
    "        install_module(\"pydsstools\")\n",
    "        print(\"PyDSS not found. If this fails repeatedly, please follow the steps below for all dependencies:\")\n",
    "        print(\"1. Install essential dependencies: NumPy, pandas, affine, bokeh, chardet\")\n",
    "        print(\"2. Install C++ Build Tools for Visual Studio 2022 from:\")\n",
    "        print(\"   https://download.visualstudio.microsoft.com/download/pr/33081bfc-10f1-42d4-8f5a-df6709b8b105/5ae40ab45b17219f8a9c505f5106bbe818bceba5f8e92bd1df5d6db8fa44d820/vs_BuildTools.exe\")\n",
    "        print(\"3. Download and install the PyDSS Wheel from:\")\n",
    "        print(\"   https://raw.githubusercontent.com/gyanz/pydsstools/master/dist/pydsstools-2.2-cp39-cp39-win_amd64.whl\")\n",
    "\n",
    "\n",
    "# List of modules to check and install if necessary\n",
    "modules = ['affine',\n",
    " 'rasterio',\n",
    " 'tables',\n",
    " 'pandas',\n",
    " 'shutil',\n",
    " 'numba',\n",
    " 'numpy',\n",
    " 'cython',\n",
    " 'requests',\n",
    " 'bs4',\n",
    " 'chardet',\n",
    " 'concurrent.futures',\n",
    " 'csv',\n",
    " 'rioxarray',\n",
    " 'xarray',\n",
    " 'pyproj',\n",
    " 'platform',\n",
    " 'scikit-learn',\n",
    " 'openpyxl',\n",
    " 'bokeh',\n",
    " 'os',\n",
    " 'matplotlib',\n",
    " 'shapely']\n",
    "\n",
    "for module in modules:\n",
    "    install_module(module)\n",
    "\n",
    "# To execute the function:\n",
    "install_osgeo_and_pydss()\n",
    "\n",
    "from pydsstools.heclib.dss import HecDss\n",
    "from pydsstools.heclib.dss.HecDss import Open\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import os\n",
    "import shutil\n",
    "from IPython.display import clear_output\n",
    "import pyproj\n",
    "import io\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from numba import jit\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from bokeh.palettes import Category20\n",
    "from bokeh.plotting import figure, save, gridplot\n",
    "from bokeh.io import output_file\n",
    "from bokeh.models import ColumnDataSource, Legend, LegendItem, HoverTool\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.styles import PatternFill, Font, Alignment\n",
    "from openpyxl.formatting.rule import CellIsRule\n",
    "from bokeh.models import DataTable, DateFormatter, TableColumn, ColumnDataSource, NumberFormatter, BooleanFormatter, StringFormatter, DateFormatter, CustomJS, ImageURL, HTMLTemplateFormatter \n",
    "from bokeh.layouts import layout\n",
    "from bokeh.io import show, save\n",
    "from bokeh.layouts import row\n",
    "from bokeh.models.widgets import DataTable, DateFormatter, TableColumn, StringEditor, IntEditor, NumberEditor, SelectEditor, Div\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clip Gauge Data to Plot Window\n",
    "\n",
    "# Define pathname_pattern\n",
    "pathname_pattern = \"/*/*/*/*/*/*/\"\n",
    "\n",
    "# Define the date range\n",
    "plot_start_date = pd.to_datetime(plot_window_start, format='%d%b%Y %H:%M:%S')\n",
    "plot_end_date = pd.to_datetime(plot_window_end, format='%d%b%Y %H:%M:%S')\n",
    "\n",
    "# Function to get the date range from the first DSS file\n",
    "# NEED TO INSERT HERE\n",
    "\n",
    "# read the gauge data CSV file, filter the data based on the date range, and write the filtered data to a new CSV file\n",
    "# This ensures that dates are aligned between the gauge data and DSS files\n",
    "\n",
    "# Read the header rows\n",
    "header_rows = pd.read_csv(os.path.join(DSS_Source_Path, gauge_csv_file_name), nrows=2)\n",
    "# Remove suffix from duplicate column names in header rows\n",
    "header_rows.columns = header_rows.columns.str.replace(r\"\\.\\d+$\", \"\", regex=True)\n",
    "# Read the data, setting the header to the third row (0-indexed)\n",
    "df = pd.read_csv(os.path.join(DSS_Source_Path, gauge_csv_file_name), skiprows=3)\n",
    "# Rename the first column to 'Date'\n",
    "df = df.rename(columns={df.columns[0]: 'Date'})\n",
    "# Remove suffix from duplicate column names\n",
    "df.columns = df.columns.str.replace(r\"\\.\\d+$\", \"\", regex=True)\n",
    "# Ensure the 'Date' column is in datetime format\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%m/%d/%Y %H:%M', errors='coerce')\n",
    "# Drop rows with NaT in 'Date' column\n",
    "df = df.dropna(subset=['Date'])\n",
    "# Filter the DataFrame based on the date range\n",
    "df = df[(df['Date'] >= plot_start_date) & (df['Date'] <= plot_end_date)]\n",
    "# Write the header rows to the new CSV file\n",
    "header_rows.to_csv(os.path.join(DSS_Source_Path, '00_Gauge_data_plot_window.csv'), index=False)\n",
    "# Append the filtered DataFrame to the new CSV file, without writing the header\n",
    "df.to_csv(os.path.join(DSS_Source_Path, '00_Gauge_data_plot_window.csv'), mode='a', index=False, header=False)\n",
    "\n",
    "# Re-define gauge CSV file name to point to the filtered file\n",
    "gauge_csv_file_name = '00_Gauge_data_plot_window.csv'\n",
    "\n",
    "print(f\"Plotting data from {plot_window_start} to {plot_window_end}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define HTML Templates\n",
    "rmse_template = \"\"\"\n",
    "            <div style=\"color:<%= \n",
    "                (function colorfromint(){\n",
    "                    if(value >= 0 && value <= 15)\n",
    "                        return('green');\n",
    "                    if(value > 15.0001 && value <= 100)\n",
    "                        return('red');                        \n",
    "                }()) %>; \n",
    "                font-weight: bold; \n",
    "                text-align: left;\n",
    "            \"><%= value.toFixed(2) %></div>\n",
    "        \"\"\"\n",
    "r_template = \"\"\"\n",
    "    <div style=\"color:<%= \n",
    "        (function colorfromint(){\n",
    "            if(value >= 0.9 && value <= 1)\n",
    "                return('green');\n",
    "            if(value < 0.9)\n",
    "                return('red');\n",
    "        }()) %>; \n",
    "        font-weight: bold; \n",
    "        text-align: left;\n",
    "    \"><%= value.toFixed(2) %></div>\n",
    "\"\"\"\n",
    "PBIAS_template = \"\"\"\n",
    "    <div style=\"color:<%= \n",
    "        (function colorfromint(){\n",
    "            if(value >= -10 && value <= 10)\n",
    "                return('green');\n",
    "            if(value < -10)\n",
    "                return('red');\n",
    "            if(value > 10)\n",
    "                return('red');\n",
    "        }()) %>; \n",
    "        font-weight: bold; \n",
    "        text-align: left;\n",
    "    \"><%= value.toFixed(2) %></div>\n",
    "\"\"\"\n",
    "NSE_template = \"\"\"\n",
    "    <div style=\"color:<%= \n",
    "        (function colorfromint(){\n",
    "            if(value >= 0.6 && value <= 1)\n",
    "                return('green');\n",
    "            if(value >= 0 && value < 0.6)\n",
    "                return('red');\n",
    "        }()) %>; \n",
    "        font-weight: bold; \n",
    "        text-align: left;\n",
    "    \"><%= value.toFixed(2) %></div>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define All Remaining Functions\n",
    "\n",
    "# Define pathname_pattern\n",
    "pathname_pattern = \"/*/*/*/*/*/*/\"\n",
    "\n",
    "def adjust_dataframe(df, river_station, parameter):\n",
    "    if parameter == 'STAGE' and river_station in chan_bottom_elevation_dict:\n",
    "        # Adjust all columns, excluding the timestamp\n",
    "        for col in df.columns[0:]:\n",
    "            df[col] -= chan_bottom_elevation_dict[river_station]\n",
    "    return df\n",
    "\n",
    "def write_data_to_excel(grouped_data, folder_path, file_name, adjust=False):\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    with pd.ExcelWriter(file_path, engine=\"xlsxwriter\") as writer:\n",
    "        for key in grouped_data:\n",
    "            river_station = key[0]\n",
    "            parameter = key[1]\n",
    "            sheet_name = f\"{river_station}_{parameter}\"\n",
    "            \n",
    "            # Rearrange the columns, so that Gauge_Data.csv is first (if available)\n",
    "            gauge_data_columns = [series for series in grouped_data[key] if gauge_csv_file_name in series.name]\n",
    "            other_columns = [series for series in grouped_data[key] if gauge_csv_file_name not in series.name]\n",
    "            \n",
    "            if len(gauge_data_columns) < 1:\n",
    "                no_gauge_series = pd.Series(name=\"No gauge\", dtype='float64')\n",
    "                gauge_data_columns = [no_gauge_series]\n",
    "\n",
    "            reordered_columns = gauge_data_columns + other_columns\n",
    "            df_concat = pd.concat(reordered_columns, axis=1)\n",
    "            \n",
    "            if adjust:\n",
    "                df_concat = adjust_dataframe(df_concat, river_station, parameter)\n",
    "                \n",
    "            df_concat.to_excel(writer, sheet_name=sheet_name)\n",
    "\n",
    "\n",
    "# Update the two original functions to use the new write_data_to_excel function\n",
    "def save_plotted_data_to_excel(grouped_data, folder_path):\n",
    "    write_data_to_excel(grouped_data, folder_path, \"1_Output_Plotted_Data.xlsx\")\n",
    "\n",
    "def save_plotted_data_to_excel_depth(grouped_data, folder_path):\n",
    "    write_data_to_excel(grouped_data, folder_path, \"1_Output_Plotted_Data_Depth.xlsx\", adjust=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def check_identical_values(df, new_series):\n",
    "    for column in df.columns:\n",
    "        if np.array_equal(df[column].head(100).values, new_series.head(100).values):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def get_gauge_labels(grouped_data, gauge_csv_file_name):\n",
    "        gauge_labels = {}\n",
    "        for key in grouped_data:\n",
    "            for series in grouped_data[key]:\n",
    "                if gauge_csv_file_name in series.name:\n",
    "                    river_station = key[0]\n",
    "                    parameter = key[1]\n",
    "                    label_name = series.name.replace(f\"{gauge_csv_file_name}_\", \"\").replace(\".1\", \"\")\n",
    "                    gauge_labels[(river_station, parameter)] = label_name\n",
    "        return gauge_labels\n",
    "\n",
    "\n",
    "def calculate_stats(df_sheet, parameter): \n",
    "        # Remove any rows with empty observed data\n",
    "        df_sheet = df_sheet[~df_sheet.iloc[:, 1].isnull()]\n",
    "\n",
    "        # Check if the data is non-empty\n",
    "        if df_sheet.empty:\n",
    "            stats = []\n",
    "        else:\n",
    "            stats = []\n",
    "            for col in df_sheet.columns[2:]:\n",
    "                n = len(df_sheet)\n",
    "                xi_mean = df_sheet.iloc[:, 1].mean()\n",
    "                xi_peak = df_sheet.iloc[:, 1].max()\n",
    "                yi = df_sheet[col].mean()\n",
    "\n",
    "                rmse = np.sqrt(mean_squared_error(df_sheet.iloc[:, 1], df_sheet[col]))\n",
    "                \n",
    "                # Determine the divisor for rmse_percent based on parameter\n",
    "                if parameter == 'STAGE':\n",
    "                    divisor = xi_mean\n",
    "                elif parameter == 'FLOW':\n",
    "                    divisor = xi_peak  # Note: xi_peak needs to be defined or passed to this function\n",
    "                else:\n",
    "                    raise ValueError(\"Unknown parameter: {}\".format(parameter))\n",
    "\n",
    "                rmse_percent = (rmse / divisor) * 100  # Calculate percentage RMSE\n",
    "\n",
    "                \n",
    "                \n",
    "                \n",
    "                r = np.corrcoef(df_sheet.iloc[:, 1], df_sheet[col])[0][1]\n",
    "\n",
    "                pbias = ((df_sheet.iloc[:, 1] - df_sheet[col]).sum() / df_sheet.iloc[:, 1].sum()) * 100  # Updated PBIAS formula\n",
    "\n",
    "                nse = 1 - ((df_sheet.iloc[:, 1] - df_sheet[col])**2).sum() / ((df_sheet.iloc[:, 1] - xi_mean)**2).sum()\n",
    "\n",
    "                col_modified = col.split('/')[:4]    # Split the column\n",
    "                stats.append(col_modified + [n, rmse_percent, r, pbias, nse])  # Replace rmse with rmse_percent\n",
    "        \n",
    "        return stats\n",
    "\n",
    "\n",
    "def plot_grouped_data_bokeh(grouped_data, folder_path, directory_name):\n",
    "    output_file(os.path.join(folder_path, f\"{directory_name}_Plots.html\"), title=bokeh_plot_title)\n",
    "    plots = []\n",
    "    gauge_labels = get_gauge_labels(grouped_data, gauge_csv_file_name)\n",
    "\n",
    "    # Load statistics data\n",
    "    stats_data = pd.read_excel(os.path.join(folder_path, '2_Output_Statistics_Depth.xlsx'), sheet_name=None)  \n",
    "\n",
    "    # Table for the \"global summaries\" data\n",
    "    if \"global summaries\" in stats_data:\n",
    "        global_summaries = stats_data[\"global summaries\"]\n",
    "        # Cleaning the data by dropping unnamed columns\n",
    "        cleaned_data = global_summaries.drop(columns=global_summaries.columns[:4])\n",
    "        global_src = ColumnDataSource(cleaned_data)\n",
    "        # Use columns from the 'global summaries' \n",
    "        global_columns = [\n",
    "                TableColumn(field=\"Simulation\", title=\"Simulation\",\n",
    "                            formatter=StringFormatter(font_style=\"bold\", text_align='left')),\n",
    "                TableColumn(field=\"RMSE [%]\", title=\"RMSE [%]\", \n",
    "                            formatter=HTMLTemplateFormatter(template=rmse_template)),\n",
    "                TableColumn(field='r', title='r', \n",
    "                            formatter=HTMLTemplateFormatter(template=r_template)),\n",
    "                TableColumn(field='PBIAS [%]', title='PBIAS [%]', \n",
    "                            formatter=HTMLTemplateFormatter(template=PBIAS_template)),\n",
    "                TableColumn(field='NSE', title='NSE', \n",
    "                            formatter=HTMLTemplateFormatter(template=NSE_template)),\n",
    "        ]\n",
    "        global_table = DataTable(source=global_src, columns=global_columns, index_position=None, width=bokeh_table_width, height=bokeh_table_height)\n",
    "\n",
    "        # Title for the table\n",
    "        table_title = Div(text='<h2>Average Statistics by Simulation</h2>', width=850)\n",
    "        \n",
    "        # Title for the table and table combined as one list to make a row\n",
    "        global_row = [table_title, global_table]\n",
    "        # Insert this row at the beginning of plots\n",
    "        plots.insert(0, global_row)\n",
    "\n",
    "\n",
    "    for key in grouped_data:\n",
    "       \n",
    "        # Skip the plot creation if gauge data is not present\n",
    "        if gauge_csv_file_name not in [series.name for series in grouped_data[key]]:\n",
    "            continue\n",
    "\n",
    "        gauge_label = gauge_labels.get(key, \"\")\n",
    "        \n",
    "        # Remaining logic for plotting and stats calculation remains same\n",
    "\n",
    "    for key in grouped_data:\n",
    "        gauge_label = gauge_labels.get(key, \"\")\n",
    "    \n",
    "        p = figure(width=bokeh_plot_width, height=bokeh_plot_height, x_axis_type=\"datetime\", title=f\"{directory_name}          {gauge_label}          RAS River Station {key[0]} - {key[1]}\")\n",
    "\n",
    "        legend_items = []\n",
    "\n",
    "        for idx, series in enumerate(grouped_data[key]):\n",
    "            if gauge_csv_file_name in series.name:  # If the series is derived from Gauge_data.csv\n",
    "                label_name = series.name.replace(f\"{gauge_csv_file_name}_\", \"\").replace(\".1\", \"\")\n",
    "            else:\n",
    "                label_name = series.name.split('/')[0]  # Extract the file name\n",
    "\n",
    "            # Create a new data source for each line plot\n",
    "            source = ColumnDataSource(data=dict(\n",
    "                x=series.index, \n",
    "                y=series.values, \n",
    "                label_name=[label_name]*len(series)\n",
    "            ))\n",
    "\n",
    "            # Define line properties\n",
    "            if gauge_csv_file_name in series.name:\n",
    "                line = p.line(x='x', y='y', source=source, line_width=2, color='black', line_dash='dashed')\n",
    "            else:\n",
    "                if base_condition_dss in series.name:\n",
    "                    line = p.line(x='x', y='y', source=source, line_width=4, color='red', line_dash='dotted')\n",
    "                else:\n",
    "                    color = Category20[20][idx % 20]\n",
    "                    line = p.line(x='x', y='y', source=source, line_width=2, color=color)\n",
    "\n",
    "            # Add a hover tool for each line\n",
    "            hover = HoverTool(tooltips=[\n",
    "                (\"Series Name\", \"@label_name\"), \n",
    "                (\"Date\", \"@x{%F}\"), \n",
    "                (\"Value\", \"@y\")\n",
    "            ], formatters={\n",
    "                \"@x\": \"datetime\"\n",
    "            }, renderers=[line])\n",
    "            p.add_tools(hover)\n",
    "\n",
    "            # Add line to the legend\n",
    "            legend_items.append(LegendItem(label=label_name, renderers=[line]))\n",
    "\n",
    "            hover = HoverTool(tooltips=[(\"Series Name\", \"@label_name\"), (\"Date\", \"@x{%F %T}\"), (\"Value\", \"@y{0.000}\")], formatters={\"@x\": \"datetime\"}, renderers=[line])\n",
    "            p.add_tools(hover)\n",
    "\n",
    "        legend = Legend(items=legend_items, location=\"top_right\")\n",
    "        p.add_layout(legend)\n",
    "\n",
    "        p.xaxis.axis_label = 'Date'\n",
    "        unit = \"CFS\" if key[1] == \"FLOW\" else \"FT\"\n",
    "       \n",
    "        if key[1] == \"FLOW\":\n",
    "            p.yaxis.axis_label = 'Discharge (CFS)'\n",
    "        else:\n",
    "            p.yaxis.axis_label = 'Stage (FT)'\n",
    "\n",
    "        #plots.append([p])\n",
    "\n",
    "        sheet_name = f\"{key[0]}_{key[1]}\"\n",
    "        if sheet_name in stats_data:\n",
    "            stats_df = stats_data[sheet_name]\n",
    "            stats_src = ColumnDataSource(stats_df)\n",
    "        \n",
    "            columns = [\n",
    "                TableColumn(field=\"Simulation\", title=\"Simulation\",\n",
    "                            formatter=StringFormatter(font_style=\"bold\", text_align='left'), width=20),\n",
    "                TableColumn(field=\"RMSE [%]\", title=\"RMSE [%]\", \n",
    "                            formatter=HTMLTemplateFormatter(template=rmse_template), width=10),\n",
    "                TableColumn(field='r', title='r', \n",
    "                            formatter=HTMLTemplateFormatter(template=r_template), width=10),\n",
    "                TableColumn(field='PBIAS [%]', title='PBIAS [%]', \n",
    "                            formatter=HTMLTemplateFormatter(template=PBIAS_template), width=10),\n",
    "                TableColumn(field='NSE', title='NSE', \n",
    "                            formatter=HTMLTemplateFormatter(template=NSE_template), width=10),\n",
    "            ]\n",
    "            stats_table = DataTable(source=stats_src, columns=columns, index_position=None, editable=False, width=bokeh_table_width, height=bokeh_table_height)\n",
    "            \n",
    "            \n",
    "            # Append the table and plot as separate rows (i.e., separate lists)\n",
    "            plots.append([p])               # Adding plot in its own row\n",
    "            plots.append([stats_table])     # Adding table in its own row\n",
    "\n",
    "        else:\n",
    "            plots.append([p])  # if stats do not exist just append plot           \n",
    "\n",
    "    layout = gridplot(plots)\n",
    "    save(layout)\n",
    "pass\n",
    "\n",
    "def calculate_calibration_statistics(DSS_Source_Path):\n",
    "    # Step 1: Read the input excel file\n",
    "    input_1_path = os.path.join(DSS_Source_Path, '1_Output_Plotted_Data_Depth.xlsx')\n",
    "    df = pd.read_excel(input_1_path, sheet_name=None)\n",
    "\n",
    "    # Step 2: Loop through all the sheets in the input file and calculate the statistics for each column separately\n",
    "    stats = {}\n",
    "    for sheet_name, df_sheet in df.items():\n",
    "        # Split sheet_name on underscore, and take the second part as the parameter value\n",
    "        parameter = sheet_name.split('_')[1]\n",
    "        sheet_stats = calculate_stats(df_sheet, parameter)\n",
    "        stats[sheet_name] = sheet_stats\n",
    "\n",
    "    # Step 4: Convert the statistics to a dataframe and save to a new excel file with one sheet per sheet in the input file\n",
    "    with pd.ExcelWriter(os.path.join(DSS_Source_Path, '2_Output_Statistics_Depth.xlsx')) as writer:\n",
    "        for sheet_name, sheet_stats in stats.items():\n",
    "            # Skip if there are no statistics for the current sheet\n",
    "            if len(sheet_stats) == 0:\n",
    "                continue\n",
    "            # Define the column names for the statistics DataFrame\n",
    "            cols = ['Simulation', 'Reach', 'Station', 'Data Type', 'n', 'RMSE [%]', 'r', 'PBIAS [%]', 'NSE']\n",
    "            # Create a DataFrame from the statistics and write it to the Excel file\n",
    "            stats_df = pd.DataFrame(sheet_stats, columns=cols)\n",
    "            stats_df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "                \n",
    "        print(\"Statistics file saved successfully.\")\n",
    "\n",
    "    # Load the saved statistics excel file\n",
    "    output_2_path = os.path.join(DSS_Source_Path, '2_Output_Statistics_Depth.xlsx')\n",
    "    # Load the Excel workbook\n",
    "    xls = pd.ExcelFile(output_2_path)\n",
    "    # Get the names of the sheets in the workbook\n",
    "    sheet_names = xls.sheet_names\n",
    "\n",
    "    # Initialize empty dataframes to store cumulative values for the four statistics\n",
    "    rmse_df = pd.DataFrame()\n",
    "    r_df = pd.DataFrame()\n",
    "    pbias_df = pd.DataFrame()\n",
    "    nse_df = pd.DataFrame()\n",
    "\n",
    "    # Extract data from sheets and combine\n",
    "    for sheet_name in sheet_names:\n",
    "        # Skip the \"global summaries\" sheet\n",
    "        if sheet_name != \"global summaries\":\n",
    "            # Read the data from the current sheet\n",
    "            df = pd.read_excel(xls, sheet_name=sheet_name)\n",
    "            # Group the data by simulation and calculate the mean for each statistic\n",
    "            rmse_group = df.groupby(\"Simulation\")[\"RMSE [%]\"].mean().reset_index()\n",
    "            r_group = df.groupby(\"Simulation\")[\"r\"].mean().reset_index()\n",
    "            pbias_group = df.groupby(\"Simulation\")[\"PBIAS [%]\"].mean().reset_index()\n",
    "            nse_group = df.groupby(\"Simulation\")[\"NSE\"].mean().reset_index()\n",
    "            # Append the grouped data to the corresponding DataFrame\n",
    "            rmse_df = pd.concat([rmse_df, rmse_group], axis=0)\n",
    "            r_df = pd.concat([r_df, r_group], axis=0)\n",
    "            pbias_df = pd.concat([pbias_df, pbias_group], axis=0)\n",
    "            nse_df = pd.concat([nse_df, nse_group], axis=0)\n",
    "\n",
    "    # Compute averages for each simulation\n",
    "    rmse_final = rmse_df.groupby(\"Simulation\")[\"RMSE [%]\"].mean().reset_index()\n",
    "    r_final = r_df.groupby(\"Simulation\")[\"r\"].mean().reset_index()\n",
    "    pbias_final = pbias_df.groupby(\"Simulation\")[\"PBIAS [%]\"].mean().reset_index()\n",
    "    nse_final = nse_df.groupby(\"Simulation\")[\"NSE\"].mean().reset_index()\n",
    "\n",
    "    # Merge the final dataframes for RMSE, r, PBIAS, and NSE based on Simulation name\n",
    "    global_summaries_df = rmse_final.merge(r_final, on=\"Simulation\", how=\"outer\")\\\n",
    "                                    .merge(pbias_final, on=\"Simulation\", how=\"outer\")\\\n",
    "                                    .merge(nse_final, on=\"Simulation\", how=\"outer\")\n",
    "    \n",
    "\n",
    "\n",
    "    # Write the global summaries dataframe to a new sheet in the Excel workbook\n",
    "    with pd.ExcelWriter(output_2_path, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:\n",
    "        global_summaries_df.to_excel(writer, sheet_name=\"global summaries\", index=False, startcol=4)\n",
    "\n",
    "    print(\"Global summaries sheet updated successfully in the Excel workbook.\")\n",
    "\n",
    "    \n",
    "    # Load all sheets into separate dataframes\n",
    "    all_dataframes = {sheet: pd.read_excel(output_2_path, sheet_name=sheet) for sheet in sheet_names}\n",
    "\n",
    "    # Open the Excel writer\n",
    "    with pd.ExcelWriter(output_2_path, engine='openpyxl') as writer:\n",
    "\n",
    "        # First, write the global summaries sheet\n",
    "        global_summaries_df.to_excel(writer, sheet_name=\"global summaries\", index=False, startcol=4)\n",
    "\n",
    "        # Then, write all other sheets\n",
    "        for sheet, data in all_dataframes.items():\n",
    "            # Avoid writing 'global summaries' again\n",
    "            if sheet != \"global summaries\":  \n",
    "                data.to_excel(writer, sheet_name=sheet, index=False)\n",
    "\n",
    "    print(\"Excel workbook updated with 'global summaries' as the first sheet.\")\n",
    "\n",
    "def apply_conditional_formatting(output_2_path):\n",
    "    # Load the workbook for applying conditional formatting\n",
    "    wb = load_workbook(output_2_path)\n",
    "\n",
    "\n",
    "    # Define the styles for conditional formatting\n",
    "    soft_green_fill = PatternFill(start_color='C6EFCE', end_color='C6EFCE', fill_type='solid')\n",
    "    soft_red_fill = PatternFill(start_color='FFC7CE', end_color='FFC7CE', fill_type='solid')\n",
    "    bold_black_font = Font(bold=True)\n",
    "\n",
    "    # Define conditional formatting rules\n",
    "    rmse_green_rule = CellIsRule(operator='between', formula=['0', '15'], stopIfTrue=True, fill=soft_green_fill, font=bold_black_font)\n",
    "    rmse_red_rule = CellIsRule(operator='between', formula=['15.0001', '100'], stopIfTrue=True, fill=soft_red_fill, font=bold_black_font)\n",
    "\n",
    "    r_green_rule = CellIsRule(operator='between', formula=['0.9', '1'], stopIfTrue=True, fill=soft_green_fill, font=bold_black_font)\n",
    "    r_red_rule = CellIsRule(operator='lessThan', formula=['0.9'], stopIfTrue=True, fill=soft_red_fill, font=bold_black_font)\n",
    "\n",
    "    pbias_green_rule = CellIsRule(operator='between', formula=['-10', '10'], stopIfTrue=True, fill=soft_green_fill, font=bold_black_font)\n",
    "    pbias_red_rule_gt = CellIsRule(operator='greaterThan', formula=['10'], stopIfTrue=True, fill=soft_red_fill, font=bold_black_font)\n",
    "    pbias_red_rule_lt = CellIsRule(operator='lessThan', formula=['-10'], stopIfTrue=True, fill=soft_red_fill, font=bold_black_font)\n",
    "\n",
    "    nse_green_rule = CellIsRule(operator='greaterThanOrEqual', formula=['0.6'], stopIfTrue=True, fill=soft_green_fill, font=bold_black_font)\n",
    "    nse_red_rule = CellIsRule(operator='lessThan', formula=['0.6'], stopIfTrue=True, fill=soft_red_fill, font=bold_black_font)\n",
    "\n",
    "\n",
    "    # Apply conditional formatting rules to each sheet\n",
    "    for sheet_name in wb.sheetnames:\n",
    "        # Get the current sheet\n",
    "        ws = wb[sheet_name]\n",
    "        \n",
    "        # Calculate the maximum length of the values in column A to adjust the column width\n",
    "        column_widths = []\n",
    "        column = ws['A']\n",
    "        max_length = 0\n",
    "        column = ws['A']\n",
    "        for cell in column:\n",
    "            try: \n",
    "                if len(str(cell.value)) > max_length:\n",
    "                    max_length = len(cell.value)\n",
    "            except:\n",
    "                pass\n",
    "        adjusted_width = (max_length + 2)\n",
    "        ws.column_dimensions['A'].width = adjusted_width\n",
    "\n",
    "        # Find the last row with data in the sheet\n",
    "        last_row = ws.max_row\n",
    "\n",
    "         # Apply formatting rules for RMSE\n",
    "        ws.conditional_formatting.add(f'F2:F{last_row}', rmse_green_rule)\n",
    "        ws.conditional_formatting.add(f'F2:F{last_row}', rmse_red_rule)\n",
    "\n",
    "        # Apply formatting rules for r\n",
    "        ws.conditional_formatting.add(f'G2:G{last_row}', r_green_rule)\n",
    "        ws.conditional_formatting.add(f'G2:G{last_row}', r_red_rule)\n",
    "\n",
    "        # Apply formatting rules for PBIAS\n",
    "        ws.conditional_formatting.add(f'H2:H{last_row}', pbias_green_rule)\n",
    "        ws.conditional_formatting.add(f'H2:H{last_row}', pbias_red_rule_gt)\n",
    "        ws.conditional_formatting.add(f'H2:H{last_row}', pbias_red_rule_lt)\n",
    "\n",
    "        # Apply formatting rules for NSE\n",
    "        ws.conditional_formatting.add(f'I2:I{last_row}', nse_green_rule)\n",
    "        ws.conditional_formatting.add(f'I2:I{last_row}', nse_red_rule)\n",
    "\n",
    "        # Set number format for RMSE, r, PBIAS, and NSE columns to display 3 decimal places\n",
    "        for row in range(2, last_row + 1):\n",
    "            ws[f'F{row}'].number_format = '0.00'  # RMSE\n",
    "            ws[f'G{row}'].number_format = '0.00'  # r\n",
    "            ws[f'H{row}'].number_format = '0.00'  # PBIAS\n",
    "            ws[f'I{row}'].number_format = '0.00'  # NSE\n",
    "\n",
    "    # Save the workbook with the applied conditional formatting\n",
    "    wb.save(output_2_path)\n",
    "    print(\"Conditional formatting applied successfully.\")    \n",
    "\n",
    "def read_dss_Data_to_CSV(folder_path):\n",
    "    # Initialize an empty DataFrame\n",
    "    df = pd.DataFrame()\n",
    "    # Loop through all files in the directory\n",
    "    for file in os.listdir(folder_path):\n",
    "        # Check if the file is a .dss file\n",
    "        if file.endswith(\".dss\"):\n",
    "            # Open the .dss file\n",
    "            with HecDss.Open(os.path.join(folder_path, file)) as fid:\n",
    "                # Get a list of pathnames in the .dss file\n",
    "                path_list = fid.getPathnameList(pathname_pattern, sort=1)\n",
    "                # Filter the pathnames based on certain conditions\n",
    "                filtered_path_list = [\n",
    "                    path for path in path_list\n",
    "                    if any(station in path for station in river_stations)\n",
    "                    and search_word in path\n",
    "                    and (search_parameter1 in path or search_parameter2 in path)                    \n",
    "                    and \"FLOW-CUM\" not in path\n",
    "                ]\n",
    "                # Loop through the filtered pathnames\n",
    "                for pathname in filtered_path_list:\n",
    "                    # Read the time series data from the .dss file\n",
    "                    ts = fid.read_ts(pathname, window=(plot_window_start, plot_window_end), trim_missing=True)\n",
    "                    # Check if the time series data is not None\n",
    "                    if ts is not None:\n",
    "                        # Get the times and values from the time series data\n",
    "                        times, values = np.array(ts.pytimes), ts.values\n",
    "                        # Construct a header for the DataFrame\n",
    "                        header = f\"{file} {pathname}\"\n",
    "                        # Create a temporary DataFrame with the time series data\n",
    "                        temp_df = pd.DataFrame({header: pd.Series(values[~ts.nodata], index=times[~ts.nodata])})\n",
    "                        # Check if the values in the temporary DataFrame are not identical to the existing DataFrame\n",
    "                        if not check_identical_values(df, temp_df[header]):\n",
    "                            # Concatenate the temporary DataFrame with the existing DataFrame\n",
    "                            df = pd.concat([df, temp_df], axis=1)\n",
    "\n",
    "    # Save the DataFrame to a .csv file\n",
    "    df.to_csv(os.path.join(folder_path, \"0_Output_raw.csv\"), index_label=\"Date\")\n",
    "    return df\n",
    "\n",
    "def process_folder(folder_path):\n",
    "\n",
    "    # Get the name of the directory from the folder path\n",
    "    directory_name = os.path.basename(folder_path)\n",
    "    # Construct the path to the gauge data file\n",
    "    gauge_data_path = os.path.join(folder_path, gauge_csv_file_name)\n",
    "    \n",
    "    # Call the function read_dss_Data_to_CSV\n",
    "    df = read_dss_Data_to_CSV(folder_path)\n",
    "\n",
    "    # Group the data by river station and parameter (STAGE or FLOW)\n",
    "    grouped_data = {(split_label[2], \"STAGE\" if \"STAGE\" in split_label[3] else \"FLOW\"): [] for column in df.columns for split_label in [column.split('/')]}\n",
    "    for column in df.columns:\n",
    "        split_label = column.split('/')\n",
    "        grouped_data[(split_label[2], \"STAGE\" if \"STAGE\" in split_label[3] else \"FLOW\")].append(df[column])\n",
    "\n",
    "    # Read the gauge data from the .csv file\n",
    "    gauge_data = pd.read_csv(gauge_data_path, header=None, nrows=3)\n",
    "    # Extract the metadata from the gauge data\n",
    "    metadata = {key: gauge_data.iloc[idx, 1:].tolist() for idx, key in enumerate([\"River Station\", \"STAGE or FLOW\", \"USGS\"])}\n",
    "    # Read the data from the gauge data\n",
    "    data = pd.read_csv(gauge_data_path, header=2, parse_dates=[\"Date\"])\n",
    "    # Set the Date column as the index of the DataFrame\n",
    "    data.set_index(\"Date\", inplace=True)\n",
    "\n",
    "    # Loop through the columns in the data\n",
    "    for idx, column in enumerate(data.columns):\n",
    "        # Get the key for the grouped data\n",
    "        key = (metadata[\"River Station\"][idx], metadata[\"STAGE or FLOW\"][idx])\n",
    "        # Check if the key exists in the grouped data\n",
    "        if key in grouped_data:\n",
    "            # Append the column to the grouped data\n",
    "            grouped_data[key].append(data[column].rename(f\"{gauge_csv_file_name}_{column}\"))\n",
    "\n",
    "    # Filter the grouped data to only include series that contain the gauge data\n",
    "    grouped_data = {k: v for k, v in grouped_data.items() if any(gauge_csv_file_name in s.name for s in v)}\n",
    "    \n",
    "    save_plotted_data_to_excel(grouped_data, folder_path)  # Save plotted data (Stage/Flow) to excel\n",
    "    save_plotted_data_to_excel_depth(grouped_data, folder_path) # Save plotted data (Depth/Flow) to excel\n",
    "\n",
    "    calculate_calibration_statistics(folder_path)\n",
    "    apply_conditional_formatting(os.path.join(folder_path, '2_Output_Statistics_Depth.xlsx'))\n",
    "    \n",
    "    # Plot the grouped data using Bokeh\n",
    "    plot_grouped_data_bokeh(grouped_data, folder_path, directory_name)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over all DSS files in the source directory and sort/add to plots\n",
    "\n",
    "for root, dirs, files in os.walk(DSS_Source_Path):\n",
    "    # If the gauge CSV file is found in the current directory, process the folder\n",
    "    if gauge_csv_file_name in files:\n",
    "        process_folder(root)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TyDSS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
