{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 User-Defined Inputs\n",
    "\n",
    "# Set Watershed Files Directory\n",
    "watershed_files_directory = r\"WF_Watershed_Files\"\n",
    "\n",
    "# Place your watershed shapefile in the folder above\n",
    "# The shapefile should contain a single polygon representing the watershed boundary\n",
    "# Make sure the polygon is a simple polygon, not a multi-polygon\n",
    "# It should have a correct crs embedded in the .prj file\n",
    "\n",
    "# Set buffer distance and get filtered stations\n",
    "buffer_distance_miles = 100\n",
    "\n",
    "# Define the date range for filtering\n",
    "timeperiod_start = \"2000-01-01\"\n",
    "timeperiod_end = \"2022-12-31\"\n",
    "\n",
    "# GHNCD Data Download (Using the AWS access per https://github.com/awslabs/open-data-docs/tree/main/docs/noaa/noaa-ghcn )\n",
    "download_station_data = \"Yes\"    # \"Yes\" or \"No\"\n",
    "# Set to \"No\" if you want to manually delete data and re-process the output files\n",
    "concurrent_downloads = 1\n",
    "# Increase if you have many stations to download\n",
    "\n",
    "#Exclusion List to remove stations with bad data\n",
    "exclude_stations_by_name = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 Install the required libraries\n",
    "\n",
    "# Install/Import Required Python Packages\n",
    "import sys\n",
    "import subprocess\n",
    "import os\n",
    "os.environ['USE_PATH_FOR_GDAL_PYTHON'] = 'YES'\n",
    "\n",
    "# Based on the import statements below, create a List of packages you want to ensure are installed\n",
    "packages = [\"os\", \"shutil\", \"pandas\", \"geopandas\", \"subprocess\", \"shapely\", \"csv\", \"concurrent.futures\", \"dask\", \"netCDF4\", \"xarray\", \"bs4\", \"tables\", \"requests\", \"numba\", \"matplotlib\"]\n",
    "\n",
    "# Logic to Install Packages\n",
    "def install(package):\n",
    "    try:\n",
    "        # Attempt to install using pip\n",
    "        print(f\"Attempting to install {package} using pip\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "        print(f\"Installed {package} successfully using pip\")\n",
    "    except subprocess.CalledProcessError:\n",
    "        try:\n",
    "            # If pip fails, try to install using conda\n",
    "            print(f\"Attempting to install {package} using conda\")\n",
    "            subprocess.check_call([\"conda\", \"install\", \"-c\", \"conda-forge\", package])\n",
    "        except subprocess.CalledProcessError:\n",
    "            print(f\"Failed to install {package} using both pip and conda\")\n",
    "\n",
    "for package in packages:\n",
    "    try:\n",
    "        # If the import succeeds, the package is installed, so we can move on\n",
    "        __import__(package)\n",
    "    except ImportError:\n",
    "        # If the import fails, the package is not installed and we try to install it\n",
    "        install(package)\n",
    "\n",
    "\n",
    "# Import All Required Libraries for Script\n",
    "import concurrent.futures\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from io import StringIO\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "import requests\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point \n",
    "from shapely.ops import unary_union\n",
    "import numpy as np\n",
    "import json\n",
    "from IPython.display import clear_output\n",
    "import xarray as xr\n",
    "import subprocess\n",
    "import sys\n",
    "import platform\n",
    "from numba import jit\n",
    "import numba\n",
    "import csv\n",
    "import dask.array as da\n",
    "\n",
    "def download_and_install_wheel(wheel_url):\n",
    "    wheel_file = wheel_url.split(\"/\")[-1]\n",
    "    response = requests.get(wheel_url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        with open(wheel_file, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", wheel_file])\n",
    "        print(f\"Successfully installed GDAL from {wheel_file}.\")\n",
    "    else:\n",
    "        print(f\"Failed to download {wheel_file}. HTTP Status Code: {response.status_code}\")\n",
    "\n",
    "# Try importing the ogr module\n",
    "try:\n",
    "    from osgeo import ogr\n",
    "    print(\"Successfully imported ogr from osgeo.\")\n",
    "except ImportError:\n",
    "    print(\"Failed to import ogr from osgeo. Attempting to download and install GDAL wheel...\")\n",
    "    \n",
    "    # Get Python version and system architecture\n",
    "    python_version = f\"cp{sys.version_info.major}{sys.version_info.minor}\"\n",
    "    arch = 'win_amd64' if platform.architecture()[0] == '64bit' else 'win32'\n",
    "    \n",
    "    # Generate the wheel URL dynamically\n",
    "    wheel_url = f\"https://download.lfd.uci.edu/pythonlibs/archived/GDAL-3.4.3-{python_version}-{python_version}-{arch}.whl\"\n",
    "    \n",
    "    download_and_install_wheel(wheel_url)\n",
    "    \n",
    "    # Re-try importing the ogr module\n",
    "    try:\n",
    "        from osgeo import ogr\n",
    "        print(\"Successfully imported ogr from osgeo.\")\n",
    "    except ImportError:\n",
    "        print(\"Still unable to import ogr from osgeo after attempting to install. Please check your environment.\")\n",
    "\n",
    "\n",
    "from osgeo import ogr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 Define all paths\n",
    "\n",
    "\n",
    "# Find .shp file within watershed_files_directory and set as watershed_boundary_shapefile\n",
    "import os\n",
    "for file in os.listdir(watershed_files_directory):\n",
    "    if file.endswith(\".shp\"):\n",
    "        watershed_boundary_shapefile = os.path.join(watershed_files_directory, file)\n",
    "        print(watershed_boundary_shapefile)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def ensure_directory_exists(directory_path: str) -> None:\n",
    "    if not os.path.exists(directory_path):\n",
    "        os.makedirs(directory_path)\n",
    "        print(f\"Created folder at: {directory_path}\")\n",
    "    else:\n",
    "        print(f\"Folder already exists at: {directory_path}\")\n",
    "\n",
    "watershed_boundary_shapefile_dir = os.path.dirname(watershed_boundary_shapefile)\n",
    "\n",
    "# 1. Define base directories and paths\n",
    "output_folder = os.path.dirname(watershed_boundary_shapefile)\n",
    "ensure_directory_exists(output_folder)\n",
    "print(\"Output Folder:\", output_folder)\n",
    "\n",
    "# 2. Paths related to global-historical-climatology-network-daily\n",
    "subfolder_path = os.path.join(output_folder, \"global-historical-climatology-network-daily\")\n",
    "ensure_directory_exists(subfolder_path)\n",
    "print(\"global-historical-climatology-network-daily Path:\", subfolder_path)\n",
    "\n",
    "# 3. Paths related to index files\n",
    "index_folder_path = os.path.join(output_folder, \"index-files\")\n",
    "index_file_path = os.path.join(index_folder_path, \"index.html\")\n",
    "csv_links_file_path = os.path.join(index_folder_path, \"parsed_links.json\")\n",
    "ensure_directory_exists(index_folder_path)\n",
    "print(\"Index File Path:\", index_file_path)\n",
    "print(\"CSV Links File Path:\", csv_links_file_path)\n",
    "\n",
    "# 4. Paths related to CSV and IDW_Grids\n",
    "csv_folder_path = os.path.join(watershed_boundary_shapefile_dir, \"global-historical-climatology-network-daily\")\n",
    "\n",
    "\n",
    "# 5. Paths related to CSV Precipitation Outputs\n",
    "csv_precipitation_folder_path = os.path.join(watershed_boundary_shapefile_dir, \"CSV_Precipitation\")\n",
    "csv_precipitation_file_path = os.path.join(csv_precipitation_folder_path, \"Precipitation.csv\")\n",
    "ensure_directory_exists(csv_precipitation_folder_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 Download ghcnd-stations.txt from AWS\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import requests\n",
    "\n",
    "# Extracting directory from user-specified shapefile path\n",
    "watershed_dir = os.path.dirname(watershed_boundary_shapefile)\n",
    "\n",
    "# Constructing new path for \"ghcnd-stations.txt\"\n",
    "index_files_dir = os.path.join(watershed_dir, 'index-files')\n",
    "os.makedirs(index_files_dir, exist_ok=True)  # Ensure the directory exists\n",
    "file_path = os.path.join(index_files_dir, \"ghcnd-stations.txt\")\n",
    "\n",
    "# Check if the file already exists\n",
    "if not os.path.exists(file_path):\n",
    "    # AWS URL to the text file\n",
    "    aws_url = \"http://noaa-ghcn-pds.s3.amazonaws.com/ghcnd-stations.txt\"\n",
    "\n",
    "    # Download and save the file from AWS\n",
    "    try:\n",
    "        response = requests.get(aws_url)\n",
    "        response.raise_for_status()\n",
    "        with open(file_path, \"w\", encoding='utf-8') as file:\n",
    "            file.write(response.text)\n",
    "        print(\"Downloaded ghcnd-stations.txt from AWS.\")\n",
    "        data = response.text  # Assign the content to data\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Failed to download from AWS due to: {e}\")\n",
    "else:\n",
    "    print(\"ghcnd-stations.txt already exists. Using the existing file.\")\n",
    "    # Read the file into data\n",
    "    with open(file_path, \"r\", encoding='utf-8') as file:\n",
    "        data = file.read()\n",
    "\n",
    "# Proceed only if data is not None\n",
    "if data is not None:\n",
    "    # Read the content into a Pandas DataFrame\n",
    "    print(\"Reading data into DataFrame stations_df\")\n",
    "    stations_df = pd.read_fwf(StringIO(data), header=None, widths=[12, 9, 10, 7, 3, 31, 4, 4, 6])\n",
    "\n",
    "    # Extract relevant data into separate columns\n",
    "    stations_df['ID'] = stations_df[0]\n",
    "    stations_df['Latitude'] = stations_df[1]\n",
    "    stations_df['Longitude'] = stations_df[2]\n",
    "    stations_df['Elevation'] = stations_df[3]\n",
    "    stations_df['Name'] = stations_df[5]\n",
    "\n",
    "    # Remove rows with NaN in 'Latitude' and 'Longitude'\n",
    "    stations_df = stations_df.dropna(subset=['Latitude', 'Longitude'])\n",
    "\n",
    "    # Drop columns 1-8 from stations_df\n",
    "    stations_df = stations_df.drop(stations_df.columns[1:9], axis=1)\n",
    "\n",
    "    # Display the first few rows of the DataFrame\n",
    "    print(stations_df.head())\n",
    "else:\n",
    "    print(\"No data to process.\")\n",
    "\n",
    "\n",
    "# NEED TO ADD - OTHER DOCUMENTATION DOWNLOADED TO INDEX FILES FOLDER \n",
    "# https://docs.opendata.aws/noaa-ghcn-pds/readme.html\n",
    "# http://noaa-ghcn-pds.s3.amazonaws.com/ghcnd-inventory.txt\n",
    "#\n",
    "#\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4 Find Stations within watershed boundary and within buffer distance\n",
    "\n",
    "# Load your boundary shapefile\n",
    "# Ensure the file path is accurate\n",
    "boundary_gdf = gpd.read_file(watershed_boundary_shapefile)\n",
    "\n",
    "# Ensure the CRS is WGS 84\n",
    "print(\"Watershed Boundary CRS: \" + str(boundary_gdf.crs.to_epsg()))\n",
    "print(\"Converting boundary_gdf to EPSG:4326\")\n",
    "\n",
    "if boundary_gdf.crs != \"EPSG:4326\":\n",
    "    boundary_gdf = boundary_gdf.to_crs(epsg=4326)\n",
    "\n",
    "# Ensure 'Latitude' and 'Longitude' are numeric\n",
    "stations_df['Latitude'] = pd.to_numeric(stations_df['Latitude'], errors='coerce')\n",
    "stations_df['Longitude'] = pd.to_numeric(stations_df['Longitude'], errors='coerce')\n",
    "\n",
    "# Handle NaN values\n",
    "if stations_df[['Latitude', 'Longitude']].isna().any().any():\n",
    "    print(\"Warning: NaN values found in station coordinates. Removing these entries.\")\n",
    "    stations_df = stations_df.dropna(subset=['Latitude', 'Longitude'])\n",
    "\n",
    "# Handle out-of-bound coordinates\n",
    "if ((stations_df['Latitude'] < -90) | (stations_df['Latitude'] > 90) |\n",
    "    (stations_df['Longitude'] < -180) | (stations_df['Longitude'] > 180)).any():\n",
    "    print(\"Warning: Invalid coordinates found. Removing these entries.\")\n",
    "    stations_df = stations_df[(stations_df['Latitude'] >= -90) & (stations_df['Latitude'] <= 90) &\n",
    "                              (stations_df['Longitude'] >= -180) & (stations_df['Longitude'] <= 180)]\n",
    "\n",
    "# Convert stations data to a GeoDataFrame\n",
    "geometry = [Point(xy) for xy in zip(stations_df['Longitude'], stations_df['Latitude'])]\n",
    "stations_gdf = gpd.GeoDataFrame(stations_df, geometry=geometry, crs=\"EPSG:4326\")\n",
    "\n",
    "# Haversine function\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
    "    dlon = lon2 - lon1 \n",
    "    dlat = lat2 - lat1 \n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1-a)) \n",
    "    miles = 3959 * c\n",
    "    return miles\n",
    "\n",
    "# Filter stations function\n",
    "def filter_stations(buffer_distance_miles):\n",
    "    boundary_union = unary_union(boundary_gdf['geometry'])\n",
    "    buffer_deg = buffer_distance_miles / (60 * 1.15)\n",
    "    bbox = [boundary_union.bounds[0] - buffer_deg, boundary_union.bounds[1] - buffer_deg, \n",
    "            boundary_union.bounds[2] + buffer_deg, boundary_union.bounds[3] + buffer_deg]\n",
    "    \n",
    "    possible_matches_index = list(stations_gdf.sindex.intersection(bbox))\n",
    "    possible_matches = stations_gdf.iloc[possible_matches_index].copy()\n",
    "    \n",
    "    possible_matches['Distance'] = possible_matches.apply(\n",
    "        lambda row: haversine(row['Latitude'], row['Longitude'], \n",
    "                              boundary_union.centroid.y, boundary_union.centroid.x), axis=1)\n",
    "    stations_within_buffer = possible_matches[possible_matches['Distance'] <= buffer_distance_miles]\n",
    "    \n",
    "    return stations_within_buffer\n",
    "\n",
    "\n",
    "_filtered_df = filter_stations(buffer_distance_miles)\n",
    "\n",
    "# Save results\n",
    "_filtered_df.to_csv(\"stations_within_buffer.csv\", index=False)\n",
    "\n",
    "# Visualization\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "boundary_gdf.boundary.plot(ax=ax, color='blue', linewidth=2, label='Whisky Chitto Boundary')\n",
    "\n",
    "if not _filtered_df.empty and not _filtered_df.isna().any().any():\n",
    "    _filtered_df.plot(ax=ax, marker='o', color='red', markersize=5, label=f'Stations within {buffer_distance_miles} miles')\n",
    "else:\n",
    "    print(\"Filtered data frame is empty or contains NaN values. Skipping plotting of stations.\")\n",
    "    \n",
    "plt.legend()\n",
    "plt.title(f'Stations within {buffer_distance_miles} miles of Whisky Chitto Boundary')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "display(_filtered_df.head())    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6 Downloading Data for Filtered Stations\n",
    "# Ensure _filtered_df is defined and contains the filtered station data\n",
    "\n",
    "base_url = \"http://noaa-ghcn-pds.s3.amazonaws.com/csv/by_station/\"\n",
    "\n",
    "\n",
    "\n",
    "# Lists to keep track of download results\n",
    "successful_downloads = []\n",
    "failed_downloads = []\n",
    "\n",
    "# Global flag to control download termination across all threads\n",
    "download_terminate = False\n",
    "\n",
    "\n",
    "# Download the GHCND documentation PDF only if it doesn't already exist\n",
    "doc_url = \"https://www.ncei.noaa.gov/data/global-historical-climatology-network-daily/doc/GHCND_documentation.pdf\"\n",
    "doc_file_path = os.path.join(index_folder_path, \"GHCND_documentation.pdf\")\n",
    "\n",
    "if not os.path.exists(doc_file_path):\n",
    "    try:\n",
    "        doc_response = requests.get(doc_url, timeout=500)\n",
    "        doc_response.raise_for_status()\n",
    "\n",
    "        # Save the documentation file\n",
    "        with open(doc_file_path, \"wb\") as file:\n",
    "            file.write(doc_response.content)\n",
    "        print(\"Downloaded GHCND documentation.\")\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Failed to download GHCND documentation due to: {str(e)}\")\n",
    "else:\n",
    "    print(\"GHCND_documentation.pdf already exists, skipping download.\")\n",
    "\n",
    "# Function to download data for a given station\n",
    "def download_station(station):\n",
    "    global download_terminate\n",
    "    \n",
    "    file_path = os.path.join(subfolder_path, f\"{station}.csv\")  \n",
    "    print(f\"Downloading data for station: {station}\")\n",
    "\n",
    "    # Skip download if file already exists\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"File for station {station} already exists. Skipping download.\")\n",
    "        successful_downloads.append(station)\n",
    "        return\n",
    "    \n",
    "    # Using a session with retry mechanism\n",
    "    session = requests.Session()\n",
    "    retries = Retry(total=3, backoff_factor=1, status_forcelist=[502, 503, 504])\n",
    "    session.mount('http://', HTTPAdapter(max_retries=retries))\n",
    "    \n",
    "    # Initial download attempt\n",
    "    download_url = os.path.join(base_url, f\"{station}.csv\")\n",
    "    print(f\"data download url: {download_url}\")\n",
    "    \n",
    "    try:\n",
    "        response = session.get(download_url, timeout=20)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        if download_terminate:\n",
    "            return\n",
    "        \n",
    "        # Constructing new path for CSV files\n",
    "        ghncd_dir = os.path.join(output_folder, 'global-historical-climatology-network-daily')  # Ensure CSVs go here\n",
    "        os.makedirs(ghncd_dir, exist_ok=True)  # Ensure the directory exists\n",
    "\n",
    "        # Assuming `file_name` is the name of the CSV file, adjust as needed\n",
    "        file_path = os.path.join(ghncd_dir, f\"{station}.csv\")  # Ensure CSVs go here\n",
    "        print(f\"file_path: {file_path}\")    \n",
    "        \n",
    "        # Save the data file\n",
    "        with open(file_path, \"wb\") as file:\n",
    "            file.write(response.content)\n",
    "        print(f\"Downloaded data for station: {station}\")\n",
    "        #Print file location\n",
    "        print(f\"File location: {file_path}\")\n",
    "        successful_downloads.append(station)\n",
    "                \n",
    "    except requests.RequestException as e:\n",
    "        # Log failure and check for timeout\n",
    "        if isinstance(e, requests.Timeout):\n",
    "            download_terminate = True\n",
    "            print(f\"Timeout occurred: {str(e)}\")\n",
    "        else:\n",
    "            print(f\"Failed to retrieve data for station: {station} due to {str(e)}\")\n",
    "        failed_downloads.append(station)\n",
    "        \n",
    "# Download process\n",
    "if download_station_data == 'Yes':\n",
    "    with ThreadPoolExecutor(max_workers=concurrent_downloads) as executor:\n",
    "        futures = {executor.submit(download_station, station): station for station in _filtered_df[\"ID\"]}\n",
    "        for future in as_completed(futures):\n",
    "            if download_terminate:\n",
    "                for f in futures:\n",
    "                    f.cancel()\n",
    "                print(\"Some downloads were cancelled due to a timeout.\")\n",
    "                break\n",
    "else:\n",
    "    print(\"Individual Station CSV Download disabled by user flag\")\n",
    "\n",
    "# Create DataFrames for successful and failed downloads\n",
    "_downloaded_df = _filtered_df[_filtered_df[\"ID\"].isin(successful_downloads)]\n",
    "_failed_df = _filtered_df[_filtered_df[\"ID\"].isin(failed_downloads)]\n",
    "\n",
    "# clear_output()\n",
    "\n",
    "print(\"Download process completed!  Re-run this cell to retry failed downloads.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7 Parse the 'ghcnd-stations.txt' file to create a DataFrame with ID, Latitude, Longitude, Elevation, Name, and Geometry columns\n",
    "\n",
    "def parse_ghcnd_stations(file_path):\n",
    "    # Define column widths and names based on the GHCN-D documentation\n",
    "    col_specs = [(0, 11), (12, 20), (21, 30), (31, 37), (38, 68), (69, 73), (74, 75), (76, 79), (80, 85)]\n",
    "    col_names = ['ID', 'Latitude', 'Longitude', 'Elevation', 'Name', 'GSN_Flag', 'HCN_CRN_Flag', 'WMO_ID', 'Method']\n",
    "\n",
    "    # Read the file with fixed-width column formatting\n",
    "    df = pd.read_fwf(file_path, colspecs=col_specs, header=None, names=col_names)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Parse the 'ghcnd-stations.txt' file to create a DataFrame\n",
    "ghcnd_stations_df = parse_ghcnd_stations(file_path)\n",
    "\n",
    "# Optionally, display the first few rows of the DataFrame\n",
    "print(ghcnd_stations_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#8 Create a new dataframe for each AWS CSV file which was downloaded, focusing on precipitation data\n",
    "\n",
    "# Initialize a dictionary to store the dataframes\n",
    "dataframes_dict = {}\n",
    "\n",
    "# Loop through each CSV file\n",
    "for file_name in os.listdir(csv_folder_path):\n",
    "    print(f\"Loading data for station {file_name}\")\n",
    "    station_id = file_name.replace(\".csv\", \"\")\n",
    "    file_path = os.path.join(csv_folder_path, file_name)\n",
    "\n",
    "    try:\n",
    "        # Load the AWS CSV data, suppressing the DtypeWarning\n",
    "        aws_data = pd.read_csv(file_path, parse_dates=['DATE'], low_memory=False)\n",
    "        \n",
    "        # Filter for precipitation data (PRCP)\n",
    "        prcp_data = aws_data[aws_data['ELEMENT'] == 'PRCP']\n",
    "\n",
    "        # Merge with station details\n",
    "        merged_data = pd.merge(prcp_data, ghcnd_stations_df, left_on='ID', right_on='ID')\n",
    "        merged_data = merged_data[['ID', 'Name', 'DATE', 'Latitude', 'Longitude', 'DATA_VALUE']]\n",
    "        merged_data.columns = ['STATION', 'NAME', 'DATE', 'LATITUDE', 'LONGITUDE', 'PRCP']\n",
    "\n",
    "        # Add the dataframe to the dictionary\n",
    "        dataframes_dict[station_id] = merged_data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load data for station {station_id} due to error: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "\n",
    "\n",
    "# Filter the dataframes based on the conditions (PRCP and date range)\n",
    "print(\"Filtering dataframes based on conditions (PRCP and date range)\")\n",
    "filtered_dataframes_dict = {\n",
    "    station_id: df \n",
    "    for station_id, df in dataframes_dict.items() \n",
    "    if (\"PRCP\" in df.columns) and\n",
    "       (df['DATE'].min() <= pd.Timestamp(timeperiod_start) and df['DATE'].max() >= pd.Timestamp(timeperiod_end))\n",
    "}\n",
    "\n",
    "# Create a dataframe containing a list of all dataframes that meet the criteria\n",
    "print(\"Creating a dataframe filtered_stations_df containing a list of all dataframes that meet the criteria\")\n",
    "filtered_stations_df = pd.DataFrame({\n",
    "    \"StationID\": list(filtered_dataframes_dict.keys()),\n",
    "    \"MinDate\": [df['DATE'].min() for df in filtered_dataframes_dict.values()],\n",
    "    \"MaxDate\": [df['DATE'].max() for df in filtered_dataframes_dict.values()],\n",
    "})\n",
    "\n",
    "print(\"Filtered stations:\")\n",
    "display(filtered_stations_df.head())\n",
    "\n",
    "# clear_output()\n",
    "\n",
    "# Display each dataframe's StationID, Name, Lat, Long, and PRCP associated with filtered_stations_df for only the time period of interest\n",
    "for station_id in filtered_stations_df[\"StationID\"]:\n",
    "    print(f\"Station {station_id}:\")\n",
    "    # Ensuring 'PRCP' is in columns before attempting to display or process it\n",
    "    if 'PRCP' in filtered_dataframes_dict[station_id].columns:\n",
    "        display(filtered_dataframes_dict[station_id][[\"STATION\", \"NAME\", \"DATE\", \"LATITUDE\", \"LONGITUDE\", \"PRCP\"]].head())\n",
    "    else:\n",
    "        print(f\"No 'PRCP' column in DataFrame for station {station_id}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#9 Plot the Watershed Outline and All Stations with Precipitation Data within the Time Period\n",
    "\n",
    "# Create a new figure and axis\n",
    "figb, ax = plt.subplots(figsize=(14, 14))\n",
    "\n",
    "# Ensure the CRS is WGS 84\n",
    "if boundary_gdf.crs != \"EPSG:4326\":\n",
    "    boundary_gdf = boundary_gdf.to_crs(epsg=4326)\n",
    "\n",
    "# Plot the watershed boundary\n",
    "boundary_gdf.boundary.plot(ax=ax, color='blue', linewidth=2)\n",
    "\n",
    "# Initialize lists to store all latitudes and longitudes\n",
    "all_lats = []\n",
    "all_lons = []\n",
    "\n",
    "# Plot each station\n",
    "for station_id, station_data in filtered_dataframes_dict.items():\n",
    "    # Filter the station data to only include rows within the time period\n",
    "    filtered_station_data = station_data[(station_data['DATE'] >= pd.Timestamp(timeperiod_start)) & (station_data['DATE'] <= pd.Timestamp(timeperiod_end))]\n",
    "    \n",
    "    # Check if the filtered station data has any precipitation data\n",
    "    if not filtered_station_data['PRCP'].isna().all():\n",
    "        # Extract the first row of the dataframe for the station's coordinates and name\n",
    "        first_row = filtered_station_data.iloc[0]\n",
    "        lat = float(first_row['LATITUDE'])\n",
    "        lon = float(first_row['LONGITUDE'])\n",
    "        \n",
    "        name_part = f\"{first_row['NAME'].split(',')[0]}\\n({first_row['STATION']})\"\n",
    "\n",
    "\n",
    "        \n",
    "        # Check if the coordinates are within the valid range\n",
    "        if not (-90 <= lat <= 90 and -180 <= lon <= 180):\n",
    "            print(f\"Invalid coordinates for station {station_id}: Latitude = {lat}, Longitude = {lon}\")\n",
    "            continue\n",
    "        \n",
    "        # Add the coordinates to the lists\n",
    "        all_lats.append(lat)\n",
    "        all_lons.append(lon)\n",
    "        \n",
    "        # Plot the station\n",
    "        ax.scatter(lon, lat)\n",
    "        ax.text(lon, lat, name_part, fontsize=7)\n",
    "\n",
    "\n",
    "# Compare Precipitation Data Limits to Watershed Boundary Limits\n",
    "boundary_coords = boundary_gdf.geometry[0].exterior.coords.xy\n",
    "\n",
    "# Check data coverage\n",
    "if min(boundary_coords[1]) < min(all_lats) or max(boundary_coords[1]) > max(all_lats) or min(boundary_coords[0]) < min(all_lons) or max(boundary_coords[0]) > max(all_lons):\n",
    "    print(\"Inadequate Data Coverage\")\n",
    "else:\n",
    "    print(\"Adequate Data Coverage\")\n",
    "\n",
    "all_lats.extend(boundary_coords[1])\n",
    "all_lons.extend(boundary_coords[0])\n",
    "\n",
    "# Display max and min of latitudes and longitudes\n",
    "print(\"Max and Min of all_lats and all_lons\")\n",
    "print(max(all_lats))\n",
    "print(min(all_lats))\n",
    "print(max(all_lons))\n",
    "print(min(all_lons))\n",
    "\n",
    "# Set the axis limits\n",
    "ax.set_xlim([min(all_lons)-0.1, max(all_lons)+0.1])\n",
    "ax.set_ylim([min(all_lats)-0.1, max(all_lats)+0.1])\n",
    "\n",
    "# Set the title and labels\n",
    "plt.title(f'Stations with Precipitation Data within {timeperiod_start} to {timeperiod_end}')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "\n",
    "# Add grid\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10 Create new cleaned dataframe with all precipitation data within time period, then fill missing values\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check if 'PRCP' column is present and filter those dataframes\n",
    "print(\"Check if 'PRCP' column is present and filter those dataframes\")\n",
    "cleaned_dataframes_dict = {station_id: data for station_id, data in filtered_dataframes_dict.items() \n",
    "                           if 'PRCP' in data.columns}\n",
    "print(\"cleaned_dataframes_dict:\")\n",
    "\n",
    "# Initialize an empty DataFrame with defined columns\n",
    "print(\"Initialize an empty DataFrame with defined columns\")\n",
    "columns = [\"STATION\", \"NAME\", \"DATE\", \"LATITUDE\", \"LONGITUDE\", \"PRCP\"]\n",
    "prcp_time_series_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "# Iterate through each station and its data in the cleaned dictionary\n",
    "for station_id, station_data in cleaned_dataframes_dict.items():\n",
    "    print(f\"Processing station {station_id}\")\n",
    "    # Filter the station data for the time period of interest\n",
    "    mask = (station_data['DATE'] >= pd.Timestamp(timeperiod_start)) & \\\n",
    "           (station_data['DATE'] <= pd.Timestamp(timeperiod_end))\n",
    "    filtered_station_data = station_data[mask].copy()\n",
    "    \n",
    "    # Check if there is any data within the time period\n",
    "    if not filtered_station_data.empty:\n",
    "        # Ensure consistent dtypes and columns before concatenation\n",
    "        # This could involve checking for NA columns, converting dtypes, etc.\n",
    "        \n",
    "        # Append the filtered data to the consolidated DataFrame\n",
    "        prcp_time_series_df = pd.concat([prcp_time_series_df, filtered_station_data], ignore_index=True)\n",
    "\n",
    "clear_output()\n",
    "\n",
    "# Display the head of the consolidated DataFrame\n",
    "display(prcp_time_series_df.head())\n",
    "\n",
    "# write prcp_time_series_df to csv\n",
    "prcp_time_series_df.to_csv(csv_precipitation_file_path, index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#11 Plot the Precipitation Time Series for All Stations (Bulk Plot)\n",
    "# Create a new figure and axis\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Iterate through each station and plot its time series\n",
    "for station_id, station_data in prcp_time_series_df.groupby('STATION'):\n",
    "    \n",
    "    # Plot PRCP\n",
    "    ax.plot(pd.to_datetime(station_data['DATE']), station_data['PRCP'], \n",
    "            label=f\"{station_data['NAME'].iloc[0]} PRCP\")\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Precipitation (mm)')\n",
    "plt.title('Precipitation Time Series for Each Station')\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "plt.grid(True)\n",
    "\n",
    "# Rotate x-axis labels for better readability\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Adjust layout to prevent clipping and make room for the legend\n",
    "#plt.tight_layout()\n",
    "#plt.subplots_adjust(right=0.75)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HEC-Cmdr-311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
