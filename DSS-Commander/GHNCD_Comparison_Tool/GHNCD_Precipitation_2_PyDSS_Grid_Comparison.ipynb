{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER INPUTS\n",
    "watershed_files_directory = r\"WF_Watershed_Files\"\n",
    "Watershed_Name = \"West Fork\" # For Chart Title\n",
    "\n",
    "# Assuming the GHNCD Data Prep script has completed.  The shapefile folder should have a CSV_Precipitation folder with the GHNCD data\n",
    "# The DSS files should be placed in a subfolder name defined below (Assumed to be in same folder as the watershed shapefile):\n",
    "DSS_Folder = \"DSS-6\"\n",
    "\n",
    "# set variable for GHNCD Search Buffer in Miles \n",
    "GHNCD_Search_Buffer_Miles = 10\n",
    "# Adjust this depending on the size of the watershed and available data\n",
    "\n",
    "# Geoshift for your final plot (in miles)\n",
    "plotextentsbuffer_miles = -3\n",
    "geoshift_miles = 0  # (x axis only)\n",
    "increase_x_axis = 0  # (x axis only)\n",
    "shift_x_axis = -5  # (x axis only)\n",
    "shift_y_axis = -5  # (y axis only)\n",
    "# Adjust to get labels to fit and create report-quality plots\n",
    "#color_ramp_vmax = 10  # (inches) #optional, if not set, will be determined by the maximum value in the DSS file\n",
    "\n",
    "# This is the EPSG code of the CRS of the DSS files (Need to override as PyDSSTools are buggy with CRS)\n",
    "Project_EPSG = \"5070\"  # SHOULD BE, EPSG 5070, 5071, or ESRI:102039 as these are all SHG\n",
    "\n",
    "# List of DSS files and their corresponding event periods\n",
    "# dss_filename = DSS containing the precipitation data\n",
    "# event_intervals = List of lists containing the start and end of the event period (Format: 'DDMMMYYYY:HHMM')\n",
    "# precip_vmax = Maximum precipitation value for the color ramp (inches)\n",
    "# Send ChatGPT a screenshot of your files and your modeled event periods, and it can edit these quickly for large lists\n",
    "\n",
    "dss_data_list = [\n",
    "  \n",
    " {'dss_filename': 'AORC_Harvey_LWIRegion4.dss',  'event_intervals': [['10AUG2017:0000', '15SEP2017:0000']], 'precip_vmax': '26'},\n",
    " {'dss_filename': 'MRMS_Harvey_LWIRegion4.dss',  'event_intervals': [['10AUG2017:0000', '15SEP2017:0000']], 'precip_vmax': '26'},  \n",
    "\n",
    " {'dss_filename': 'AORC_March2016_LWIRegion4.dss',  'event_intervals': [['21FEB2016:0000', '26MAR2016:0000']], 'precip_vmax': '12'},\n",
    " {'dss_filename': 'MRMS_March2016_LWIRegion4.dss',  'event_intervals': [['21FEB2016:0000', '26MAR2016:0000']], 'precip_vmax': '12'},\n",
    "\n",
    " ]\n",
    "\n",
    "''' Example of dss_data_list\n",
    "\n",
    "{'dss_filename': 'AORC_Harvey_LWIRegion4.dss',  'event_intervals': [['10AUG2017:0000', '15SEP2017:0000']], 'precip_vmax': '26'},\n",
    "\n",
    " '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required Libraries\n",
    "import subprocess\n",
    "import sys\n",
    "import platform\n",
    "\n",
    "def install_module(module_name):\n",
    "    try:\n",
    "        __import__(module_name)\n",
    "        #print(f\"{module_name} is already installed.\")\n",
    "    except ImportError:\n",
    "        print(f\"{module_name} not found. Installing...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-U\", module_name])\n",
    "\n",
    "def python_version_check(version):\n",
    "    current_version = \".\".join(map(str, sys.version_info[:3]))\n",
    "    if current_version != version:\n",
    "        print(f\"Current Python version ({current_version}) does not match required version ({version}).\")\n",
    "        print(\"Please create an environment with Python version: \", version)\n",
    "\n",
    "# Check Python version # THIS SHOULD CHECK THAT PYTHON VERSION IS 3.9.1 OR GREATER - PLEASE FIX\n",
    "python_version_check(\"3.9.1\")\n",
    "\n",
    "\n",
    "def download_and_install_wheel(wheel_url):\n",
    "    import requests \n",
    "    wheel_file = wheel_url.split(\"/\")[-1]\n",
    "    response = requests.get(wheel_url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        with open(wheel_file, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", wheel_file])\n",
    "        print(f\"Successfully installed GDAL from {wheel_file}.\")\n",
    "    else:\n",
    "        print(f\"Failed to download {wheel_file}. HTTP Status Code: {response.status_code}\")\n",
    "\n",
    "\n",
    "\n",
    "def install_osgeo_and_pydss():\n",
    "    # Check and install osgeo\n",
    "    try:\n",
    "        from osgeo import ogr\n",
    "        print(\"Successfully imported ogr from osgeo.\")\n",
    "    except ImportError:\n",
    "        print(\"Failed to import ogr from osgeo. Attempting to download and install GDAL wheel...\")\n",
    "\n",
    "        # Get Python version and system architecture\n",
    "        python_version = f\"cp{sys.version_info.major}{sys.version_info.minor}\"\n",
    "        arch = 'win_amd64' if platform.architecture()[0] == '64bit' else 'win32'\n",
    "\n",
    "        # Generate the wheel URL dynamically\n",
    "        wheel_url = f\"https://download.lfd.uci.edu/pythonlibs/archived/GDAL-3.4.3-{python_version}-{python_version}-{arch}.whl\"\n",
    "\n",
    "        download_and_install_wheel(wheel_url)\n",
    "\n",
    "        # Re-try importing the ogr module\n",
    "        try:\n",
    "            from osgeo import ogr\n",
    "            print(\"Successfully imported ogr from osgeo.\")\n",
    "        except ImportError:\n",
    "            print(\"Still unable to import ogr from osgeo after attempting to install. Please check your environment.\")\n",
    "\n",
    "    # Check and install PyDSS\n",
    "    try:\n",
    "        from pydsstools.heclib.dss import HecDss\n",
    "    except ImportError:\n",
    "        install_module(\"pydsstools\")\n",
    "        print(\"PyDSS not found. If this fails repeatedly, please follow the steps below for all dependencies:\")\n",
    "        print(\"1. Install essential dependencies: NumPy, pandas, affine, bokeh, chardet\")\n",
    "        print(\"2. Install C++ Build Tools for Visual Studio 2022 from:\")\n",
    "        print(\"   https://download.visualstudio.microsoft.com/download/pr/33081bfc-10f1-42d4-8f5a-df6709b8b105/5ae40ab45b17219f8a9c505f5106bbe818bceba5f8e92bd1df5d6db8fa44d820/vs_BuildTools.exe\")\n",
    "        print(\"3. Download and install the PyDSS Wheel from:\")\n",
    "        print(\"   https://raw.githubusercontent.com/gyanz/pydsstools/master/dist/pydsstools-2.2-cp39-cp39-win_amd64.whl\")\n",
    "\n",
    "\n",
    "# List of modules to check and install if necessary\n",
    "modules = ['affine',\n",
    " 'rasterio',\n",
    " 'tables',\n",
    " 'pandas',\n",
    " 'shutil',\n",
    " 'numba',\n",
    " 'numpy',\n",
    " 'cython',\n",
    " 'requests',\n",
    " 'bs4',\n",
    " 'chardet',\n",
    " 'concurrent.futures',\n",
    " 'csv',\n",
    " 'rioxarray',\n",
    " 'xarray',\n",
    " 'pyproj',\n",
    " 'platform',\n",
    " 'dask',\n",
    " 'os',\n",
    " 'matplotlib',\n",
    " 'shapely']\n",
    "\n",
    "for module in modules:\n",
    "    install_module(module)\n",
    "\n",
    "# To execute the function:\n",
    "install_osgeo_and_pydss()\n",
    "\n",
    "from pydsstools.heclib.dss import HecDss\n",
    "from pydsstools.heclib.dss.HecDss import Open\n",
    "from pydsstools.heclib.utils import BoundingBox\n",
    "import rasterio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from numba import jit\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import rioxarray as rxr\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "import os\n",
    "import shutil\n",
    "from IPython.display import clear_output\n",
    "from shapely.geometry import Point\n",
    "import pyproj\n",
    "from rasterio.features import geometry_mask\n",
    "from rasterio.crs import CRS\n",
    "import io\n",
    "import shutil\n",
    "import re\n",
    "from IPython.display import clear_output  # Importing the clear_output function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logic to loop over each event and produce the output files\n",
    "\n",
    "\n",
    "\n",
    "# Find .shp file within watershed_files_directory and set as Watershed_Shapefile\n",
    "import os\n",
    "for file in os.listdir(watershed_files_directory):\n",
    "    if file.endswith(\".shp\"):\n",
    "        Watershed_Shapefile = os.path.join(watershed_files_directory, file)\n",
    "        print(Watershed_Shapefile)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "\n",
    "# Define Paths and Output Files\n",
    "\n",
    "\n",
    "failed_files = []  # List to hold the filenames of failed files\n",
    "\n",
    "output_files_folder = os.path.join(os.path.dirname(Watershed_Shapefile), \"Output_Files\")\n",
    "# Create output_files_folder if it doesn't exist\n",
    "if not os.path.exists(output_files_folder):\n",
    "    os.mkdir(output_files_folder)\n",
    "    \n",
    "\n",
    "\n",
    "# Initialize statistics_df before the loop\n",
    "csv_path = os.path.join(output_files_folder, \"Precipitation_Statistics_All_Events.csv\")\n",
    "if not os.path.exists(csv_path):\n",
    "    statistics_df = pd.DataFrame(columns=['Event_Name', 'Gridded_Data_Label', 'Event_Start_Date', 'Event_End_Date', 'Event_Duration', 'Average_Precipitation', 'Maximum_Precipitation', 'Minimum_Precipitation'])\n",
    "    statistics_df.to_csv(csv_path, index=False)\n",
    "else:\n",
    "    statistics_df = pd.read_csv(csv_path)\n",
    "\n",
    "\n",
    "\n",
    "# Loop through each DSS file and its event periods\n",
    "for dss_entry in dss_data_list:\n",
    "    # Capture stdout\n",
    "    log_capture_string = io.StringIO()\n",
    "    sys.stdout = log_capture_string\n",
    "    DSS_File = r\"{}\".format(dss_entry[\"dss_filename\"])\n",
    "    Event_Periods = dss_entry[\"event_intervals\"]\n",
    "    color_ramp_vmax = dss_entry[\"precip_vmax\"]\n",
    "    Event_Name = DSS_File.split('_')[1]\n",
    "    Gridded_Data_Label = DSS_File.split('_')[0]\n",
    "    \n",
    "    # Gridded_Data_Label = \"MRMS\" or \"AORC\"\n",
    "\n",
    "    #clear_output(wait=True)\n",
    "    # Watershed Shapefile Folder + Output_Files + Event Name\n",
    "    \n",
    "    event_output_folder = os.path.join(output_files_folder, f\"{Event_Name}_{Gridded_Data_Label}_{GHNCD_Search_Buffer_Miles}mi\")\n",
    "    log_file_path = os.path.join(event_output_folder, 'event_log.txt')\n",
    "\n",
    "    print(f\"Working on {DSS_File} for the period {Event_Periods[0][0]} to {Event_Periods[0][1]}\")\n",
    "\n",
    "\n",
    "    # Step 1: Display DSS Catalog and Convert to Pandas Dataframe\n",
    "    pathname_pattern =\"/*/*/*/*/*/*/\"\n",
    "\n",
    "    DSS_Path = os.path.join(os.path.dirname(Watershed_Shapefile), DSS_Folder, DSS_File)\n",
    "\n",
    "    \n",
    "\n",
    "    # Check to see if output_files_folder folder exists\n",
    "    if not os.path.exists(output_files_folder):\n",
    "        os.mkdir(output_files_folder)\n",
    "\n",
    "    # If folder event_output_folder exists, delete it and create a new one, otherwise create it\n",
    "    if os.path.exists(event_output_folder):\n",
    "        shutil.rmtree(event_output_folder)\n",
    "        os.mkdir(event_output_folder)\n",
    "    else:\n",
    "        os.mkdir(event_output_folder)     \n",
    "\n",
    "\n",
    "\n",
    "    # Precip_CSV_Path = directory of watershed_shapefile + shapefile + CSV_Precipitation + Precipitation.csv\n",
    "    Precip_CSV_Path = os.path.join(os.path.dirname(Watershed_Shapefile), \"CSV_Precipitation\", \"Precipitation.csv\")\n",
    "    Total_precipitation_tiff_path = os.path.join(event_output_folder, \"Total_precipitation.tif\")\n",
    "    Precip_stations_shapefile_path = os.path.join(event_output_folder, \"Precip_stations.shp\")\n",
    "    Precip_stations_csv_path = os.path.join(event_output_folder, \"Precip_stations.csv\")\n",
    "    Report_Figure_Path = os.path.join(event_output_folder, f\"Report_Figure_{Event_Name}_{Gridded_Data_Label}_{GHNCD_Search_Buffer_Miles}mi.png\")\n",
    "    '''\n",
    "    PLEASE NOTE \n",
    "    in matplotlib, rasters must be masked to the plot extents or they will be distorted\n",
    "\n",
    "    '''\n",
    "\n",
    "\n",
    "\n",
    "    # Open DSS File and organize data\n",
    "\n",
    "    with Open(DSS_Path) as fid:\n",
    "        path_list = fid.getPathnameList(pathname_pattern,sort=1)\n",
    "        df = pd.DataFrame(path_list, columns=['Path'])\n",
    "        # Full pathname catalog as pandas dataframe\n",
    "        #display(df)\n",
    "\n",
    "    # Use pandas dataframe to store pathname catalog\n",
    "\n",
    "    # pathname is based on / separators, use the \"/\" as delimiter and break pathname into individual components\n",
    "    pathname_catalog_df = pd.DataFrame(df['Path'].str.split('/').tolist(), columns=['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'])\n",
    "    print(\"Pathname Catalog Dataframe (pathname_catalog_df):\")\n",
    "    #display(pathname_catalog_df)\n",
    "\n",
    "    # Data is hourly, first 2 pathnames are:\n",
    "    #/SHG/LWIREGION4/PRECIPITATION/01APR2016:0000/01APR2016:0100/AORC/\n",
    "    #/SHG/LWIREGION4/PRECIPITATION/01APR2016:0100/01APR2016:0200/AORC/\n",
    "\n",
    "    '''Sample of pathname_catalog_df dataframe:\n",
    "\n",
    "    A\tB\tC\tD\tE\tF\tG\tH\n",
    "    0\tSHG\tLWIREGION4\tPRECIPITATION\t01APR2016:0000\t01APR2016:0100\tAORC\n",
    "    ...\n",
    "    1584\t\tSHG\tLWIREGION4\tPRECIPITATION\t31MAR2016:2300\t31MAR2016:2400\tAORC\t\n",
    "    '''\n",
    "\n",
    "    # Transform the 'E' and 'F' columns to datetime format for proper sorting\n",
    "    def convert_to_datetime(date_str):\n",
    "        if \":2400\" in date_str:\n",
    "            date = pd.to_datetime(date_str.replace(\":2400\", \":0000\"), format='%d%b%Y:%H%M')\n",
    "            date += pd.Timedelta(days=1)\n",
    "            return date\n",
    "        return pd.to_datetime(date_str, format='%d%b%Y:%H%M')\n",
    "\n",
    "    pathname_catalog_df['E-T'] = pathname_catalog_df['E'].apply(convert_to_datetime)\n",
    "    pathname_catalog_df['F-T'] = pathname_catalog_df['F'].apply(convert_to_datetime)\n",
    "\n",
    "    # Sort pathname_catalog_df by 'E-T' column (transformed start date)\n",
    "    pathname_catalog_df = pathname_catalog_df.sort_values(by=['E-T'])\n",
    "\n",
    "    #Display the sorted pathname_catalog_df\n",
    "    # display(pathname_catalog_df)  #  Uncomment to debug\n",
    "\n",
    "    # Start Date = First Pathname's 'E-T' Column\n",
    "    dss_start_date = pathname_catalog_df['E-T'].iloc[0]\n",
    "    print(f\"DSS Start Date: {dss_start_date}\")\n",
    "\n",
    "    # Timestep = Second Pathname's 'F-T' Column - First Pathname's 'E-T' Column\n",
    "    dss_timestep = pathname_catalog_df['F-T'].iloc[0] - pathname_catalog_df['E-T'].iloc[0]\n",
    "    print(f\"DSS Timestep: {dss_timestep}\")\n",
    "\n",
    "    # End Date = Last Pathname's 'F-T' Column\n",
    "    dss_end_date = pathname_catalog_df['F-T'].iloc[-1]\n",
    "    print(f\"DSS End Date: {dss_end_date}\")\n",
    "    print(\"\")\n",
    "\n",
    "    # Duration = Last Pathname's 'F-T' Column - First Pathname's 'E-T' Column\n",
    "    dss_duration = pathname_catalog_df['F-T'].iloc[-1] - pathname_catalog_df['E-T'].iloc[0]\n",
    "    print(f\"DSS Total Duration: {dss_duration}\")\n",
    "\n",
    "    # Convert Event_Periods to datetime format\n",
    "    Event_Periods_dt = [(convert_to_datetime(start), convert_to_datetime(end)) for start, end in Event_Periods]\n",
    "    print(Event_Periods_dt)\n",
    "\n",
    "    # Convert the first tuple to a list\n",
    "    temp_list = list(Event_Periods_dt[0])\n",
    "\n",
    "    # Modify the list elements\n",
    "    if temp_list[0] < dss_start_date:\n",
    "        temp_list[0] = dss_start_date\n",
    "        print(f\"DSS Start Date is later than Event Start Date. Changing Event Start Date to {dss_start_date}\")\n",
    "    if temp_list[1] > dss_end_date:\n",
    "        temp_list[1] = dss_end_date\n",
    "        print(f\"DSS End Date is earlier than Event End Date. Changing Event End Date to {dss_end_date}\")\n",
    "\n",
    "    # Convert the list back to a tuple and replace the original tuple\n",
    "    Event_Periods_dt[0] = tuple(temp_list)\n",
    "\n",
    "    \n",
    "    # Using Event Periods, filter dataframe to only include pathnames that fall within the event periods\n",
    "    # Create a new dataframe pathname_catalog_for_event_periods_df\n",
    "    pathname_catalog_for_event_periods_df = pathname_catalog_df[pathname_catalog_df['E-T'].between(Event_Periods_dt[0][0], Event_Periods_dt[0][1])]\n",
    "\n",
    "    # Then, exclude E-T and F-T columns\n",
    "    pathname_catalog_for_event_periods_df = pathname_catalog_for_event_periods_df.drop(columns=['E-T', 'F-T'])\n",
    "\n",
    "    # Then create a new column with the full recreated pathname\n",
    "    pathname_catalog_for_event_periods_df['Path'] = pathname_catalog_for_event_periods_df.apply(lambda row: '/'.join(row.values.astype(str)), axis=1)\n",
    "\n",
    "    # display(pathname_catalog_for_event_periods_df)  # Debug only\n",
    "\n",
    "    # -------- Previous code block is to create pathname_catalog_for_event_periods_df dataframe --------\n",
    "\n",
    "    # EVENT-LEVEL TOTAL PRECIPITATION (Export GeoTIFF and Spatial Average Precipitation)\n",
    "\n",
    "\n",
    "    Event_Name_Title = re.sub(r'(\\D)(\\d)', r'\\1 \\2', Event_Name)\n",
    "\n",
    "\n",
    "\n",
    "    dss_start_title = str(dss_start_date).split(\" \")[0].split(\"-\")[1:]\n",
    "    dss_end_title = str(dss_end_date).split(\" \")[0].split(\"-\")[1:]\n",
    "\n",
    "    # Joining the split parts if needed (for example if you have MM-DD format)\n",
    "    dss_start_title = '-'.join(dss_start_title)\n",
    "    dss_end_title = '-'.join(dss_end_title)\n",
    "\n",
    "\n",
    "    Report_Figure_Title = f\"{Watershed_Name}\\nGHNCD Stations vs {Gridded_Data_Label} Gridded Precipitation\\n {Event_Name_Title}: {dss_start_title} to {dss_end_title}\"\n",
    "\n",
    "        \n",
    "    # Initialize a variable for total precipitation\n",
    "    total_precip = None\n",
    "\n",
    "    tmp_tiff_folder = os.path.join(event_output_folder, \"/tmp_tiff\")  \n",
    "\n",
    "    # Loop over each pathname in the filtered dataframe\n",
    "    count = 0  # Initialize counter\n",
    "    for pathname in pathname_catalog_for_event_periods_df['Path']:\n",
    "        # Open the DSS file\n",
    "        with Open(DSS_Path) as fid:\n",
    "            # Increment the count\n",
    "            count += 1\n",
    "\n",
    "            # Display the current pathname every 10 iterations\n",
    "            if count % 10 == 0:\n",
    "                print(\"Reading pathname:\", pathname)\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "            # Read the grid data from the DSS file\n",
    "            ds = fid.read_grid(pathname)\n",
    "            grid_array = ds.read()\n",
    "            ds._crs = rasterio.crs.CRS.from_epsg(Project_EPSG)\n",
    "            profile = ds.profile\n",
    "            #override profile to project epsg\n",
    "            profile['crs'] = rasterio.crs.CRS.from_epsg(Project_EPSG)\n",
    "            rio_dataset = ds.raster._as_rasterio_dataset()\n",
    "            xarr_data = rxr.open_rasterio(rio_dataset)\n",
    "            rio_dataset.close()\n",
    "\n",
    "            # If the total precipitation grid is not initialized, set it to the current grid\n",
    "            if total_precip is None:\n",
    "                total_precip = xarr_data\n",
    "            else:\n",
    "                # Add the current grid data to the total precipitation grid\n",
    "                total_precip += xarr_data\n",
    "\n",
    "            max_precip = total_precip.max().item()\n",
    "\n",
    "            if count % 10 == 0:\n",
    "                print(\"Max Precip:\", max_precip)\n",
    "    \n",
    "            \n",
    "    # Count number of pathnames in the filtered dataframe\n",
    "    print(\"Number of Pathnames: \", len(pathname_catalog_for_event_periods_df))\n",
    "\n",
    "    # Convert total_precip to inches\n",
    "    total_precip_inches = total_precip * 0.0393701\n",
    "\n",
    "    # Print max precip in MM and inches\n",
    "    max_precip = total_precip.max().item()\n",
    "    print(\"Max Precip: \", max_precip, \" MM\")\n",
    "    max_precip_inches = total_precip_inches.max().item()\n",
    "    print(\"Max Precip: \", max_precip_inches, \" Inches\")\n",
    "\n",
    "    # Write the total precipitation grid to a GeoTIFF file, only if it doesn't already exist\n",
    "    if not os.path.exists(Total_precipitation_tiff_path):\n",
    "        total_precip_inches.rio.to_raster(Total_precipitation_tiff_path)\n",
    "    else:\n",
    "        print(\"Total_Precipitation.tif already exists.\")\n",
    "\n",
    "    # THIS IS GOOD, THE TOTAL PRECIP TIFF LOOKS FINE IN QGIS\n",
    "\n",
    "    # Load the raster file using rioxarray\n",
    "    total_precip_inches_tiff = rxr.open_rasterio(Total_precipitation_tiff_path)\n",
    "\n",
    "    # Print max of total_precip_inches_tiff\n",
    "    print(\"Total Precipitation GeoTIFF Maximum Value: \", total_precip_inches_tiff.max().item())\n",
    "\n",
    "    # Display raster crs (SHOULD BE SHG, EPSG: 5070)\n",
    "    print(\"Total Precipitation GeoTIFF Coordinate System: \", total_precip_inches_tiff.rio.crs)\n",
    "\n",
    "    # Plot the total precipitation tiff file (full extents)\n",
    "    fig, ax = plt.subplots(figsize=(5,5))\n",
    "    cax = ax.imshow(total_precip_inches_tiff[0], cmap='GnBu')\n",
    "    plt.colorbar(cax, ax=ax, label=\"Inches Precipitation\")\n",
    "    ax.set_title(f\"Total Precipitation for {Event_Name}, {Gridded_Data_Label} (Full DSS Extents)\")\n",
    "    f\"{Event_Name}_{Gridded_Data_Label}_{GHNCD_Search_Buffer_Miles}mi\"\n",
    "    plt.show()\n",
    "\n",
    "    # -------- Previous code block is to create total_precip_inches_tiff --------\n",
    "\n",
    "    # Mask the total precipitation tiff file, calculate statistics and plot sampled data for visual inspection\n",
    "\n",
    "    # Import watershed shapefile as watershed_SHG (convert to SHG projection, EPSG: 5070)\n",
    "    watershed_SHG = gpd.read_file(Watershed_Shapefile)\n",
    "    # Print existing watershed projection\n",
    "    print(\"Watershed Original Projection: \", watershed_SHG.crs)\n",
    "    watershed_SHG = watershed_SHG.to_crs(\"ESRI:102039\")\n",
    "\n",
    "\n",
    "    # Find the largest polygon by area\n",
    "    largest_polygon = watershed_SHG.loc[watershed_SHG['geometry'].area.idxmax()]\n",
    "\n",
    "    # Extract XY coordinates from the largest polygon's exterior\n",
    "    coords = np.array(largest_polygon.geometry.exterior.coords)\n",
    "\n",
    "    # Calculate mean, min, and max for X and Y coordinates\n",
    "    mean_x, mean_y = np.mean(coords[:, 0]), np.mean(coords[:, 1])\n",
    "    print(\"Mean X: \", mean_x)\n",
    "    min_x, min_y = np.min(coords[:, 0]), np.min(coords[:, 1])\n",
    "    print(\"Min X: \", min_x)\n",
    "    max_x, max_y = np.max(coords[:, 0]), np.max(coords[:, 1])\n",
    "    print(\"Max X: \", max_x)\n",
    "\n",
    "    # Set the extents for the plot with a 10% buffer\n",
    "    plot_extents = [min_x - (max_x - min_x) * 1, max_x + (max_x - min_x) * 1, min_y - (max_y - min_y) * 1, max_y + (max_y - min_y) * 1]\n",
    "\n",
    "\n",
    "    # Mask the total precipitation tiff file to the plot extents\n",
    "    # This will ensure that the gridded data scales properly with the watershed shapefile\n",
    "\n",
    "    from shapely.geometry import box\n",
    "    import geopandas as gpd\n",
    "\n",
    "    # Create bounding box geometry from plot extents [minx, maxx, miny, maxy]\n",
    "    bounding_box = box(plot_extents[0], plot_extents[2], plot_extents[1], plot_extents[3])\n",
    "\n",
    "    # Create a GeoDataFrame with the bounding box\n",
    "    bbox_gdf = gpd.GeoDataFrame({'geometry': [bounding_box]}, crs=total_precip_inches_tiff.rio.crs)\n",
    "\n",
    "    # Clip the raster with the bounding box\n",
    "    clipped_total_precip_inches_tiff = total_precip_inches_tiff.rio.clip(bbox_gdf.geometry)\n",
    "\n",
    "\n",
    "\n",
    "    # Plot the total precipitation tiff file and watershed boundary in SHG\n",
    "    fig, ax = plt.subplots(figsize=(5,5))\n",
    "    cax = ax.imshow(clipped_total_precip_inches_tiff[0], cmap='GnBu', extent=plot_extents)\n",
    "    watershed_SHG.plot(ax=ax, facecolor='none', edgecolor='red')\n",
    "    plt.colorbar(cax, ax=ax, label=\"Inches Precipitation\")\n",
    "    ax.set_title(f\"Total Precipitation for {Event_Name}, {Gridded_Data_Label}, SHG\")\n",
    "    plt.show()\n",
    "    fig.savefig(os.path.join(event_output_folder, \"Report_Figure_TotalPrecip_and_Boundary_Zoomed.png\"))\n",
    "\n",
    "    # -------- Previous code block is to create clipped_total_precip_inches_tiff --------\n",
    "\n",
    "    # Rasterize the watershed polygon and calculate statistics within the watershed for the event period\n",
    "\n",
    "    # Import watershed shapefile as watershed_SHG (convert to SHG projection, EPSG: 5070)\n",
    "    watershed_SHG = gpd.read_file(Watershed_Shapefile)\n",
    "    # Print existing watershed projection\n",
    "    print(\"Watershed Original Projection: \", watershed_SHG.crs)\n",
    "    watershed_SHG = watershed_SHG.to_crs(\"ESRI:102039\")\n",
    "    print(\"Watershed New Projection: \", watershed_SHG.crs)\n",
    "\n",
    "    # Find the largest polygon by area\n",
    "    shapes = [(largest_polygon.geometry, 1)]\n",
    "\n",
    "    # Get extents of watershed_SHG before rasterizing\n",
    "    # Extract XY coordinates from the largest polygon's exterior\n",
    "    watershed_shg_coords_before_rasterizing = np.array(largest_polygon.geometry.exterior.coords)\n",
    "\n",
    "    # calculate extent of coordinates\n",
    "    watershed_shg_coords_before_rasterizing_extents = [np.min(watershed_shg_coords_before_rasterizing[:, 0]), np.min(watershed_shg_coords_before_rasterizing[:, 1]), np.max(watershed_shg_coords_before_rasterizing[:, 0]), np.max(watershed_shg_coords_before_rasterizing[:, 1])]\n",
    "    print(\"Watershed SHG Extents Before Rasterizing: \", watershed_shg_coords_before_rasterizing_extents)\n",
    "\n",
    "\n",
    "\n",
    "    import rasterio\n",
    "    from rasterio.mask import mask\n",
    "\n",
    "    # Read GeoTIFF using rasterio\n",
    "    with rasterio.open(Total_precipitation_tiff_path) as src:\n",
    "        # Print CRS of src\n",
    "        print(\"Total Precipitation GeoTIFF Coordinate System: \", src.crs)\n",
    "        geoms = [largest_polygon.geometry.__geo_interface__]\n",
    "        out_image, out_transform = mask(src, geoms, invert=False, crop=True)\n",
    "\n",
    "\n",
    "    masked_precip = xr.DataArray(out_image[0], coords={'y': range(out_image.shape[1]), 'x': range(out_image.shape[2])}, dims=['y', 'x'])\n",
    "\n",
    "    # Now 'masked_precip' contains the same data as 'total_precip_inches_tiff' but masked outside the largest_polygon.geometry\n",
    "\n",
    "\n",
    "    # Print number of points within the watershed\n",
    "    print(\"Number of points within the watershed: \", masked_precip.count().item())\n",
    "\n",
    "    # 3. Calculate the average, maximum, and minimum precipitation within the watershed for the event period\n",
    "    avg_precip_watershed = masked_precip.mean().item()\n",
    "    max_precip_watershed = masked_precip.max().item()\n",
    "    min_precip_watershed = masked_precip.min().item()\n",
    "\n",
    "    print(f\"Average precipitation within the watershed for the event period: {avg_precip_watershed:.2f} inches\")\n",
    "    print(f\"Maximum precipitation within the watershed for the event period: {max_precip_watershed:.2f} inches\")\n",
    "    print(f\"Minimum precipitation within the watershed for the event period: {min_precip_watershed:.2f} inches\")\n",
    "\n",
    "    # Create a text file in the event_output_folder to store the statistics\n",
    "    with open(os.path.join(event_output_folder, \"Precipitation_Statistics.txt\"), 'w') as f:\n",
    "        f.write(f\"Average precipitation within the watershed for the event period: {avg_precip_watershed:.2f} inches\\n\")\n",
    "        f.write(f\"Maximum precipitation within the watershed for the event period: {max_precip_watershed:.2f} inches\\n\")\n",
    "        f.write(f\"Minimum precipitation within the watershed for the event period: {min_precip_watershed:.2f} inches\\n\")\n",
    "    \n",
    "    # Create dataframe of statistics, each loop will append to this dataframe\n",
    "\n",
    "    # Specify the order of columns\n",
    "    columns = ['Event_Name', 'Gridded_Data_Label', 'Event_Start_Date', 'Event_End_Date', 'Event_Duration', 'Average_Precipitation', 'Maximum_Precipitation', 'Minimum_Precipitation']\n",
    "    \n",
    "    # Populate new_row dictionary with the calculated statistics and other details\n",
    "    new_row = {\n",
    "        'Event_Name': Event_Name,\n",
    "        'Gridded_Data_Label': DSS_File.split('_')[0],\n",
    "        'Event_Start_Date': dss_start_date,\n",
    "        'Event_End_Date': dss_end_date,\n",
    "        'Event_Duration': dss_duration,\n",
    "        'Average_Precipitation': avg_precip_watershed,\n",
    "        'Maximum_Precipitation': max_precip_watershed,\n",
    "        'Minimum_Precipitation': min_precip_watershed\n",
    "    }\n",
    "    print(new_row)\n",
    "    # Create a DataFrame from the new row\n",
    "    new_row_df = pd.DataFrame([new_row], columns=columns)\n",
    "\n",
    "    # Concatenate the existing DataFrame with the new row DataFrame\n",
    "    statistics_df = pd.concat([statistics_df, new_row_df], ignore_index=True)\n",
    "\n",
    "    # Save the DataFrame\n",
    "    statistics_df.to_csv(csv_path, index=False)\n",
    "    print(\"Statistics dataframe updated.\")\n",
    "\n",
    "\n",
    "    \n",
    "    # Extract XY coordinates from the largest polygon's exterior\n",
    "    coords = np.array(largest_polygon.geometry.exterior.coords)\n",
    "\n",
    "    # Calculate min, and max for X and Y coordinates for plot extents\n",
    "\n",
    "    # Calculate extents for the masked_precip\n",
    "    y_size, x_size = masked_precip.shape\n",
    "    x_min, y_max = out_transform * (0, 0)\n",
    "    x_max, y_min = out_transform * (x_size, y_size)\n",
    "\n",
    "    plot_extents_masked_precip = [x_min, x_max, y_min, y_max]\n",
    "\n",
    "    # Plot masked points and watershed boundary\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    cax = ax.imshow(masked_precip, cmap='GnBu', extent=plot_extents_masked_precip)\n",
    "    watershed_SHG.plot(ax=ax, facecolor='none', edgecolor='red')\n",
    "    plt.colorbar(cax, ax=ax, label=\"Inches Precipitation\")\n",
    "    ax.set_title(f\"Total Precipitation for {Event_Name}, {Gridded_Data_Label}, SHG\")\n",
    "    plt.show()\n",
    "    fig.savefig(os.path.join(event_output_folder, \"Report_Figure_TotalPrecip_Watershed_Boundary.png\"))\n",
    "\n",
    "\n",
    "    masked_total_precipitation_tiff_path = os.path.join(event_output_folder, \"Masked_total_precipitation.tif\")\n",
    "    '''\n",
    "    # Output masked_precip to a GeoTIFF file\n",
    "    with rasterio.open(Total_precipitation_tiff_path) as src:\n",
    "        profile = src.profile\n",
    "\n",
    "    ...\n",
    "    ...\n",
    "    ...\n",
    "\n",
    "    with rasterio.open(masked_total_precipitation_tiff_path, 'w', **profile) as dst:\n",
    "        dst.write(out_image)\n",
    "    print(\"Masked Total Precipitation GeoTIFF written to: \", masked_total_precipitation_tiff_path)\n",
    "    '''\n",
    "\n",
    "    # -------- Previous code block is to create masked_total_precipitation_tiff --------\n",
    "\n",
    "    # Read GHNCD Daily Precip CSV from GHNCD Script output\n",
    "\n",
    "    #Import Precip_CSV_Path as dataframe GHNCDailyPrecip_df\n",
    "    GHNCDailyPrecip_df = pd.read_csv(Precip_CSV_Path, low_memory=False)\n",
    "    #display(GHNCDailyPrecip_df)\n",
    "\n",
    "    '''GHNCDailyPrecip_df sample:\n",
    "        STATION\tNAME\tDATE\tLATITUDE\tLONGITUDE\tPRCP\tELEVATION\tPRCP_ATTRIBUTES\tSNOW\tSNOW_ATTRIBUTES\t...\tWT17\tWT17_ATTRIBUTES\tWT19\tWT19_ATTRIBUTES\tWT21\tWT21_ATTRIBUTES\tWT22\tWT22_ATTRIBUTES\tWV03\tWV03_ATTRIBUTES\n",
    "    0\tUS1LAAV0001\tBUNKIE 0.3 WSW, LA US\t2015-01-01\t30.951738\t-92.192111\t3.0\t24.1\t,,N\tNaN\tNaN\t...\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\n",
    "    1\tUS1LAAV0001\tBUNKIE 0.3 WSW, LA US\t2015-01-02\t30.951738\t-92.192111\t23.0\t24.1\t,,N\tNaN\tNaN\t...\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\n",
    "    '''\n",
    "\n",
    "    # From Event Periods, create a start and end date for the daily data\n",
    "    GHNCD_start_date = Event_Periods_dt[0][0].strftime(\"%Y-%m-%d\")\n",
    "    GHNCD_end_date = Event_Periods_dt[0][1].strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # Create an empty list to store data\n",
    "    summed_data = []\n",
    "\n",
    "\n",
    "    # Define transformers for coordinate conversion\n",
    "    transformer = pyproj.Transformer.from_crs(4326, 5070, always_xy=True)\n",
    "\n",
    "    # Loop through each unique station\n",
    "    for station in GHNCDailyPrecip_df['STATION'].unique():\n",
    "        # Filter GHNCDailyPrecip_df by station and date range\n",
    "        GHNCDailyPrecip_df_station = GHNCDailyPrecip_df[GHNCDailyPrecip_df['STATION'] == station]\n",
    "        latitude = GHNCDailyPrecip_df_station['LATITUDE'].iloc[0]\n",
    "        longitude = GHNCDailyPrecip_df_station['LONGITUDE'].iloc[0]\n",
    "        GHNCDailyPrecip_df_station_filtered = GHNCDailyPrecip_df_station[GHNCDailyPrecip_df_station['DATE'].between(GHNCD_start_date, GHNCD_end_date)]\n",
    "        name = GHNCDailyPrecip_df_station['NAME'].iloc[0]\n",
    "\n",
    "        # Get the sum of the PRCP column\n",
    "        GHNCDailyPrecip_df_station_sum = GHNCDailyPrecip_df_station_filtered['PRCP'].sum()\n",
    "        \n",
    "        # Convert coordinates to SHG projection\n",
    "        x, y = transformer.transform(longitude, latitude)\n",
    "        \n",
    "        # Append to the list\n",
    "        summed_data.append({\n",
    "            'STATION': station, \n",
    "            'NAME': name,\n",
    "            'PRCP_SUM': GHNCDailyPrecip_df_station_sum,\n",
    "            'LATITUDE': latitude,\n",
    "            'LONGITUDE': longitude,\n",
    "            'SHG X': x,\n",
    "            'SHG Y': y\n",
    "        })\n",
    "\n",
    "    # Convert the list to a dataframe\n",
    "    GHNCDailyPrecip_df_sum_event_period = pd.DataFrame(summed_data)\n",
    "\n",
    "    # Convert from tenths of MM to inches\n",
    "    GHNCDailyPrecip_df_sum_event_period['PRCP-IN'] = (GHNCDailyPrecip_df_sum_event_period['PRCP_SUM'] / 10) * 0.0393701\n",
    "\n",
    "    # Display the new dataframe\n",
    "    #display(GHNCDailyPrecip_df_sum_event_period)\n",
    "\n",
    "    '''GHNCDailyPrecip_df_sum_event_period sample:\n",
    "        STATION\tPRCP_SUM\tLATITUDE\tLONGITUDE\tSHG X\tSHG Y\tPRCP-IN\n",
    "    0\tUS1LAAV0001\t1446.0\t30.951738\t-92.192111\t362681.496986\t882366.707973\t5.692916\n",
    "    1\tUS1LABG0002\t1170.0\t30.471300\t-93.159080\t272210.873826\t825771.822095\t4.606302\n",
    "    2\tUS1LACC0003\t675.0\t30.232246\t-92.998641\t288417.148472\t799708.875420\t2.657482\n",
    "    '''\n",
    "\n",
    "    # SHG is EPSG: 5070 (Standard Hydrologic Grid)\n",
    "    # Using the \"SHG X\" and \"SHG Y\" columns from the dataframe GHNCDailyPrecip_df_sum_event_period, create a geodataframe with the points and PRCP-IN as the attribute\n",
    "\n",
    "\n",
    "\n",
    "    # Convert the \"SHG X\" and \"SHG Y\" columns into a geometry column\n",
    "    geometry = [Point(xy) for xy in zip(GHNCDailyPrecip_df_sum_event_period['SHG X'], GHNCDailyPrecip_df_sum_event_period['SHG Y'])]\n",
    "\n",
    "    # Create a GeoDataFrame using the geometry column and PRCP-IN as the attribute\n",
    "    geo_df = gpd.GeoDataFrame(GHNCDailyPrecip_df_sum_event_period, geometry=geometry, crs=\"EPSG:5070\")\n",
    "\n",
    "    # Display the first few rows of the GeoDataFrame\n",
    "    geo_df.head()\n",
    "\n",
    "    # -------- Previous code block is to create geo_df --------\n",
    "\n",
    "    # Extract Gridded Precipitation from Total Precipitation TIFF file and Plot with GHNC Precipitation\n",
    "\n",
    "    # Open the raster file and check its CRS\n",
    "    with rasterio.open(Total_precipitation_tiff_path) as src:\n",
    "        crs = src.crs\n",
    "\n",
    "    print(f\"The CRS of the raster file is: {crs}\")\n",
    "\n",
    "\n",
    "    # Load the raster data using rasterio\n",
    "    with rasterio.open(Total_precipitation_tiff_path) as src:\n",
    "        \n",
    "        # Create a function to extract values from the raster at given x, y coordinates\n",
    "        def extract_raster_value(x, y, raster):\n",
    "            for val in raster.sample([(x, y)]):\n",
    "                return val[0]\n",
    "\n",
    "        # Apply the function to each station's coordinates to get the raster values\n",
    "        geo_df['Grid_PRCP'] = geo_df.apply(lambda row: extract_raster_value(row['SHG X'], row['SHG Y'], src), axis=1)\n",
    "\n",
    "\n",
    "    # Display geodataframe\n",
    "    #display(geo_df)\n",
    "\n",
    "\n",
    "\n",
    "    # Filter Geodataframe, excluding stations greater than 10 miles from the watershed boundary\n",
    "    geo_df_filtered = geo_df[geo_df['geometry'].distance(largest_polygon.geometry) < GHNCD_Search_Buffer_Miles * 1609.34]\n",
    "\n",
    "    # Filter Geodataframe, excluding stations with Grid_PRCP less than 0.01 inches\n",
    "    geo_df_filtered = geo_df_filtered[geo_df_filtered['Grid_PRCP'] > 0.01]\n",
    "\n",
    "\n",
    "    # Write geodataframe to shapefile\n",
    "    geo_df_filtered.to_file(Precip_stations_shapefile_path)\n",
    "\n",
    "    # Write Station, Name and PRCP-IN and Grid_PRCP to CSV\n",
    "    geo_df_filtered[['STATION', 'NAME', 'SHG X', 'SHG Y', 'PRCP-IN', 'Grid_PRCP']].to_csv(Precip_stations_csv_path, index=False)\n",
    "\n",
    "    \n",
    "\n",
    "    # Existing extents of geo_df_filtered\n",
    "    x_min_filtered = geo_df_filtered['SHG X'].min()\n",
    "    x_max_filtered = geo_df_filtered['SHG X'].max()\n",
    "    y_min_filtered = geo_df_filtered['SHG Y'].min()\n",
    "    y_max_filtered = geo_df_filtered['SHG Y'].max()\n",
    "\n",
    "    # Existing extents of watershed_SHG\n",
    "    x_min_shg = watershed_shg_coords_before_rasterizing_extents[0] - GHNCD_Search_Buffer_Miles * 1609.34 / 1\n",
    "    x_max_shg = watershed_shg_coords_before_rasterizing_extents[2] + GHNCD_Search_Buffer_Miles * 1609.34 / 1\n",
    "    y_min_shg = watershed_shg_coords_before_rasterizing_extents[1] - GHNCD_Search_Buffer_Miles * 1609.34 / 1\n",
    "    y_max_shg = watershed_shg_coords_before_rasterizing_extents[3] + GHNCD_Search_Buffer_Miles * 1609.34 / 1\n",
    "\n",
    "    # Determine the greater extents for each boundary\n",
    "    x_min = min(x_min_filtered, x_min_shg) - (increase_x_axis * 1609.34) + (shift_x_axis * 1609.34)\n",
    "    x_max = max(x_max_filtered, x_max_shg) + (increase_x_axis * 1609.34) + (shift_x_axis * 1609.34)\n",
    "    y_min = min(y_min_filtered, y_min_shg) - (plotextentsbuffer_miles) * 1609.34 + (shift_y_axis * 1609.34)\n",
    "    y_max = max(y_max_filtered, y_max_shg) + (plotextentsbuffer_miles) * 1609.34 + (shift_y_axis * 1609.34)\n",
    "\n",
    "    # Set the greater extents for the plot\n",
    "    correct_plot_extents = [x_min, x_max, y_min, y_max]\n",
    "\n",
    "    # Mask the total precipitation tiff file to the plot extents\n",
    "\n",
    "    from shapely.geometry import box\n",
    "    import geopandas as gpd\n",
    "    import rasterio\n",
    "    from rasterio.mask import mask\n",
    "\n",
    "    # Create bounding box geometry from plot extents [minx, maxx, miny, maxy]\n",
    "    bounding_box = box(correct_plot_extents[0], correct_plot_extents[2], correct_plot_extents[1], correct_plot_extents[3])\n",
    "\n",
    "    # Create a GeoDataFrame with the bounding box\n",
    "    bbox_gdf = gpd.GeoDataFrame({'geometry': [bounding_box]}, crs=crs)  # Assuming crs is already defined in your code\n",
    "\n",
    "    with rasterio.open(Total_precipitation_tiff_path) as src:\n",
    "        # Convert bounding box to the same CRS as the raster\n",
    "        bbox_gdf = bbox_gdf.to_crs(src.crs)\n",
    "        \n",
    "        # Mask the raster data with the bounding box\n",
    "        out_image, out_transform = mask(src, bbox_gdf.geometry, crop=True)\n",
    "\n",
    "    \n",
    "\n",
    "    '''\n",
    "    Example\n",
    "\n",
    "    # Mask the total precipitation tiff file to the plot extents\n",
    "    # This will ensure that the gridded data scales properly with the watershed shapefile\n",
    "\n",
    "    # Create bounding box geometry from plot extents [minx, maxx, miny, maxy]\n",
    "    bounding_box_report = box(plot_extents[0], plot_extents[2], plot_extents[1], plot_extents[3])\n",
    "\n",
    "    # Create a GeoDataFrame with the bounding box\n",
    "    bbox_gdf_report = gpd.GeoDataFrame({'geometry': [bounding_box]}, crs=total_precip_inches_tiff.rio.crs)\n",
    "\n",
    "    # Clip the raster with the bounding box\n",
    "    clipped_report_total_precip_inches_tiff = total_precip_inches_tiff.rio.clip(bbox_gdf.geometry)\n",
    "    '''\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 9))\n",
    "\n",
    "    # Plot the total precipitation TIFF file (masked)\n",
    "    #cax = ax.imshow(out_image[0], cmap='GnBu', vmin=min_precip_watershed, vmax=max_precip_watershed, extent=correct_plot_extents)\n",
    "    cax = ax.imshow(out_image[0], cmap='GnBu', vmin=min_precip_watershed, vmax=color_ramp_vmax, extent=correct_plot_extents)\n",
    "\n",
    "    # Plot the watershed boundary\n",
    "    watershed_SHG.plot(ax=ax, facecolor='none', edgecolor='red')\n",
    "    # Plot the GeoDataFrame with station data\n",
    "    geo_df_filtered.plot(ax=ax, color='black', markersize=7)\n",
    "    # Add labels to each point (both original and AORC)\n",
    "    for x, y, label, aorc, name in zip(geo_df_filtered['SHG X'], geo_df_filtered['SHG Y'], geo_df_filtered['PRCP-IN'], geo_df_filtered['Grid_PRCP'], geo_df_filtered['NAME']):\n",
    "        ax.text(x, y, f'{label:.2f}  \\n{name}  ', fontsize=6, ha='right', va='center')\n",
    "        ax.text(x, y, f'   {aorc:.2f}\\n  {Gridded_Data_Label}', fontsize=6, ha='left', va='center')\n",
    "    # Set the extents based on the geo_df_filtered data\n",
    "    ax.set_xlim([x_min, x_max])\n",
    "    ax.set_ylim([y_min, y_max])\n",
    "    # Add a colorbar and title\n",
    "    plt.colorbar(cax, ax=ax, label=\"Inches Precipitation\")\n",
    "    ax.set_title(Report_Figure_Title)\n",
    "    # Show the combined plot\n",
    "    plt.show()\n",
    "\n",
    "    # Save plot to file Report_Figure_Path\n",
    "    fig.savefig(Report_Figure_Path)\n",
    "    display(geo_df_filtered)\n",
    "\n",
    "    # Write Log File\n",
    "    sys.stdout = sys.__stdout__\n",
    "    log_contents = log_capture_string.getvalue()\n",
    "    with open(log_file_path, 'w') as f:\n",
    "        f.write(log_contents)\n",
    "    log_contents = []\n",
    "\n",
    "    # Close Rasterio Dataset\n",
    "    rio_dataset.close()\n",
    "    \n",
    "\n",
    "# Save the DataFrame after the loop\n",
    "statistics_df.to_csv(csv_path, index=False)\n",
    "print(\"Statistics dataframe updated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search all subfolders and child subfolders of output_files_folder for Precip_stations.csv\n",
    "\n",
    "\n",
    "'''Example of Precip_stations.csv\n",
    "STATION\tNAME\tSHG X\tSHG Y\tPRCP-IN\tGrid_PRCP\n",
    "US1LABG0002\tRAGLEY 5.0 SE, LA US\t272210.8738\t825771.8221\t0\t5.4645705\n",
    "USC00162641\tDRY CREEK, LA US\t273269.0971\t855829.9724\t5.30315247\t6.3082705\n",
    "USC00165266\tLEESVILLE, LA US\t262324.2722\t900025.6272\t9.17717031\t9.112999\n",
    "USC00166938\tOBERLIN FIRE TOWER, LA US\t308605.782\t841640.1602\t5.29134144\t4.7503963\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "# Compile Event CSV Outputs for GHNCD vs Gridded Comparisons\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def search_csv_files(folder):\n",
    "    csv_files = []\n",
    "    for root, _, files in os.walk(folder):\n",
    "        for file in files:\n",
    "            if file.endswith(\"Precip_stations.csv\"):\n",
    "                csv_files.append(os.path.join(root, file))\n",
    "    return csv_files\n",
    "\n",
    "output_files_folder = os.path.join(os.path.dirname(Watershed_Shapefile), \"Output_Files\")\n",
    "def main():\n",
    "    csv_files = search_csv_files(output_files_folder)\n",
    "    \n",
    "    all_dataframes = []\n",
    "    \n",
    "    for csv_file in csv_files:\n",
    "        # Read the CSV files into dataframes\n",
    "        df = pd.read_csv(csv_file)\n",
    "        \n",
    "        # Using the parent folder name as the Event Name, create a new column in the dataframe\n",
    "        parent_folder = os.path.basename(os.path.dirname(csv_file))\n",
    "        df['Event Name'] = parent_folder\n",
    "        \n",
    "        all_dataframes.append(df)\n",
    "    \n",
    "    # Concatenate the dataframes into one dataframe\n",
    "    final_dataframe = pd.concat(all_dataframes, ignore_index=True)\n",
    "    \n",
    "    # Calculate PRCP-IN minus Grid_PRCP and add a new column to the dataframe named \"GHNCD - GRIDDED\"\n",
    "    final_dataframe[\"GHNCD - GRIDDED\"] = final_dataframe[\"PRCP-IN\"] - final_dataframe[\"Grid_PRCP\"]\n",
    "    \n",
    "    # Save the dataframe to a CSV file in the output_files_folder named \"GHNCD_vs_Gridded.csv\"\n",
    "    final_dataframe.to_csv(os.path.join(output_files_folder, \"GHNCD_vs_Gridded.csv\"), index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testtest2-311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
