{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Atlas 14 ASC Grid Postprocessing Script\n",
    "Script 2/3 for Atlas 14 Spatial Variance Analysis\n",
    "\n",
    "Author: William (Bill) Katzenmeyer, P.E., C.F.M. (C.H. Fenstermaker and Associates, LLC) \n",
    "\n",
    "Source: https://github.com/billk-FM/HEC-Commander-Tools\n",
    "\n",
    "#### NOAA Data Source:\n",
    "https://hdsc.nws.noaa.gov/pub/hdsc/data/tx/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 Define file paths to watershed polygon, state polygon, and example asc files to be used as general figures\n",
    "watershed_boundary_file = r'Region4_HUC_Boundaries.geojson'\n",
    "state_boundary_file = r'State_Boundary.geojson'\n",
    "asc_file_name_1 = r'LWI_Region4/se50yr06ha/se50yr06ha.asc'\n",
    "asc_file_name_2 = r'LWI_Region4/tx50yr06ha/tx50yr06ha.asc'\n",
    "\n",
    "# Default CRS assumption for asc files \n",
    "asc_file_default_EPSG = \"4269\"\n",
    "\n",
    "# Target CRS for all script operations and outputs\n",
    "reproject_to_epsg = \"4269\"\n",
    "\n",
    "# Input Directory with combined ASC File Datasets (this should come from a previous step on revision)\n",
    "input_directory = r'LWI_Region4'\n",
    "\n",
    "# Set the base folder path\n",
    "base_folder = r'LWI_Region4'\n",
    "\n",
    "# Output Directory for PNG and CSV Outputs\n",
    "import os\n",
    "output_directory = os.path.join(input_directory, 'Watershed_Statistical_Analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 Automatically Import and Install Libraries\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_and_import(package_name, import_name=None):\n",
    "    if import_name is None:\n",
    "        import_name = package_name\n",
    "    try:\n",
    "        __import__(import_name)\n",
    "    except ImportError:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package_name])\n",
    "        globals()[import_name] = __import__(import_name)\n",
    "\n",
    "# Installation and import statements\n",
    "install_and_import(\"os\")\n",
    "install_and_import(\"numpy\")\n",
    "install_and_import(\"rioxarray\")\n",
    "install_and_import(\"matplotlib.pyplot\", \"matplotlib\")\n",
    "install_and_import(\"geopandas\", \"geopandas\")\n",
    "install_and_import(\"pyproj\")\n",
    "install_and_import(\"json\")\n",
    "install_and_import(\"shapely.geometry\", \"shapely\")\n",
    "install_and_import(\"xarray\")\n",
    "install_and_import(\"affine\")\n",
    "install_and_import(\"rasterio\")\n",
    "install_and_import(\"tqdm\")\n",
    "install_and_import(\"shutil\")\n",
    "install_and_import(\"pandas\")\n",
    "install_and_import(\"pathlib\")\n",
    "install_and_import(\"IPython.display\", \"IPython\")\n",
    "\n",
    "# Import statements\n",
    "import os\n",
    "import numpy as np\n",
    "import rioxarray\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "from pyproj import CRS\n",
    "import json\n",
    "from shapely.geometry import shape\n",
    "import xarray as xr\n",
    "from affine import Affine\n",
    "from rasterio.enums import Resampling\n",
    "from tqdm import tqdm\n",
    "import rasterio\n",
    "import shutil\n",
    "from rasterio.transform import from_origin\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from IPython.display import display\n",
    "import geopandas as gpd\n",
    "import rioxarray\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import mapping, box\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rasterio.transform import from_origin\n",
    "from rasterio.warp import Resampling\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 Extract metadata from file name and add to dataframe asc_file_names with column \"filename\" \"return interval\" \"duration\", \"duration_units\", \"duration_hours\"\n",
    "\n",
    "def extract_metadata(filename):\n",
    "    # Extract the base filename without path\n",
    "    print(\"/n-----   Extracting Metadata   -----\")\n",
    "\n",
    "    base_filename = os.path.basename(filename)\n",
    "    \n",
    "    # Extract return interval (numbers before \"yr\")\n",
    "    return_interval_match = re.search(r'(\\d+)yr', base_filename)\n",
    "    if return_interval_match:\n",
    "        return_interval = int(return_interval_match.group(1))\n",
    "    else:\n",
    "        raise ValueError(f\"Unable to extract return interval from filename: {base_filename}\")\n",
    "    \n",
    "    # Extract duration (2 numbers before \"ha\", \"da\", or \"ma\", which denote Hours Days or Months)\n",
    "    duration_match = re.search(r'(\\d{2})(ha|da|ma)', base_filename)\n",
    "    if duration_match:\n",
    "        duration = int(duration_match.group(1))\n",
    "        duration_units = duration_match.group(2)\n",
    "        if duration_units == \"ha\":\n",
    "            duration_hours = duration\n",
    "        elif duration_units == \"da\":\n",
    "            duration_hours = duration * 24\n",
    "        elif duration_units == \"ma\":\n",
    "            duration_hours = duration * 24 * 30  # Months to Hours\n",
    "    else:\n",
    "        raise ValueError(f\"Unable to extract duration from filename: {base_filename}\")\n",
    "    \n",
    "    return {\n",
    "        \"filename\": filename,\n",
    "        \"return_interval\": return_interval,\n",
    "        \"duration\": duration,\n",
    "        \"duration_units\": duration_units,\n",
    "        \"duration_hours\": duration_hours\n",
    "    }\n",
    "\n",
    "# Create a list of ASC filenames\n",
    "asc_filenames = [asc_file_name_1, asc_file_name_2]\n",
    "\n",
    "# Initialize the DataFrame if it doesn't exist\n",
    "if 'asc_file_names' not in globals():\n",
    "    asc_file_names = pd.DataFrame(columns=[\"filename\", \"return_interval\", \"duration\", \"duration_units\", \"duration_hours\"])\n",
    "\n",
    "# Extract metadata for each file\n",
    "metadata = []\n",
    "for filename in asc_filenames:\n",
    "    try:\n",
    "        # Check if the filename already exists in the DataFrame\n",
    "        if filename in asc_file_names['filename'].values:\n",
    "            print(f\"Skipping {filename} as it already exists in the DataFrame.\")\n",
    "            continue\n",
    "        metadata.append(extract_metadata(filename))\n",
    "    except ValueError as e:\n",
    "        print(f\"Error processing {filename}: {str(e)}\")\n",
    "\n",
    "# Append new data to the DataFrame\n",
    "if metadata:\n",
    "    new_data = pd.DataFrame(metadata)\n",
    "    asc_file_names = pd.concat([asc_file_names, new_data], ignore_index=True)\n",
    "\n",
    "print(\"asc_file_names:\")\n",
    "display(asc_file_names)\n",
    "\n",
    "\n",
    "\n",
    "# Load the GeoJSON files\n",
    "watershed_boundary_gdf = gpd.read_file(watershed_boundary_file)\n",
    "state_boundary_gdf = gpd.read_file(state_boundary_file)\n",
    "\n",
    "# Print the GeoDataFrame names and display them\n",
    "print(\"watershed_boundary_gdf\")\n",
    "display(watershed_boundary_gdf)\n",
    "\n",
    "print(\"state_boundary_gdf\")\n",
    "display(state_boundary_gdf)\n",
    "\n",
    "# Reproject GeoDataFrames to the target CRS\n",
    "target_crs = f\"EPSG:{reproject_to_epsg}\"\n",
    "watershed_boundary_gdf = watershed_boundary_gdf.to_crs(target_crs)\n",
    "state_boundary_gdf = state_boundary_gdf.to_crs(target_crs)\n",
    "\n",
    "# Print CRS information to verify\n",
    "print(\"state_gdf CRS:\", state_boundary_gdf.crs)\n",
    "print(\"watershed_gdf CRS:\", watershed_boundary_gdf.crs)\n",
    "\n",
    "# Merge the boundaries into one GeoDataFrame\n",
    "all_polygons = gpd.GeoDataFrame(pd.concat([state_boundary_gdf, watershed_boundary_gdf], ignore_index=True))\n",
    "\n",
    "# Print CRS information of the combined GeoDataFrame\n",
    "print(\"all_polygons CRS:\", all_polygons.crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and test functions\n",
    "Uncomment plt.show() lines if you want to see plots as they are generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4 Define Function to Combine ASC Files\n",
    "\n",
    "import os\n",
    "\n",
    "# Assuming asc_file_name_1 is defined earlier in your script\n",
    "# Example: asc_file_name_1 = 'LWI_Region4/se50yr06ha/se50yr06ha.asc'\n",
    "\n",
    "\n",
    "def combine_asc_files(asc_file_name_1, asc_file_name_2):\n",
    "\n",
    "    print(\"/n-----   Combining multistate asc files   -----\")\n",
    "    # Generate .prj file names for the ASC files\n",
    "    asc_prj_name_1 = asc_file_name_1.replace('.asc', '.prj')\n",
    "    asc_prj_name_2 = asc_file_name_2.replace('.asc', '.prj')\n",
    "\n",
    "    print(f\"ASC file 1 PRJ file: {asc_prj_name_1}\")\n",
    "    print(f\"ASC file 2 PRJ file: {asc_prj_name_2}\")\n",
    "\n",
    "    # Load the .asc files with rioxarray\n",
    "    raster_1 = rioxarray.open_rasterio(asc_file_name_1)\n",
    "    raster_2 = rioxarray.open_rasterio(asc_file_name_2)\n",
    "\n",
    "    # read CRS from prj and set it for both rasters\n",
    "    with open(asc_prj_name_1, 'r') as prj_file:\n",
    "        crs = prj_file.read()\n",
    "    raster_1.rio.set_crs(crs, inplace=True)\n",
    "    raster_2.rio.set_crs(crs, inplace=True)\n",
    "\n",
    "\n",
    "    # Print extents of the .asc files\n",
    "    #print(\"raster_1 extent:\", raster_1.rio.bounds())\n",
    "    #print(\"raster_2 extent:\", raster_2.rio.bounds())\n",
    "\n",
    "    # Calculate the extent of the watersheds GeoDataFrame\n",
    "    watersheds_bounds = all_polygons.total_bounds\n",
    "    left, bottom, right, top = watersheds_bounds\n",
    "\n",
    "    # Expand the extent by 10%\n",
    "    width = right - left\n",
    "    height = top - bottom\n",
    "    left -= width * 0.1\n",
    "    right += width * 0.1\n",
    "    bottom -= height * 0.1\n",
    "    top += height * 0.1\n",
    "\n",
    "    # Print combined extent values\n",
    "    print(f\"Combined extent (watersheds +10%): left={left}, bottom={bottom}, right={right}, top={top}\")\n",
    "\n",
    "    # Calculate new dimensions based on raster_1's resolution\n",
    "    resolution_x, resolution_y = raster_1.rio.resolution()\n",
    "    #print(f\"Resolution of raster_1: x={resolution_x}, y={resolution_y}\")\n",
    "\n",
    "    # Take absolute value of y-resolution for correct height calculation\n",
    "    width = int((right - left) / abs(resolution_x))\n",
    "    height = int((top - bottom) / abs(resolution_y))\n",
    "\n",
    "    # Print new dimensions\n",
    "    #print(f\"New dimensions: width={width}, height={height}\")\n",
    "\n",
    "    # Check if dimensions are valid\n",
    "    if width <= 0 or height <= 0:\n",
    "        raise ValueError(\"Calculated dimensions are not valid. Width and height must be positive.\")\n",
    "\n",
    "    # Create new transform for the combined extent\n",
    "    new_transform = from_origin(left, top, abs(resolution_x), abs(resolution_y))\n",
    "    #print(f\"New transform: {new_transform}\")\n",
    "\n",
    "    # Reproject raster_1 to the new dimensions and combined extent\n",
    "    raster_1_extended = raster_1.rio.reproject(\n",
    "        raster_1.rio.crs,\n",
    "        transform=new_transform,\n",
    "        shape=(height, width),\n",
    "        resampling=Resampling.nearest\n",
    "    )\n",
    "    print(\"Reprojected raster_1_extended\")\n",
    "\n",
    "    # Align raster_2 to the coordinates of the extended raster_1\n",
    "    raster_2_aligned = raster_2.rio.reproject_match(raster_1_extended)\n",
    "    print(\"Aligned raster_2 to raster_1_extended\")\n",
    "\n",
    "    raster_2_bounds = raster_2_aligned.rio.bounds()\n",
    "    #print(\"Raster 2 bounds, as calculated after reindex:\", raster_2_bounds) # debug only\n",
    "\n",
    "    merged_raster = raster_1_extended.where(raster_1_extended != -9, raster_2_aligned)\n",
    "\n",
    "    # Print the actual extents of merged_raster\n",
    "    merged_raster_extents = merged_raster.rio.bounds()\n",
    "    print(\"merged_raster extent, as calculated after interpolation:\", merged_raster_extents)\n",
    "\n",
    "    #print(f\"left: {left}\")\n",
    "    #print(f\"bottom: {bottom}\")\n",
    "    #print(f\"right: {right}\")\n",
    "    #print(f\"top: {top}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # base_folder is defined above\n",
    "    # combined asc folder name the same as  asc_file_name_1, but with the first two characters of the folder name replaced with 'cb'\n",
    "    \n",
    "\n",
    "    asc_base_file_name = 'cb' + os.path.basename(asc_file_name_1)[2:]\n",
    "    #print(f\"asc_base_file_name: {asc_base_file_name}\")\n",
    "    asc_base_without_ext = os.path.splitext(asc_base_file_name)[0]  # remove the .asc from the file name, Example cb50yr06ha.asc becomes cb50yr06ha\n",
    "    #print(f\"asc_base_without_ext: {asc_base_without_ext}\")\n",
    "    \n",
    "    combined_asc_folder_name = os.path.join(base_folder, asc_base_without_ext)\n",
    "    #print(f\"combined_asc_folder_name: {combined_asc_folder_name}\")\n",
    "\n",
    "\n",
    "    combined_asc_file_name = f\"cb{os.path.basename(asc_base_without_ext)[2:]}\"\n",
    "\n",
    "    \n",
    "    combined_asc_file_path = os.path.join(base_folder, asc_base_without_ext, asc_base_file_name)\n",
    "\n",
    "\n",
    "    # Step 1: Extract file paths and names\n",
    "    xml_filename = asc_file_name_1.replace(r'.asc', r'.xml')\n",
    "    prj_filename = asc_file_name_1.replace(r'.asc', r'.prj')\n",
    "\n",
    "    #print(f\"xml_filename: {xml_filename}\")\n",
    "    #print(f\"prj_filename: {prj_filename}\")\n",
    "\n",
    "    # Step 2: Split the path to get the directory and file name\n",
    "    xml_path, xml_file = os.path.split(xml_filename)\n",
    "    prj_path, prj_file = os.path.split(prj_filename)\n",
    "\n",
    "    # Step 3: Split the path to get all folder names\n",
    "    xml_path_parts = xml_path.split(os.sep)\n",
    "    prj_path_parts = prj_path.split(os.sep)\n",
    "\n",
    "    # Step 4: Replace the first two letters of the second folder level with \"cb\"\n",
    "    # Ensure there are at least two parts in the path\n",
    "    if len(xml_path_parts) > 1:\n",
    "        xml_path_parts[1] = 'cb' + xml_path_parts[1][2:]\n",
    "    if len(prj_path_parts) > 1:\n",
    "        prj_path_parts[1] = 'cb' + prj_path_parts[1][2:]\n",
    "\n",
    "    # Step 5: Reconstruct the new paths\n",
    "    new_xml_path = os.sep.join(xml_path_parts)\n",
    "    new_prj_path = os.sep.join(prj_path_parts)\n",
    "\n",
    "    # Step 6: Replace the first two letters of the file name with \"cb\"\n",
    "    new_xml_file = 'cb' + xml_file[2:]\n",
    "    new_prj_file = 'cb' + prj_file[2:]\n",
    "\n",
    "    # Step 7: Combine the modified paths and file names\n",
    "    xml_cb_sub = os.path.join(new_xml_path, new_xml_file).replace(os.sep, '/')\n",
    "    prj_cb_sub = os.path.join(new_prj_path, new_prj_file).replace(os.sep, '/')\n",
    "\n",
    "    #print(f\"xml_cb_sub: {xml_cb_sub}\")\n",
    "    #print(f\"prj_cb_sub: {prj_cb_sub}\")\n",
    "\n",
    "    xml_renamed = xml_cb_sub\n",
    "    prj_renamed = prj_cb_sub\n",
    "\n",
    "    #print(f\"xml_renamed: {xml_renamed}\")\n",
    "    #print(f\"prj_renamed: {prj_renamed}\")\n",
    "\n",
    "    # Convert merged_raster values from 1000ths of an inch to inches\n",
    "    merged_raster = merged_raster / 1000\n",
    "\n",
    "\n",
    "    # Plot the merged raster and the polygon boundaries with extent defined by combined_bounds\n",
    "    # Extract metadata from the filename\n",
    "    metadata = extract_metadata(asc_file_name_1)\n",
    "    # Create the plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    cbar = merged_raster.plot(ax=ax, cmap='viridis', vmin=0, vmax=merged_raster.max())\n",
    "    cbar.colorbar.set_label(\"Total Precipitation (Inches)\")\n",
    "    all_polygons.boundary.plot(ax=ax, edgecolor='red')\n",
    "    plt.xlim(left, right)\n",
    "    plt.ylim(bottom, top)\n",
    "\n",
    "    # Set the title with duration and return interval information\n",
    "    plt.title(f\"Atlas 14 \\n Return Interval: {metadata['return_interval']} years, Duration: {metadata['duration']} {metadata['duration_units']}\")\n",
    "\n",
    "    # Label the axes\n",
    "    ax.set_xlabel(\"Longitude, Degrees\")\n",
    "    ax.set_ylabel(\"Latitude, Degrees\")\n",
    "\n",
    "    #plt.show()\n",
    "\n",
    "    # Create the combined folder if it doesn't exist\n",
    "    os.makedirs(combined_asc_folder_name, exist_ok=True)\n",
    "\n",
    "    # use shutil to copy xml_filename and prj_filename to their renamed paths\n",
    "    # Use shutil to copy xml_filename and prj_filename to their renamed paths\n",
    "    shutil.copy(xml_filename , xml_renamed)\n",
    "    print(f\"Copied {xml_filename} to {xml_renamed}\")\n",
    "    \n",
    "    shutil.copy(prj_filename, prj_renamed)\n",
    "    print(f\"Copied {prj_filename} to {prj_renamed}\")\n",
    "\n",
    "\n",
    "\n",
    "    # Save the merged raster as a new ASC file\n",
    "    merged_raster.rio.to_raster(combined_asc_file_path)\n",
    "    print(f\"Combined ASC file saved: {combined_asc_file_path}\")\n",
    "\n",
    "    # Close rio raster file\n",
    "    # Close the raster datasets\n",
    "    raster_1.close()\n",
    "    raster_2.close()\n",
    "    merged_raster.close()\n",
    "\n",
    "    #print(\"Raster datasets closed successfully.\")\n",
    "\n",
    "    # Generate plot file name\n",
    "    plot_file_name = os.path.join(combined_asc_folder_name, f\"cb{os.path.basename(asc_file_name_1)[2:-4]}_plot.png\")\n",
    "\n",
    "\n",
    "    input_directory = os.path.dirname(os.path.dirname(asc_file_name_1))\n",
    "    output_directory = os.path.join(input_directory, 'Watershed_Statistical_Analysis')\n",
    "\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory, exist_ok=True)\n",
    "        #print(f\"Output directory created: {output_directory}\")\n",
    "    else:\n",
    "        print(f\"Output directory already exists: {output_directory}\")\n",
    "\n",
    "\n",
    "    # Save the plot in the output directory\n",
    "    plot_file_name = os.path.join(output_directory, f\"AA - Regional Report Figure cb{os.path.basename(asc_file_name_1)[2:-4]}_plot.png\")\n",
    "    plt.savefig(plot_file_name)\n",
    "    print(f\"Plot saved: {plot_file_name}\")\n",
    "\n",
    "    # Close the plot to free up memory\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return combined_asc_file_path, plot_file_name, merged_raster\n",
    "\n",
    "\n",
    "# Generate combined folder and file names\n",
    "\n",
    "\n",
    "combined_asc_file_path, plot_file_name, merged_raster = combine_asc_files(asc_file_name_1, asc_file_name_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5 Define Function to Produce Report Plot and Statistics for each polygon in watershed_boundary_file\n",
    "\n",
    "# INPUTS: input_dir is 2 levels up from asc_file_name_1 = r'LWI_Region4/se50yr06ha/se50yr06ha.asc' (in this example, /LWI_Region4)\n",
    "# OUTPUT: \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import mapping\n",
    "import pandas as pd\n",
    "\n",
    "def calculate_grid_statistics_by_polygon(combined_asc_file_path, merged_raster, polygon_index, watershed_boundary_gdf):\n",
    "    # Define the input and output directories\n",
    "    results_dataframe=None\n",
    "    print(\"/n-----   Calculating Statistics for Each Watershed Polygon   -----\")\n",
    "\n",
    "    if results_dataframe is None:\n",
    "        results_dataframe = pd.DataFrame(columns=['File Name', 'Max (inches)', 'Min (inches)', 'Mean (inches)', 'Range (%)', 'polygon_name'])\n",
    "\n",
    "    #print(\"Processing merged raster...\")\n",
    "\n",
    "    try:\n",
    "        # Use the specified polygon in watershed_boundary_gdf\n",
    "        polygon = watershed_boundary_gdf.iloc[[polygon_index]]\n",
    "\n",
    "        polygon_name = polygon['name'].values[0]\n",
    "        #print(f\"Polygon {polygon_index} converted to simple geometry\")\n",
    "\n",
    "        polygon = polygon.set_crs(watershed_boundary_gdf.crs)\n",
    "        print(f\"Polygon {polygon_index} selected from watershed_boundary_gdf\")\n",
    "\n",
    "        # Print extents of polygon\n",
    "        polygon_bounds = polygon.geometry.bounds\n",
    "        #print(\"Polygon bounds:\")\n",
    "        #display(polygon_bounds)\n",
    "\n",
    "        # Calculate 10% larger extents\n",
    "        x_min, y_min, x_max, y_max = polygon_bounds.iloc[0]\n",
    "        x_buffer = (x_max - x_min) * 0.1\n",
    "        y_buffer = (y_max - y_min) * 0.1\n",
    "        expanded_bounds = (x_min - x_buffer, y_min - y_buffer, x_max + x_buffer, y_max + y_buffer)\n",
    "        #print(f\"Expanded bounds: {expanded_bounds}\")\n",
    "\n",
    "        # Convert the polygon's geometry to a format suitable for clipping\n",
    "        polygon_geom = [mapping(polygon.geometry.iloc[0])]\n",
    "\n",
    "        # If merged_raster hasn't been loaded, \n",
    "\n",
    "\n",
    "        # Clip the merged raster with the polygon's geometry\n",
    "        clipped_raster = merged_raster.rio.clip(polygon_geom, watershed_boundary_gdf.crs)\n",
    "        #print(f\"Merged raster clipped with polygon {polygon_index}\")\n",
    "\n",
    "        # Remove no-data values (assuming -9 is the no-data value)\n",
    "        clipped_raster = clipped_raster.where(clipped_raster != -9)\n",
    "        #print(\"No-data values removed from clipped raster\")\n",
    "\n",
    "        # Calculate statistics\n",
    "        max_value = clipped_raster.max().values.item()\n",
    "        min_value = clipped_raster.min().values.item()\n",
    "        mean_value = clipped_raster.mean().values.item()\n",
    "        range_percentage = ((max_value - min_value) / max_value) * 100\n",
    "        print(f\"Statistics calculated: Max={max_value}, Min={min_value}, Mean={mean_value}, Range%={range_percentage}\")\n",
    "\n",
    "        # Append results to the DataFrame\n",
    "        new_row = pd.DataFrame([{\n",
    "            'File Name': os.path.basename(combined_asc_file_path),\n",
    "            'Max (inches)': max_value,\n",
    "            'Min (inches)': min_value,\n",
    "            'Mean (inches)': mean_value,\n",
    "            'Range (%)': range_percentage,\n",
    "            'polygon_name': polygon['name'].values[0],\n",
    "            'Results_name': f\"{polygon['name'].values[0]} {os.path.basename(combined_asc_file_path)}\"\n",
    "        }])\n",
    "        results_dataframe = pd.concat([results_dataframe, new_row], ignore_index=True)\n",
    "        #print(\"Results appended to the DataFrame\")\n",
    "\n",
    "        metadata = extract_metadata(combined_asc_file_path)\n",
    "\n",
    "        # Create and save the plot\n",
    "        fig, ax = plt.subplots(figsize=(10, 10))\n",
    "        import numpy as np\n",
    "        max_color_limit = np.ceil(max_value)  # Round up to the nearest whole number\n",
    "        min_color_limit = np.floor(min_value)  # Round down to the nearest whole number\n",
    "        im = clipped_raster.plot(ax=ax, cmap='viridis', label='Precipitation (inches)', vmin=min_color_limit, vmax=max_color_limit)\n",
    "        polygon.geometry.boundary.plot(ax=ax, color='black', linewidth=2, label='Watershed', zorder=10)\n",
    "        ax.set_xlim(expanded_bounds[0], expanded_bounds[2])\n",
    "        ax.set_ylim(expanded_bounds[1], expanded_bounds[3])\n",
    "        # Add statistics to the plot title\n",
    "        stats_text = f\"Max: {max_value:.2f}, Min: {min_value:.2f}, Mean: {mean_value:.2f}, Range: {range_percentage:.2f}%\"\n",
    " \n",
    "        plt.title(f\"{polygon_name} \\n Atlas 14 \\n Return Interval: {metadata['return_interval']} years, Duration: {metadata['duration']} {metadata['duration_units']}\\n{stats_text}\")\n",
    "        ax.set_xlabel(\"Longitude, Degrees\")\n",
    "        ax.set_ylabel(\"Latitude, Degrees\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        #plt.show()\n",
    "        \n",
    "        # Save the plot as PNG\n",
    "        plot_file_name = f\"{polygon_name} {os.path.splitext(os.path.basename(combined_asc_file_path))[0]}_plot.png\"\n",
    "        plot_file_path = os.path.join(output_directory, plot_file_name)\n",
    "        plt.savefig(plot_file_path)\n",
    "        plt.close()\n",
    "        print(f\"Plot saved: {plot_file_path}\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing merged raster: {str(e)}\")\n",
    "\n",
    "    #print(\"Results DataFrame:\")\n",
    "    #display(results_dataframe)\n",
    "\n",
    "    return results_dataframe\n",
    "\n",
    "\n",
    "# Example of walking over watershed_boundary_gdf and running the function for all polygons\n",
    "results_df = pd.DataFrame()  # Initialize an empty DataFrame\n",
    "for polygon_index in range(len(watershed_boundary_gdf)):\n",
    "    results_dataframe = calculate_grid_statistics_by_polygon(combined_asc_file_path, merged_raster, polygon_index, watershed_boundary_gdf)\n",
    "    results_df = pd.concat([results_df, results_dataframe], ignore_index=True)\n",
    "    print(\"results_dataframe:\")\n",
    "    #display(results_dataframe)\n",
    "\n",
    "print(\"Final Results DataFrame:\")\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process all ASC files and Create Maps and Statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6 Define function to process all coombined asc files in the base folder, calculate figures and statistics, and save to csv\n",
    "\n",
    "print(\"ASC file processing completed.\")\n",
    "\n",
    "print(\"Starting to process ASC files...\")\n",
    "\n",
    "# Get all subfolders in the base folder\n",
    "subfolders = [f.path for f in os.scandir(base_folder) if f.is_dir()]\n",
    "\n",
    "# use the first 2 letters in the base file name of asc_file_name_1 as state_code\n",
    "state_code = os.path.basename(asc_file_name_1)[:2]\n",
    "print(f\"state_code: {state_code}\")\n",
    "second_state_code = os.path.basename(asc_file_name_2)[:2]\n",
    "print(f\"second_state_code: {second_state_code}\")\n",
    "\n",
    "# Filter subfolders starting with state_code (this should only return one state's files, the other state will be added based on this)\n",
    "state_code_folders = [folder for folder in subfolders if os.path.basename(folder).startswith('se')]\n",
    "print(f\"state_code_folders: {state_code_folders}\")\n",
    "\n",
    "# Walk over folders and combine, calculate statistics, and save to csv\n",
    "\n",
    "results_df = None\n",
    "\n",
    "results_combined = pd.DataFrame()  # Initialize an empty DataFrame\n",
    "\n",
    "\n",
    "\n",
    "for folder in tqdm(state_code_folders, desc=\"Processing folders\"):\n",
    "    print(f\"\\n Processing folder: {folder}\")\n",
    "    base_folder_name = os.path.basename(folder)\n",
    "    print(f\"base_folder_name: {base_folder_name}\")\n",
    "    \n",
    "    parent_folder = os.path.dirname(os.path.dirname(asc_file_name_1))\n",
    "    print(f\"parent_folder: {parent_folder}\")\n",
    "\n",
    "    second_state_foldername = f\"{second_state_code}{base_folder_name[2:]}\"\n",
    "    print(f\"second_state_foldername: {second_state_foldername}\")\n",
    "\n",
    "    second_state_folder = os.path.join(parent_folder, second_state_foldername)\n",
    "    print(f\"second_state_folder: {second_state_folder}\")\n",
    "    \n",
    "    cb_folder = os.path.join(parent_folder, f\"cb{base_folder_name[2:]}\")\n",
    "    print(f\"cb_folder: {cb_folder}\")\n",
    "    \n",
    "    # Check if corresponding second_state_folder folder exists\n",
    "    if not os.path.exists(second_state_folder):\n",
    "        raise FileNotFoundError(f\"second_state_folder does not exist: {second_state_folder}\")\n",
    "    \n",
    "    print(f\"Checking for existence of second_state_folder: {second_state_folder}\")\n",
    "   \n",
    "    # Define file paths\n",
    "    asc_file_name_1 = os.path.join(folder, f\"{base_folder_name}.asc\")\n",
    "    print(f\"asc_file_name_1: {asc_file_name_1}\")\n",
    "    \n",
    "    asc_file_name_2 = os.path.join(second_state_folder, f\"{second_state_code}{base_folder_name[2:]}.asc\")\n",
    "    print(f\"asc_file_name_2: {asc_file_name_2}\")\n",
    "    \n",
    "    # Check if input files exist\n",
    "    if not os.path.exists(asc_file_name_1) or not os.path.exists(asc_file_name_2):\n",
    "        print(f\"Error: Input files not found. Skipping...\")\n",
    "        # Raise exception if either doesnt exist\n",
    "\n",
    "    else:\n",
    "        # Create 'cb' folder if it doesn't exist\n",
    "        os.makedirs(cb_folder, exist_ok=True)\n",
    "\n",
    "        # Further script logic uses folder names, so empty folders will make the script crash.  \n",
    "        try:\n",
    "            # Combine ASC datasets\n",
    "            print(\"Combining ASC files...\")\n",
    "            combined_asc_file_path, plot_file_name, merged_raster = combine_asc_files(asc_file_name_1, asc_file_name_2)        \n",
    "            print(f\"Combined ASC file created: {combined_asc_file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while combining ASC files: {e}\")\n",
    "            raise\n",
    "\n",
    "    # Load merged data before running statistics\n",
    "    merged_raster = rioxarray.open_rasterio(combined_asc_file_path)\n",
    "\n",
    "    # Initialize an empty list to accumulate results\n",
    "    results_accumulator = []\n",
    "\n",
    "\n",
    "    # Example of walking over watershed_boundary_gdf and running the function for all polygons\n",
    "    results_df = pd.DataFrame()  # Initialize an empty DataFrame\n",
    "    for polygon_index in range(len(watershed_boundary_gdf)):\n",
    "        results_dataframe = calculate_grid_statistics_by_polygon(combined_asc_file_path, merged_raster, polygon_index, watershed_boundary_gdf)\n",
    "        results_df = pd.concat([results_df, results_dataframe], ignore_index=True).drop_duplicates()\n",
    "        print(\"results_dataframe:\")\n",
    "        #display(results_dataframe)\n",
    "\n",
    "    print(\"Final Results DataFrame:\")\n",
    "    #display(results_df)\n",
    "\n",
    "    # Add to results_combined dataframe to hold results for all files and polygons\n",
    "    results_combined = pd.concat([results_combined, results_df], ignore_index=True)\n",
    "    print(\"results_combined:\")\n",
    "    print(f\"Total number of entries in results_combined: {len(results_combined)}, here are a few\")\n",
    "    display(results_combined.head())\n",
    "\n",
    "    # Close the merged_raster to free up resources\n",
    "    merged_raster.close()\n",
    "    print(\"Closed merged_raster\")\n",
    "\n",
    "# Save results file to CSV in Watershed_Statistical_Analysis folder (output_directory)\n",
    "csv_file_path = os.path.join(output_directory, 'merged_raster_statistics.csv')\n",
    "\n",
    "# Save the compiled results DataFrame to a CSV file\n",
    "results_combined.to_csv(csv_file_path, index=False)\n",
    "print(f\"Results saved to: {csv_file_path}\")\n",
    "\n",
    "display(results_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7 Split Results by Polygon to provide a CSV for each \n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "results_data_from_csv = results_combined\n",
    "\n",
    "\n",
    "def split_csv_by_polygon(csv_file_path):\n",
    "    # Load the dataset\n",
    "    data = pd.read_csv(csv_file_path)\n",
    "    print(\"Loaded data from CSV file.\")\n",
    "    \n",
    "    # Ensure that each unique 'File Name' has exactly 15 unique 'polygon_name' entries\n",
    "    file_name_groups = data.groupby('File Name')\n",
    "\n",
    "    # Dictionary to store dataframes for each polygon_name\n",
    "    polygon_dfs = {}\n",
    "\n",
    "    # Iterate over each group and split by 'polygon_name'\n",
    "    for file_name, group in file_name_groups:\n",
    "        for polygon_name, polygon_group in group.groupby('polygon_name'):\n",
    "            if polygon_name not in polygon_dfs:\n",
    "                polygon_dfs[polygon_name] = []\n",
    "            # Append each polygon_group to the corresponding list in the dictionary\n",
    "            polygon_dfs[polygon_name].append(polygon_group)\n",
    "\n",
    "    # Get the directory of the input file\n",
    "    output_directory = os.path.dirname(csv_file_path)\n",
    "\n",
    "    # create output directory if it doesnt exist\n",
    "    if not os.path.exists(\"output_csv_by_polygon\"):\n",
    "        os.makedirs(\"output_csv_by_polygon\")\n",
    "\n",
    "\n",
    "    # Save each polygon dataframe to a separate CSV file in the same directory as the input file\n",
    "    for polygon_name, polygon_group_list in polygon_dfs.items():\n",
    "        combined_polygon_group = pd.concat(polygon_group_list)\n",
    "        output_path = os.path.join(\"output_csv_by_polygon\", f\"{polygon_name.replace(' ', '_')}.csv\")\n",
    "        combined_polygon_group.to_csv(output_path, index=False)\n",
    "        print(f\"Saved CSV for polygon: {polygon_name}\")\n",
    "\n",
    "    print(f\"CSV files have been split and saved in: {output_directory}\")\n",
    "\n",
    "# Example usage\n",
    "# csv_file_path = r'path_to_your_csv_file.csv'\n",
    "split_csv_by_polygon(csv_file_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Atlas14_311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
