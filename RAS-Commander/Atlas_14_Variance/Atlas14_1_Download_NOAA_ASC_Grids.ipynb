{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Atlas 14 ASC Grid Download Script\n",
    "Script 1/3 for Atlas 14 Spatial Variance Analysis\n",
    "\n",
    "Author: William (Bill) Katzenmeyer, P.E., C.F.M. (C.H. Fenstermaker and Associates, LLC) \n",
    "\n",
    "Source: https://github.com/billk-FM/HEC-Commander-Tools\n",
    "\n",
    "#### NOAA Data Source:\n",
    "https://hdsc.nws.noaa.gov/pub/hdsc/data/tx/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 User Input: Base URL, State Datasets, and Output Directory\n",
    "HDSC_DATA_URL = \"https://hdsc.nws.noaa.gov/pub/hdsc/data/\"\n",
    "STATE_DIR_1 = \"hdsc_tx_data\"\n",
    "STATE_DIR_2 = \"hdsc_se_data\"\n",
    "# Open the base url in your browser to choose directory names\n",
    "\n",
    "# User-defined number of concurrent HTTP requests\n",
    "num_concurrent_requests = 4\n",
    "\n",
    "# Define destination directory for unzipped files\n",
    "dest_dir = \"LWI_Region4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 Automated import/installation of necessary libraries\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_and_import(package, import_name=None):\n",
    "    import_name = import_name or package\n",
    "    try:\n",
    "        __import__(import_name)\n",
    "    except ImportError:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "        __import__(import_name)\n",
    "\n",
    "# Install and import packages\n",
    "install_and_import(\"os\")\n",
    "install_and_import(\"requests\")\n",
    "install_and_import(\"beautifulsoup4\", \"bs4\")\n",
    "install_and_import(\"tqdm\")\n",
    "install_and_import(\"concurrent.futures\")\n",
    "install_and_import(\"re\")\n",
    "install_and_import(\"IPython.display\", \"IPython\")\n",
    "install_and_import(\"pandas\")\n",
    "install_and_import(\"numpy\")\n",
    "install_and_import(\"shutil\")\n",
    "install_and_import(\"zipfile\")\n",
    "\n",
    "\n",
    "# Import statements\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, unquote\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import re\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download all Atlas 14 Grid files from the NOAA website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 Download Atlas 14 Grids\n",
    "\n",
    "def download_atlas14_files(local_directory):\n",
    "    \"\"\"\n",
    "    Downloads Atlas 14 grid files from the NOAA website to a specified local directory.\n",
    "\n",
    "    Parameters:\n",
    "    local_directory (str): The local directory where the downloaded files will be stored.\n",
    "    \n",
    "    This function constructs the base URL for the specified state code, creates the local directory if it doesn't exist,\n",
    "    retrieves the HTML content from the NOAA website, processes the content to find files to download, and then downloads\n",
    "    each file concurrently.\n",
    "    \"\"\"\n",
    "    # Extract the state code from the local directory path\n",
    "    state_code = local_directory[5:7]  # split from STATE_DIR_1, the 2 characters in the 6th and 7th position\n",
    "    base_url = f\"{HDSC_DATA_URL}{state_code}/\"  # HDSC_DATA_URL + STATE_CODE + \"/\"\n",
    "\n",
    "    # Create the local directory if it doesn't exist\n",
    "    if not os.path.exists(local_directory):\n",
    "        os.makedirs(local_directory)\n",
    "        print(f\"Created directory: {local_directory}\")\n",
    "\n",
    "    def parse_file_sizes(html_content):\n",
    "        \"\"\"\n",
    "        Parses the HTML content to extract file names and their corresponding sizes.\n",
    "\n",
    "        Parameters:\n",
    "        html_content (str): The HTML content of the page to parse.\n",
    "\n",
    "        Returns:\n",
    "        dict: A dictionary mapping file names to their sizes in bytes.\n",
    "        \"\"\"\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "        file_sizes = {}\n",
    "        for row in soup.find_all('tr'):\n",
    "            cols = row.find_all('td')\n",
    "            if len(cols) >= 5:\n",
    "                file_name = cols[1].text.strip()\n",
    "                size_text = cols[3].text.strip()\n",
    "                if size_text.endswith('K') or size_text.endswith('M'):\n",
    "                    size = float(size_text[:-1])\n",
    "                    if size_text.endswith('K'):\n",
    "                        size *= 1024\n",
    "                    elif size_text.endswith('M'):\n",
    "                        size *= 1024 * 1024\n",
    "                    file_sizes[file_name] = int(size)\n",
    "        return file_sizes\n",
    "\n",
    "    def download_file(url, local_path, expected_size, retry_count=0):\n",
    "        \"\"\"\n",
    "        Downloads a file from a given URL to a specified local path, checking the file size.\n",
    "\n",
    "        Parameters:\n",
    "        url (str): The URL of the file to download.\n",
    "        local_path (str): The local path where the file will be saved.\n",
    "        expected_size (int): The expected size of the file in bytes.\n",
    "        retry_count (int): The current retry count for the download (default is 0).\n",
    "        \"\"\"\n",
    "        def check_file_size(path, expected):\n",
    "            \"\"\"\n",
    "            Checks if the file at the given path matches the expected size.\n",
    "\n",
    "            Parameters:\n",
    "            path (str): The path to the file.\n",
    "            expected (int): The expected size of the file in bytes.\n",
    "\n",
    "            Returns:\n",
    "            bool: True if the file size is within 5% of the expected size, False otherwise.\n",
    "            \"\"\"\n",
    "            if os.path.exists(path):\n",
    "                actual_size = os.path.getsize(path)\n",
    "                return abs(actual_size - expected) <= 0.05 * expected\n",
    "            return False\n",
    "\n",
    "        if check_file_size(local_path, expected_size):\n",
    "            print(f\"File already exists and is within 5% of expected size: {local_path}\")\n",
    "            return\n",
    "\n",
    "        if os.path.exists(local_path):\n",
    "            os.remove(local_path)\n",
    "            print(f\"Deleted existing file: {local_path}\")\n",
    "\n",
    "        print(f\"Attempting to download: {url} (Expected size: {expected_size / 1024:.2f} KB)\")\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            response = requests.get(url, stream=True)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            total_size = int(response.headers.get('content-length', 0))\n",
    "            block_size = 1024  # 1 Kibibyte\n",
    "\n",
    "            with open(local_path, 'wb') as file:\n",
    "                for data in response.iter_content(block_size):\n",
    "                    file.write(data)\n",
    "            \n",
    "            elapsed_time = time.time() - start_time\n",
    "            download_speed = (total_size / 1024) / elapsed_time  # KB/s\n",
    "\n",
    "            if check_file_size(local_path, expected_size):\n",
    "                print(f\"Successfully downloaded: {local_path} (Elapsed time: {elapsed_time:.2f} s, Speed: {download_speed:.2f} KB/s)\")\n",
    "            else:\n",
    "                if retry_count < 1:\n",
    "                    print(f\"Downloaded file size mismatch. Retrying: {local_path}\")\n",
    "                    os.remove(local_path)\n",
    "                    time.sleep(1)\n",
    "                    download_file(url, local_path, expected_size, retry_count + 1)\n",
    "                else:\n",
    "                    print(f\"Skipping file due to repeated size mismatch: {local_path}\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Failed to download {url}. Error: {e}\")\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "    def process_html_content(html_content):\n",
    "        \"\"\"\n",
    "        Processes the HTML content to extract links to files that need to be downloaded.\n",
    "\n",
    "        Parameters:\n",
    "        html_content (str): The HTML content of the page to process.\n",
    "\n",
    "        Returns:\n",
    "        list: A list of file links to download.\n",
    "        \"\"\"\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "        files_to_download = []\n",
    "\n",
    "        for link in soup.find_all(\"a\"):\n",
    "            href = link.get(\"href\")\n",
    "            if href and not href.startswith((\"../\", \"/\")) and '?' not in href:\n",
    "                if href.endswith(('.zip', '.pdf', '.txt', '.csv')) and not href.startswith(\"drive-download\"):\n",
    "                    files_to_download.append(href)\n",
    "\n",
    "        return files_to_download\n",
    "\n",
    "    # Download the HTML content\n",
    "    print(f\"Downloading HTML content from {base_url}\")\n",
    "    try:\n",
    "        response = requests.get(base_url)\n",
    "        response.raise_for_status()\n",
    "        html_content = response.text\n",
    "        print(\"Successfully downloaded HTML content\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Failed to download HTML content. Error: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "    print(f\"Starting to process HTML content from {base_url}\")\n",
    "    files_to_download = process_html_content(html_content)\n",
    "    file_sizes = parse_file_sizes(html_content)\n",
    "\n",
    "    print(f\"Found {len(files_to_download)} files to download\")\n",
    "\n",
    "    def download_with_pause(file):\n",
    "        \"\"\"\n",
    "        Downloads a file with a pause, constructing the URL and local path.\n",
    "\n",
    "        Parameters:\n",
    "        file (str): The name of the file to download.\n",
    "        \"\"\"\n",
    "        file_url = urljoin(base_url, file)\n",
    "        local_path = os.path.join(local_directory, unquote(file))\n",
    "        expected_size = file_sizes.get(file, 0)\n",
    "        download_file(file_url, local_path, expected_size)\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=num_concurrent_requests) as executor:\n",
    "        executor.map(download_with_pause, files_to_download)\n",
    "\n",
    "    print(\"All downloads completed.\")\n",
    "\n",
    "# Download Atlas 14 Data for both specified directories\n",
    "download_atlas14_files(STATE_DIR_1)\n",
    "download_atlas14_files(STATE_DIR_2)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy Subset of Files to Separate Directory (LWI_Region4) and unzip for postprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4 Unzip files \n",
    "def unzip_files_to_directory(local_dir):\n",
    "    # Define the source and destination directories\n",
    "    source_dir = local_dir\n",
    "    \n",
    "\n",
    "    # Create the destination directory if it doesn't exist\n",
    "    if not os.path.exists(dest_dir):\n",
    "        os.makedirs(dest_dir)\n",
    "\n",
    "    # Unzip all zip files directly into their own subfolder in the destination directory\n",
    "    for file in os.listdir(source_dir):\n",
    "        if file.endswith(\".zip\") and \"_\" not in file and not file.endswith(\"l.zip\") and not file.endswith(\"u.zip\"):\n",
    "            zip_file_path = os.path.join(source_dir, file)\n",
    "            if zipfile.is_zipfile(zip_file_path):\n",
    "                with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "                    extract_dir = os.path.join(dest_dir, os.path.splitext(file)[0])\n",
    "                    if not os.path.exists(extract_dir):\n",
    "                        os.makedirs(extract_dir)\n",
    "                    zip_ref.extractall(extract_dir)\n",
    "                    print(f\"Extracted: {file} to {extract_dir}\")\n",
    "            else:\n",
    "                print(f\"Not a zip file: {file}\")\n",
    "\n",
    "    print(\"Unzipping completed.\")\n",
    "\n",
    "unzip_files_to_directory(STATE_DIR_1)\n",
    "unzip_files_to_directory(STATE_DIR_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, run script 2 which will combine asc grids and generate regional plots and statistics. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webscrape",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
