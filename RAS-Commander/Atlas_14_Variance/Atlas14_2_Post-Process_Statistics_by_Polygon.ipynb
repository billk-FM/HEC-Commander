{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Atlas 14 ASC Grid Postprocessing Script\n",
    "Script 2/3 for Atlas 14 Spatial Variance Analysis\n",
    "\n",
    "Author: William (Bill) Katzenmeyer, P.E., C.F.M. (C.H. Fenstermaker and Associates, LLC) \n",
    "\n",
    "Source: https://github.com/billk-FM/HEC-Commander-Tools\n",
    "\n",
    "#### NOAA Data Source:\n",
    "https://hdsc.nws.noaa.gov/pub/hdsc/data/tx/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 Define file paths to watershed polygon, state polygon, and example asc files to be used as general figures\n",
    "watershed_boundary_file = r'Region4_HUC_Boundaries.geojson'\n",
    "state_boundary_file = r'State_Boundary.geojson'\n",
    "asc_file_name_1 = r'LWI_Region4/se50yr06ha/se50yr06ha.asc'\n",
    "asc_file_name_2 = r'LWI_Region4/tx50yr06ha/tx50yr06ha.asc'\n",
    "\n",
    "# Default CRS assumption for asc files \n",
    "asc_file_default_EPSG = \"4269\"\n",
    "\n",
    "# Target CRS for all script operations and outputs\n",
    "reproject_to_epsg = \"4269\"\n",
    "\n",
    "# Input Directory with combined ASC File Datasets (this should come from a previous step on revision)\n",
    "input_directory = r'LWI_Region4'\n",
    "\n",
    "# Set the base folder path\n",
    "base_folder = r'LWI_Region4'\n",
    "\n",
    "# Output Directory for PNG and CSV Outputs\n",
    "import os\n",
    "output_directory = os.path.join(input_directory, 'Watershed_Statistical_Analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 Automatically Import and Install Libraries\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_and_import(package_name, import_name=None):\n",
    "    if import_name is None:\n",
    "        import_name = package_name\n",
    "    try:\n",
    "        __import__(import_name)\n",
    "    except ImportError:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package_name])\n",
    "        globals()[import_name] = __import__(import_name)\n",
    "\n",
    "# Installation and import statements\n",
    "install_and_import(\"os\")\n",
    "install_and_import(\"numpy\")\n",
    "install_and_import(\"rioxarray\")\n",
    "install_and_import(\"matplotlib.pyplot\", \"matplotlib\")\n",
    "install_and_import(\"geopandas\", \"geopandas\")\n",
    "install_and_import(\"pyproj\")\n",
    "install_and_import(\"json\")\n",
    "install_and_import(\"shapely.geometry\", \"shapely\")\n",
    "install_and_import(\"xarray\")\n",
    "install_and_import(\"affine\")\n",
    "install_and_import(\"rasterio\")\n",
    "install_and_import(\"tqdm\")\n",
    "install_and_import(\"shutil\")\n",
    "install_and_import(\"pandas\")\n",
    "install_and_import(\"pathlib\")\n",
    "install_and_import(\"IPython.display\", \"IPython\")\n",
    "\n",
    "# Import statements\n",
    "import os\n",
    "import numpy as np\n",
    "import rioxarray\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "from pyproj import CRS\n",
    "import json\n",
    "from shapely.geometry import shape\n",
    "import xarray as xr\n",
    "from affine import Affine\n",
    "from rasterio.enums import Resampling\n",
    "from tqdm import tqdm\n",
    "import rasterio\n",
    "import shutil\n",
    "from rasterio.transform import from_origin\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from IPython.display import display\n",
    "import geopandas as gpd\n",
    "import rioxarray\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import mapping, box\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rasterio.transform import from_origin\n",
    "from rasterio.warp import Resampling\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 Extract metadata from file name and add to dataframe asc_file_names with columns \"filename\", \"return_interval\", \"duration\", \"duration_units\", \"duration_hours\"\n",
    "\n",
    "def extract_metadata(filename):\n",
    "    \"\"\"\n",
    "    Extract metadata from the given filename.\n",
    "\n",
    "    This function extracts the return interval and duration from the filename,\n",
    "    which is expected to follow a specific naming convention. The return interval\n",
    "    is the number preceding 'yr', and the duration is the two digits preceding\n",
    "    'ha', 'da', or 'ma', which represent hours, days, or months respectively.\n",
    "\n",
    "    Parameters:\n",
    "    filename (str): The full path of the file from which to extract metadata.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing the extracted metadata including:\n",
    "        - filename: The original filename\n",
    "        - return_interval: The extracted return interval as an integer\n",
    "        - duration: The extracted duration as an integer\n",
    "        - duration_units: The units of duration ('ha', 'da', or 'ma')\n",
    "        - duration_hours: The duration converted to hours as an integer\n",
    "\n",
    "    Raises:\n",
    "    ValueError: If the return interval or duration cannot be extracted from the filename.\n",
    "    \"\"\"\n",
    "    print(\"/n-----   Extracting Metadata   -----\")\n",
    "\n",
    "    base_filename = os.path.basename(filename)\n",
    "    \n",
    "    # Extract return interval (numbers before \"yr\")\n",
    "    return_interval_match = re.search(r'(\\d+)yr', base_filename)\n",
    "    if return_interval_match:\n",
    "        return_interval = int(return_interval_match.group(1))\n",
    "    else:\n",
    "        raise ValueError(f\"Unable to extract return interval from filename: {base_filename}\")\n",
    "    \n",
    "    # Extract duration (2 numbers before \"ha\", \"da\", or \"ma\", which denote Hours, Days, or Months)\n",
    "    duration_match = re.search(r'(\\d{2})(ha|da|ma)', base_filename)\n",
    "    if duration_match:\n",
    "        duration = int(duration_match.group(1))\n",
    "        duration_units = duration_match.group(2)\n",
    "        if duration_units == \"ha\":\n",
    "            duration_hours = duration\n",
    "        elif duration_units == \"da\":\n",
    "            duration_hours = duration * 24\n",
    "        elif duration_units == \"ma\":\n",
    "            duration_hours = duration * 24 * 30  # Convert Months to Hours\n",
    "    else:\n",
    "        raise ValueError(f\"Unable to extract duration from filename: {base_filename}\")\n",
    "    \n",
    "    return {\n",
    "        \"filename\": filename,\n",
    "        \"return_interval\": return_interval,\n",
    "        \"duration\": duration,\n",
    "        \"duration_units\": duration_units,\n",
    "        \"duration_hours\": duration_hours\n",
    "    }\n",
    "\n",
    "# Create a list of ASC filenames\n",
    "asc_filenames = [asc_file_name_1, asc_file_name_2]\n",
    "\n",
    "# Initialize the DataFrame if it doesn't exist\n",
    "if 'asc_file_names' not in globals():\n",
    "    asc_file_names = pd.DataFrame(columns=[\"filename\", \"return_interval\", \"duration\", \"duration_units\", \"duration_hours\"])\n",
    "\n",
    "# Extract metadata for each file\n",
    "metadata = []\n",
    "for filename in asc_filenames:\n",
    "    try:\n",
    "        # Check if the filename already exists in the DataFrame\n",
    "        if filename in asc_file_names['filename'].values:\n",
    "            print(f\"Skipping {filename} as it already exists in the DataFrame.\")\n",
    "            continue\n",
    "        metadata.append(extract_metadata(filename))\n",
    "    except ValueError as e:\n",
    "        print(f\"Error processing {filename}: {str(e)}\")\n",
    "\n",
    "# Append new data to the DataFrame\n",
    "if metadata:\n",
    "    new_data = pd.DataFrame(metadata)\n",
    "    asc_file_names = pd.concat([asc_file_names, new_data], ignore_index=True)\n",
    "\n",
    "print(\"asc_file_names:\")\n",
    "display(asc_file_names)\n",
    "\n",
    "# Load the GeoJSON files\n",
    "watershed_boundary_gdf = gpd.read_file(watershed_boundary_file)\n",
    "state_boundary_gdf = gpd.read_file(state_boundary_file)\n",
    "\n",
    "# Print the GeoDataFrame names and display them\n",
    "print(\"watershed_boundary_gdf\")\n",
    "display(watershed_boundary_gdf)\n",
    "\n",
    "print(\"state_boundary_gdf\")\n",
    "display(state_boundary_gdf)\n",
    "\n",
    "# Reproject GeoDataFrames to the target CRS\n",
    "target_crs = f\"EPSG:{reproject_to_epsg}\"\n",
    "watershed_boundary_gdf = watershed_boundary_gdf.to_crs(target_crs)\n",
    "state_boundary_gdf = state_boundary_gdf.to_crs(target_crs)\n",
    "\n",
    "# Print CRS information to verify\n",
    "print(\"state_gdf CRS:\", state_boundary_gdf.crs)\n",
    "print(\"watershed_gdf CRS:\", watershed_boundary_gdf.crs)\n",
    "\n",
    "# Merge the boundaries into one GeoDataFrame\n",
    "all_polygons = gpd.GeoDataFrame(pd.concat([state_boundary_gdf, watershed_boundary_gdf], ignore_index=True))\n",
    "\n",
    "# Print CRS information of the combined GeoDataFrame\n",
    "print(\"all_polygons CRS:\", all_polygons.crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and test functions\n",
    "Uncomment plt.show() lines if you want to see plots as they are generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 Define Function to Combine ASC Files\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import rioxarray\n",
    "from rasterio.transform import from_origin\n",
    "from rasterio.enums import Resampling\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def combine_asc_files(asc_file_name_1, asc_file_name_2):\n",
    "    \"\"\"\n",
    "    Combines two ASC files into a single raster file, reprojecting and aligning them as necessary.\n",
    "    \n",
    "    Parameters:\n",
    "    asc_file_name_1 (str): The file path of the first ASC file.\n",
    "    asc_file_name_2 (str): The file path of the second ASC file.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: A tuple containing the file path of the combined ASC file, the plot file name, and the merged raster.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"/n-----   Combining multistate asc files   -----\")\n",
    "    \n",
    "    # Generate .prj file names for the ASC files\n",
    "    asc_prj_name_1 = asc_file_name_1.replace('.asc', '.prj')\n",
    "    asc_prj_name_2 = asc_file_name_2.replace('.asc', '.prj')\n",
    "\n",
    "    print(f\"ASC file 1 PRJ file: {asc_prj_name_1}\")\n",
    "    print(f\"ASC file 2 PRJ file: {asc_prj_name_2}\")\n",
    "\n",
    "    # Load the .asc files with rioxarray\n",
    "    raster_1 = rioxarray.open_rasterio(asc_file_name_1)\n",
    "    raster_2 = rioxarray.open_rasterio(asc_file_name_2)\n",
    "\n",
    "    # Read CRS from prj and set it for both rasters\n",
    "    with open(asc_prj_name_1, 'r') as prj_file:\n",
    "        crs = prj_file.read()\n",
    "    raster_1.rio.set_crs(crs, inplace=True)\n",
    "    raster_2.rio.set_crs(crs, inplace=True)\n",
    "\n",
    "    # Calculate the extent of the watersheds GeoDataFrame\n",
    "    watersheds_bounds = all_polygons.total_bounds\n",
    "    left, bottom, right, top = watersheds_bounds\n",
    "\n",
    "    # Expand the extent by 10%\n",
    "    width = right - left\n",
    "    height = top - bottom\n",
    "    left -= width * 0.1\n",
    "    right += width * 0.1\n",
    "    bottom -= height * 0.1\n",
    "    top += height * 0.1\n",
    "\n",
    "    # Print combined extent values\n",
    "    print(f\"Combined extent (watersheds +10%): left={left}, bottom={bottom}, right={right}, top={top}\")\n",
    "\n",
    "    # Calculate new dimensions based on raster_1's resolution\n",
    "    resolution_x, resolution_y = raster_1.rio.resolution()\n",
    "    width = int((right - left) / abs(resolution_x))\n",
    "    height = int((top - bottom) / abs(resolution_y))\n",
    "\n",
    "    # Check if dimensions are valid\n",
    "    if width <= 0 or height <= 0:\n",
    "        raise ValueError(\"Calculated dimensions are not valid. Width and height must be positive.\")\n",
    "\n",
    "    # Create new transform for the combined extent\n",
    "    new_transform = from_origin(left, top, abs(resolution_x), abs(resolution_y))\n",
    "\n",
    "    # Reproject raster_1 to the new dimensions and combined extent\n",
    "    raster_1_extended = raster_1.rio.reproject(\n",
    "        raster_1.rio.crs,\n",
    "        transform=new_transform,\n",
    "        shape=(height, width),\n",
    "        resampling=Resampling.nearest\n",
    "    )\n",
    "    print(\"Reprojected raster_1_extended\")\n",
    "\n",
    "    # Align raster_2 to the coordinates of the extended raster_1\n",
    "    raster_2_aligned = raster_2.rio.reproject_match(raster_1_extended)\n",
    "    print(\"Aligned raster_2 to raster_1_extended\")\n",
    "\n",
    "    # Merge the two rasters\n",
    "    merged_raster = raster_1_extended.where(raster_1_extended != -9, raster_2_aligned)\n",
    "\n",
    "    # Print the actual extents of merged_raster\n",
    "    merged_raster_extents = merged_raster.rio.bounds()\n",
    "    print(\"merged_raster extent, as calculated after interpolation:\", merged_raster_extents)\n",
    "\n",
    "    # Generate combined ASC folder name\n",
    "    asc_base_file_name = 'cb' + os.path.basename(asc_file_name_1)[2:]\n",
    "    asc_base_without_ext = os.path.splitext(asc_base_file_name)[0]  # remove the .asc from the file name\n",
    "    \n",
    "    combined_asc_folder_name = os.path.join(base_folder, asc_base_without_ext)\n",
    "    combined_asc_file_path = os.path.join(base_folder, asc_base_without_ext, asc_base_file_name)\n",
    "\n",
    "    # Step 1: Extract file paths and names\n",
    "    xml_filename = asc_file_name_1.replace(r'.asc', r'.xml')\n",
    "    prj_filename = asc_file_name_1.replace(r'.asc', r'.prj')\n",
    "\n",
    "    # Step 2: Split the path to get the directory and file name\n",
    "    xml_path, xml_file = os.path.split(xml_filename)\n",
    "    prj_path, prj_file = os.path.split(prj_filename)\n",
    "\n",
    "    # Step 3: Split the path to get all folder names\n",
    "    xml_path_parts = xml_path.split(os.sep)\n",
    "    prj_path_parts = prj_path.split(os.sep)\n",
    "\n",
    "    # Step 4: Replace the first two letters of the second folder level with \"cb\"\n",
    "    if len(xml_path_parts) > 1:\n",
    "        xml_path_parts[1] = 'cb' + xml_path_parts[1][2:]\n",
    "    if len(prj_path_parts) > 1:\n",
    "        prj_path_parts[1] = 'cb' + prj_path_parts[1][2:]\n",
    "\n",
    "    # Step 5: Reconstruct the new paths\n",
    "    new_xml_path = os.sep.join(xml_path_parts)\n",
    "    new_prj_path = os.sep.join(prj_path_parts)\n",
    "\n",
    "    # Step 6: Replace the first two letters of the file name with \"cb\"\n",
    "    new_xml_file = 'cb' + xml_file[2:]\n",
    "    new_prj_file = 'cb' + prj_file[2:]\n",
    "\n",
    "    # Step 7: Combine the modified paths and file names\n",
    "    xml_cb_sub = os.path.join(new_xml_path, new_xml_file).replace(os.sep, '/')\n",
    "    prj_cb_sub = os.path.join(new_prj_path, new_prj_file).replace(os.sep, '/')\n",
    "\n",
    "    xml_renamed = xml_cb_sub\n",
    "    prj_renamed = prj_cb_sub\n",
    "\n",
    "    # Convert merged_raster values from 1000ths of an inch to inches\n",
    "    merged_raster = merged_raster / 1000\n",
    "\n",
    "    # Plot the merged raster and the polygon boundaries\n",
    "    metadata = extract_metadata(asc_file_name_1)\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    cbar = merged_raster.plot(ax=ax, cmap='viridis', vmin=0, vmax=merged_raster.max())\n",
    "    cbar.colorbar.set_label(\"Total Precipitation (Inches)\")\n",
    "    all_polygons.boundary.plot(ax=ax, edgecolor='red')\n",
    "    plt.xlim(left, right)\n",
    "    plt.ylim(bottom, top)\n",
    "\n",
    "    # Set the title with duration and return interval information\n",
    "    plt.title(f\"Atlas 14 \\n Return Interval: {metadata['return_interval']} years, Duration: {metadata['duration']} {metadata['duration_units']}\")\n",
    "\n",
    "    # Label the axes\n",
    "    ax.set_xlabel(\"Longitude, Degrees\")\n",
    "    ax.set_ylabel(\"Latitude, Degrees\")\n",
    "\n",
    "    # Create the combined folder if it doesn't exist\n",
    "    os.makedirs(combined_asc_folder_name, exist_ok=True)\n",
    "\n",
    "    # Copy xml_filename and prj_filename to their renamed paths\n",
    "    shutil.copy(xml_filename, xml_renamed)\n",
    "    print(f\"Copied {xml_filename} to {xml_renamed}\")\n",
    "    \n",
    "    shutil.copy(prj_filename, prj_renamed)\n",
    "    print(f\"Copied {prj_filename} to {prj_renamed}\")\n",
    "\n",
    "    # Save the merged raster as a new ASC file\n",
    "    merged_raster.rio.to_raster(combined_asc_file_path)\n",
    "    print(f\"Combined ASC file saved: {combined_asc_file_path}\")\n",
    "\n",
    "    # Close the raster datasets\n",
    "    raster_1.close()\n",
    "    raster_2.close()\n",
    "    merged_raster.close()\n",
    "\n",
    "    # Generate plot file name\n",
    "    plot_file_name = os.path.join(combined_asc_folder_name, f\"cb{os.path.basename(asc_file_name_1)[2:-4]}_plot.png\")\n",
    "\n",
    "    input_directory = os.path.dirname(os.path.dirname(asc_file_name_1))\n",
    "    output_directory = os.path.join(input_directory, 'Watershed_Statistical_Analysis')\n",
    "\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory, exist_ok=True)\n",
    "        print(f\"Output directory created: {output_directory}\")\n",
    "    else:\n",
    "        print(f\"Output directory already exists: {output_directory}\")\n",
    "\n",
    "    # Save the plot in the output directory\n",
    "    plot_file_name = os.path.join(output_directory, f\"AA - Regional Report Figure cb{os.path.basename(asc_file_name_1)[2:-4]}_plot.png\")\n",
    "    plt.savefig(plot_file_name)\n",
    "    print(f\"Plot saved: {plot_file_name}\")\n",
    "\n",
    "    # Close the plot to free up memory\n",
    "    plt.close()\n",
    "\n",
    "    return combined_asc_file_path, plot_file_name, merged_raster\n",
    "\n",
    "# Generate combined folder and file names\n",
    "combined_asc_file_path, plot_file_name, merged_raster = combine_asc_files(asc_file_name_1, asc_file_name_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Function to Produce Report Plot and Statistics for each polygon in watershed_boundary_file\n",
    "# \n",
    "# This function calculates statistics for a given polygon from a watershed boundary GeoDataFrame\n",
    "# and generates a plot of the clipped raster data. It computes the maximum, minimum, mean, \n",
    "# and range of raster values within the polygon and appends the results to a DataFrame.\n",
    "#\n",
    "# INPUTS:\n",
    "# - combined_asc_file_path: Path to the combined ASC file.\n",
    "# - merged_raster: The merged raster data to be analyzed.\n",
    "# - polygon_index: Index of the polygon in the watershed_boundary_gdf to analyze.\n",
    "# - watershed_boundary_gdf: GeoDataFrame containing the watershed polygons.\n",
    "#\n",
    "# OUTPUT:\n",
    "# - results_dataframe: DataFrame containing the calculated statistics for the polygon.\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import mapping\n",
    "import pandas as pd\n",
    "\n",
    "def calculate_grid_statistics_by_polygon(combined_asc_file_path, merged_raster, polygon_index, watershed_boundary_gdf):\n",
    "    \"\"\"Calculate statistics for a specific polygon and generate a plot.\n",
    "    \n",
    "    Args:\n",
    "        combined_asc_file_path (str): Path to the combined ASC file.\n",
    "        merged_raster (xarray.DataArray): The merged raster data to be analyzed.\n",
    "        polygon_index (int): Index of the polygon in the watershed_boundary_gdf to analyze.\n",
    "        watershed_boundary_gdf (GeoDataFrame): GeoDataFrame containing the watershed polygons.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: A DataFrame containing the calculated statistics for the polygon.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize results DataFrame\n",
    "    results_dataframe = pd.DataFrame(columns=['File Name', 'Max (inches)', 'Min (inches)', 'Mean (inches)', 'Range (%)', 'polygon_name'])\n",
    "    print(\"/n-----   Calculating Statistics for Each Watershed Polygon   -----\")\n",
    "\n",
    "    try:\n",
    "        # Use the specified polygon in watershed_boundary_gdf\n",
    "        polygon = watershed_boundary_gdf.iloc[[polygon_index]]\n",
    "        polygon_name = polygon['name'].values[0]\n",
    "        polygon = polygon.set_crs(watershed_boundary_gdf.crs)\n",
    "        print(f\"Polygon {polygon_index} selected from watershed_boundary_gdf\")\n",
    "\n",
    "        # Calculate bounds and expand them by 10%\n",
    "        polygon_bounds = polygon.geometry.bounds\n",
    "        x_min, y_min, x_max, y_max = polygon_bounds.iloc[0]\n",
    "        x_buffer = (x_max - x_min) * 0.1\n",
    "        y_buffer = (y_max - y_min) * 0.1\n",
    "        expanded_bounds = (x_min - x_buffer, y_min - y_buffer, x_max + x_buffer, y_max + y_buffer)\n",
    "\n",
    "        # Convert the polygon's geometry to a format suitable for clipping\n",
    "        polygon_geom = [mapping(polygon.geometry.iloc[0])]\n",
    "\n",
    "        # Clip the merged raster with the polygon's geometry\n",
    "        clipped_raster = merged_raster.rio.clip(polygon_geom, watershed_boundary_gdf.crs)\n",
    "\n",
    "        # Remove no-data values (assuming -9 is the no-data value)\n",
    "        clipped_raster = clipped_raster.where(clipped_raster != -9)\n",
    "\n",
    "        # Calculate statistics\n",
    "        max_value = clipped_raster.max().values.item()\n",
    "        min_value = clipped_raster.min().values.item()\n",
    "        mean_value = clipped_raster.mean().values.item()\n",
    "        range_percentage = ((max_value - min_value) / max_value) * 100\n",
    "        print(f\"Statistics calculated: Max={max_value}, Min={min_value}, Mean={mean_value}, Range%={range_percentage}\")\n",
    "\n",
    "        # Append results to the DataFrame\n",
    "        new_row = pd.DataFrame([{\n",
    "            'File Name': os.path.basename(combined_asc_file_path),\n",
    "            'Max (inches)': max_value,\n",
    "            'Min (inches)': min_value,\n",
    "            'Mean (inches)': mean_value,\n",
    "            'Range (%)': range_percentage,\n",
    "            'polygon_name': polygon['name'].values[0],\n",
    "            'Results_name': f\"{polygon['name'].values[0]} {os.path.basename(combined_asc_file_path)}\"\n",
    "        }])\n",
    "        results_dataframe = pd.concat([results_dataframe, new_row], ignore_index=True)\n",
    "\n",
    "        # Extract metadata for the plot title\n",
    "        metadata = extract_metadata(combined_asc_file_path)\n",
    "\n",
    "        # Create and save the plot\n",
    "        fig, ax = plt.subplots(figsize=(10, 10))\n",
    "        import numpy as np\n",
    "        max_color_limit = np.ceil(max_value)  # Round up to the nearest whole number\n",
    "        min_color_limit = np.floor(min_value)  # Round down to the nearest whole number\n",
    "        im = clipped_raster.plot(ax=ax, cmap='viridis', label='Precipitation (inches)', vmin=min_color_limit, vmax=max_color_limit)\n",
    "        polygon.geometry.boundary.plot(ax=ax, color='black', linewidth=2, label='Watershed', zorder=10)\n",
    "        ax.set_xlim(expanded_bounds[0], expanded_bounds[2])\n",
    "        ax.set_ylim(expanded_bounds[1], expanded_bounds[3])\n",
    "        \n",
    "        # Add statistics to the plot title\n",
    "        stats_text = f\"Max: {max_value:.2f}, Min: {min_value:.2f}, Mean: {mean_value:.2f}, Range: {range_percentage:.2f}%\"\n",
    "        plt.title(f\"{polygon_name} \\n Atlas 14 \\n Return Interval: {metadata['return_interval']} years, Duration: {metadata['duration']} {metadata['duration_units']}\\n{stats_text}\")\n",
    "        ax.set_xlabel(\"Longitude, Degrees\")\n",
    "        ax.set_ylabel(\"Latitude, Degrees\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the plot as PNG\n",
    "        plot_file_name = f\"{polygon_name} {os.path.splitext(os.path.basename(combined_asc_file_path))[0]}_plot.png\"\n",
    "        plot_file_path = os.path.join(output_directory, plot_file_name)\n",
    "        plt.savefig(plot_file_path)\n",
    "        plt.close()\n",
    "        print(f\"Plot saved: {plot_file_path}\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing merged raster: {str(e)}\")\n",
    "\n",
    "    return results_dataframe\n",
    "\n",
    "# Example of walking over watershed_boundary_gdf and running the function for all polygons\n",
    "results_df = pd.DataFrame()  # Initialize an empty DataFrame\n",
    "for polygon_index in range(len(watershed_boundary_gdf)):\n",
    "    results_dataframe = calculate_grid_statistics_by_polygon(combined_asc_file_path, merged_raster, polygon_index, watershed_boundary_gdf)\n",
    "    results_df = pd.concat([results_df, results_dataframe], ignore_index=True)\n",
    "\n",
    "print(\"Final Results DataFrame:\")\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process all ASC files and Create Maps and Statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6 Define function to process all combined asc files in the base folder, calculate figures and statistics, and save to csv\n",
    "\n",
    "print(\"ASC file processing completed.\")\n",
    "\n",
    "print(\"Starting to process ASC files...\")\n",
    "\n",
    "# Get all subfolders in the base folder\n",
    "subfolders = [f.path for f in os.scandir(base_folder) if f.is_dir()]\n",
    "\n",
    "# Use the first 2 letters in the base file name of asc_file_name_1 as state_code\n",
    "state_code = os.path.basename(asc_file_name_1)[:2]\n",
    "print(f\"state_code: {state_code}\")\n",
    "second_state_code = os.path.basename(asc_file_name_2)[:2]\n",
    "print(f\"second_state_code: {second_state_code}\")\n",
    "\n",
    "# Filter subfolders starting with state_code (this should only return one state's files, the other state will be added based on this)\n",
    "state_code_folders = [folder for folder in subfolders if os.path.basename(folder).startswith('se')]\n",
    "print(f\"state_code_folders: {state_code_folders}\")\n",
    "\n",
    "# Walk over folders and combine, calculate statistics, and save to csv\n",
    "\n",
    "results_df = None\n",
    "\n",
    "results_combined = pd.DataFrame()  # Initialize an empty DataFrame\n",
    "\n",
    "# Iterate over each folder that matches the state code\n",
    "for folder in tqdm(state_code_folders, desc=\"Processing folders\"):\n",
    "    print(f\"\\n Processing folder: {folder}\")\n",
    "    base_folder_name = os.path.basename(folder)\n",
    "    print(f\"base_folder_name: {base_folder_name}\")\n",
    "    \n",
    "    parent_folder = os.path.dirname(os.path.dirname(asc_file_name_1))\n",
    "    print(f\"parent_folder: {parent_folder}\")\n",
    "\n",
    "    second_state_foldername = f\"{second_state_code}{base_folder_name[2:]}\"\n",
    "    print(f\"second_state_foldername: {second_state_foldername}\")\n",
    "\n",
    "    second_state_folder = os.path.join(parent_folder, second_state_foldername)\n",
    "    print(f\"second_state_folder: {second_state_folder}\")\n",
    "    \n",
    "    cb_folder = os.path.join(parent_folder, f\"cb{base_folder_name[2:]}\")\n",
    "    print(f\"cb_folder: {cb_folder}\")\n",
    "    \n",
    "    # Check if corresponding second_state_folder folder exists\n",
    "    if not os.path.exists(second_state_folder):\n",
    "        raise FileNotFoundError(f\"second_state_folder does not exist: {second_state_folder}\")\n",
    "    \n",
    "    print(f\"Checking for existence of second_state_folder: {second_state_folder}\")\n",
    "   \n",
    "    # Define file paths\n",
    "    asc_file_name_1 = os.path.join(folder, f\"{base_folder_name}.asc\")\n",
    "    print(f\"asc_file_name_1: {asc_file_name_1}\")\n",
    "    \n",
    "    asc_file_name_2 = os.path.join(second_state_folder, f\"{second_state_code}{base_folder_name[2:]}.asc\")\n",
    "    print(f\"asc_file_name_2: {asc_file_name_2}\")\n",
    "    \n",
    "    # Check if input files exist\n",
    "    if not os.path.exists(asc_file_name_1) or not os.path.exists(asc_file_name_2):\n",
    "        print(f\"Error: Input files not found. Skipping...\")\n",
    "        # Raise exception if either doesn't exist\n",
    "\n",
    "    else:\n",
    "        # Create 'cb' folder if it doesn't exist\n",
    "        os.makedirs(cb_folder, exist_ok=True)\n",
    "\n",
    "        # Further script logic uses folder names, so empty folders will make the script crash.  \n",
    "        try:\n",
    "            # Combine ASC datasets\n",
    "            print(\"Combining ASC files...\")\n",
    "            combined_asc_file_path, plot_file_name, merged_raster = combine_asc_files(asc_file_name_1, asc_file_name_2)        \n",
    "            print(f\"Combined ASC file created: {combined_asc_file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while combining ASC files: {e}\")\n",
    "            raise\n",
    "\n",
    "    # Load merged data before running statistics\n",
    "    merged_raster = rioxarray.open_rasterio(combined_asc_file_path)\n",
    "\n",
    "    # Initialize an empty list to accumulate results\n",
    "    results_accumulator = []\n",
    "\n",
    "    # Example of walking over watershed_boundary_gdf and running the function for all polygons\n",
    "    results_df = pd.DataFrame()  # Initialize an empty DataFrame\n",
    "    for polygon_index in range(len(watershed_boundary_gdf)):\n",
    "        results_dataframe = calculate_grid_statistics_by_polygon(combined_asc_file_path, merged_raster, polygon_index, watershed_boundary_gdf)\n",
    "        results_df = pd.concat([results_df, results_dataframe], ignore_index=True).drop_duplicates()\n",
    "        print(\"results_dataframe:\")\n",
    "        #display(results_dataframe)\n",
    "\n",
    "    print(\"Final Results DataFrame:\")\n",
    "    #display(results_df)\n",
    "\n",
    "    # Add to results_combined dataframe to hold results for all files and polygons\n",
    "    results_combined = pd.concat([results_combined, results_df], ignore_index=True)\n",
    "    print(\"results_combined:\")\n",
    "    print(f\"Total number of entries in results_combined: {len(results_combined)}, here are a few\")\n",
    "    display(results_combined.head())\n",
    "\n",
    "    # Close the merged_raster to free up resources\n",
    "    merged_raster.close()\n",
    "    print(\"Closed merged_raster\")\n",
    "\n",
    "# Save results file to CSV in Watershed_Statistical_Analysis folder (output_directory)\n",
    "csv_file_path = os.path.join(output_directory, 'merged_raster_statistics.csv')\n",
    "\n",
    "# Save the compiled results DataFrame to a CSV file\n",
    "results_combined.to_csv(csv_file_path, index=False)\n",
    "print(f\"Results saved to: {csv_file_path}\")\n",
    "\n",
    "display(results_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7 Split Results by Polygon to provide a CSV for each \n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "results_data_from_csv = results_combined\n",
    "\n",
    "\n",
    "def split_csv_by_polygon(csv_file_path):\n",
    "    \"\"\"\n",
    "    Splits the combined results CSV file by polygon and saves each polygon's data to a separate CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    csv_file_path (str): The path to the input CSV file containing combined results.\n",
    "\n",
    "    This function loads the dataset, groups the data by 'File Name' and 'polygon_name',\n",
    "    and saves each group to a separate CSV file in the 'output_csv_by_polygon' directory.\n",
    "    \"\"\"\n",
    "    # Load the dataset from the provided CSV file path\n",
    "    data = pd.read_csv(csv_file_path)\n",
    "    print(\"Loaded data from CSV file.\")\n",
    "    \n",
    "    # Group the data by 'File Name' to ensure unique entries for processing\n",
    "    file_name_groups = data.groupby('File Name')\n",
    "\n",
    "    # Dictionary to store dataframes for each polygon_name\n",
    "    polygon_dfs = {}\n",
    "\n",
    "    # Iterate over each group and split by 'polygon_name'\n",
    "    for file_name, group in file_name_groups:\n",
    "        for polygon_name, polygon_group in group.groupby('polygon_name'):\n",
    "            if polygon_name not in polygon_dfs:\n",
    "                polygon_dfs[polygon_name] = []\n",
    "            # Append each polygon_group to the corresponding list in the dictionary\n",
    "            polygon_dfs[polygon_name].append(polygon_group)\n",
    "\n",
    "    # Get the directory of the input file\n",
    "    output_directory = os.path.dirname(csv_file_path)\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    if not os.path.exists(\"output_csv_by_polygon\"):\n",
    "        os.makedirs(\"output_csv_by_polygon\")\n",
    "\n",
    "    # Save each polygon dataframe to a separate CSV file in the output directory\n",
    "    for polygon_name, polygon_group_list in polygon_dfs.items():\n",
    "        combined_polygon_group = pd.concat(polygon_group_list)\n",
    "        output_path = os.path.join(\"output_csv_by_polygon\", f\"{polygon_name.replace(' ', '_')}.csv\")\n",
    "        combined_polygon_group.to_csv(output_path, index=False)\n",
    "        print(f\"Saved CSV for polygon: {polygon_name}\")\n",
    "\n",
    "    print(f\"CSV files have been split and saved in: {output_directory}\")\n",
    "\n",
    "# Example usage\n",
    "# csv_file_path = r'path_to_your_csv_file.csv'\n",
    "split_csv_by_polygon(csv_file_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Atlas14_311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
